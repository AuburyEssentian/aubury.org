"use strict";(globalThis.webpackChunkaubury_blog=globalThis.webpackChunkaubury_blog||[]).push([[8130],{7735(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"epoch-transition-tax","metadata":{"permalink":"/blog/epoch-transition-tax","source":"@site/blog/2026-02-28-epoch-transition-tax.md","title":"The Epoch Transition Tax","description":"Every 6.4 minutes, Ethereum\'s consensus clients slow down to process epoch state transitions. The head update latency spike \u2014 and the 6\xd7 wrong-head attestation rate that follows \u2014 is measurable, consistent, and varies dramatically by client.","date":"2026-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"attestations","permalink":"/blog/tags/attestations"},{"inline":true,"label":"epoch","permalink":"/blog/tags/epoch"},{"inline":true,"label":"clients","permalink":"/blog/tags/clients"}],"readingTime":4.48,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"epoch-transition-tax","title":"The Epoch Transition Tax","description":"Every 6.4 minutes, Ethereum\'s consensus clients slow down to process epoch state transitions. The head update latency spike \u2014 and the 6\xd7 wrong-head attestation rate that follows \u2014 is measurable, consistent, and varies dramatically by client.","authors":"aubury","tags":["ethereum","consensus","attestations","epoch","clients"],"date":"2026-02-28T00:00:00.000Z"},"unlisted":false,"nextItem":{"title":"The Quiet Consolidation: Ethereum Lost 110,000 Validators After Pectra","permalink":"/blog/2026/02/28/maxeb-consolidation"}},"content":"Every 6.4 minutes, Ethereum\'s consensus clients have a problem. At the boundary between epochs, they need to do expensive work \u2014 update validator balances, compute committee assignments, tick the justification/finalization machinery. While they\'re doing it, the network doesn\'t stop. Blocks keep arriving. Attesters keep committing to what they see.\\n\\nWhat happens to validators whose client is still mid-computation when the attestation window opens? They vote for the wrong head.\\n\\n\x3c!-- truncate --\x3e\\n\\nHere\'s how this looks in the data. The `beacon_api_eth_v1_events_head` table records when each monitoring node\'s beacon API emits a \\"new head\\" event \u2014 the moment the CL has processed the block and updated its view of the chain. Across 7 days and all five major CL clients, the pattern is consistent and striking.\\n\\n```sql\\nSELECT \\n  meta_consensus_implementation,\\n  if(slot % 32 = 0, \'epoch_boundary\', \'mid_epoch\') AS slot_type,\\n  round(quantileExact(0.5)(propagation_slot_start_diff), 0) AS p50_ms,\\n  round(100.0 * countIf(propagation_slot_start_diff > 3000) / count(), 2) AS pct_after_3s\\nFROM beacon_api_eth_v1_events_head\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND propagation_slot_start_diff BETWEEN 0 AND 12000\\nGROUP BY meta_consensus_implementation, slot_type\\nORDER BY meta_consensus_implementation, slot_type\\n```\\n\\nThe global block arrival p50 is 1.78 seconds from slot start \u2014 that\'s the network baseline.\\n\\n![Epoch Transition Tax chart](/img/epoch-transition-tax.png)\\n\\nThe bars show head update time per CL client: blue for mid-epoch, red for epoch boundaries, with the orange extension showing p90. The yellow dashed line at 3 seconds is the rough attestation cut-off \u2014 attesters who haven\'t seen the latest block by then will vote for the wrong head.\\n\\n**Lighthouse** takes a median 2.21 seconds to update its head in normal slots. At epoch boundaries, that becomes **3.93 seconds** \u2014 with 66% of head events arriving after the 3-second mark. **Prysm** hits 3.26 seconds at boundaries, with 60% late. **Grandine** barely moves: 1.95 seconds normally, 2.27 seconds at boundaries, and only 22% late.\\n\\nThe same data broken out by slot position shows the effect is surgical \u2014 it hits exactly slot 0 and nobody else:\\n\\n```sql\\n-- Attestation head accuracy by epoch slot position\\nSELECT \\n  slot % 32 AS epoch_position,\\n  round(100.0 * sum(votes_head) / sum(votes_max), 3) AS head_accuracy_pct,\\n  round(100.0 * sum(votes_other) / sum(votes_max), 3) AS wrong_head_pct\\nFROM mainnet.fct_attestation_correctness_canonical\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\nGROUP BY epoch_position\\nORDER BY epoch_position\\n```\\n\\n**Slot 0: 94.28% head accuracy (5.41% wrong-head). Slots 1\u201331: 98.73\u201399.11%.** A 6\xd7 spike in wrong-head votes, at one precise moment, that returns to baseline immediately at slot 1.\\n\\nThe slot 1 recovery deserves attention. For Lighthouse: 3.93 seconds at slot 0, 2.23 seconds at slot 1 \u2014 an immediate snap back. The epoch transition computation completes, the client catches up, and everything normalizes. There is no cascading degradation. The damage is contained to a single slot, every 32 slots.\\n\\nThe why isn\'t mysterious. At every epoch boundary, consensus clients must compute:\\n\\n- **Validator balance updates** \u2014 pending rewards, penalties, inactivity leaks applied across all ~960,000 validators\\n- **Committee shuffling** \u2014 new committees and proposer duties for the next 32 slots\\n- **Justification and finalization** \u2014 advancing the finality gadget based on recent participation\\n- **Sync committee selection** \u2014 periodic rotation of the 512-validator sync committee\\n\\nDifferent clients have made different choices about *when* to do this work. Grandine appears to precompute or parallelize aggressively, absorbing most of the overhead before the slot boundary hits. Lighthouse does more of it inline during block processing, which explains both the longer median and the dramatic p99 (11.2 seconds \u2014 nearly a full slot).\\n\\nThat p99 number is worth sitting with. One in a hundred epoch-boundary slots, a Lighthouse node takes more than 11 seconds to update its head view after receiving the block. The slot is 12 seconds long.\\n\\nThe seven-day daily consistency confirms this isn\'t noise:\\n\\n```sql\\nSELECT \\n  toDate(slot_start_date_time) AS day,\\n  meta_consensus_implementation,\\n  round(quantileExact(0.5)(propagation_slot_start_diff), 0) AS p50_ms,\\n  round(100.0 * countIf(propagation_slot_start_diff > 3000) / count(), 1) AS pct_after_3s\\nFROM beacon_api_eth_v1_events_head\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND slot % 32 = 0\\n  AND meta_consensus_implementation IN (\'lighthouse\', \'grandine\', \'teku\', \'nimbus\')\\nGROUP BY day, meta_consensus_implementation\\nORDER BY meta_consensus_implementation, day\\n```\\n\\nLighthouse ranges from 3.25\u20134.12 seconds p50 across 8 days, with 56\u201370% of head events late. Grandine stays at 2.19\u20132.49 seconds throughout. The spread is structural, not episodic.\\n\\nA few caveats worth stating clearly. The `beacon_api_eth_v1_events_head` data comes from monitoring sentries watching specific beacon nodes \u2014 not from validators directly. Lighthouse has 147,799 monitoring nodes, Nimbus 50,202, and Teku 50,142, making their numbers very reliable. Grandine (4 nodes) and Prysm (11 nodes) have much smaller samples and should be read directionally rather than precisely.\\n\\nThe 6\xd7 wrong-head rate at epoch boundaries is the network aggregate \u2014 it reflects the combined effect of all client implementations, including their distributions across the real validator set. Clients with larger validator market share contribute more to that number.\\n\\nEthereum does 225 epoch transitions per day. Each one costs roughly a third of attesters their head vote accuracy for a single slot. The math: 5.41% wrong-head at epoch boundary slots, vs 0.90% at normal slots \u2014 an extra 4.5 percentage points per epoch transition. That\'s a recurring tax, paid precisely and predictably, every 6.4 minutes.\\n\\nThe network tolerates it fine. Finality still works. But if you\'re running validators and care about maximising rewards, you have a clear signal about which clients absorb the epoch transition overhead with the least damage.\\n\\n*Data: [`beacon_api_eth_v1_events_head`](https://ethpandaops.io/data/) (7 days, mainnet) \xb7 [`fct_attestation_correctness_canonical`](https://ethpandaops.io/data/) (7 days, CBT) \u2014 ethpandaops Xatu*"},{"id":"/2026/02/28/maxeb-consolidation","metadata":{"permalink":"/blog/2026/02/28/maxeb-consolidation","source":"@site/blog/2026-02-28-maxeb-consolidation.md","title":"The Quiet Consolidation: Ethereum Lost 110,000 Validators After Pectra","description":"Since Pectra activated EIP-7251 in May 2025, the Ethereum validator set has quietly shrunk by 10% while a new class of mega-validators emerged. Three thousand validators now hold more than 1,024 ETH each \u2014 and the pace is accelerating.","date":"2026-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"validators","permalink":"/blog/tags/validators"},{"inline":true,"label":"maxeb","permalink":"/blog/tags/maxeb"},{"inline":true,"label":"pectra","permalink":"/blog/tags/pectra"},{"inline":true,"label":"eip-7251","permalink":"/blog/tags/eip-7251"},{"inline":true,"label":"staking","permalink":"/blog/tags/staking"}],"readingTime":5.1,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"The Quiet Consolidation: Ethereum Lost 110,000 Validators After Pectra","description":"Since Pectra activated EIP-7251 in May 2025, the Ethereum validator set has quietly shrunk by 10% while a new class of mega-validators emerged. Three thousand validators now hold more than 1,024 ETH each \u2014 and the pace is accelerating.","authors":["aubury"],"tags":["ethereum","validators","maxeb","pectra","eip-7251","staking"],"date":"2026-02-28T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The Epoch Transition Tax","permalink":"/blog/epoch-transition-tax"},"nextItem":{"title":"The State Cache Cliff","permalink":"/blog/state-cache-cliff"}},"content":"On May 7, 2025, Ethereum\'s Pectra upgrade (fork name: Electra) activated EIP-7251 \u2014 the Maximum Effective Balance change. The idea was to let validators hold up to 2,048 ETH each, unlocking two things: compounding rewards for validators who opt in, and simpler operations for large stakers who no longer need to manage thousands of 32-ETH keys.\\n\\nMost coverage focused on the compounding angle. The real story turned out to be something else.\\n\\nIn the nine months since Pectra, Ethereum\'s active validator set has shrunk by 110,007 validators \u2014 from 1,068,860 to 958,853. Meanwhile, 3,055 mega-validators holding more than 1,024 ETH each have emerged from essentially nowhere.\\n\\n\x3c!-- truncate --\x3e\\n\\n![MaxEB consolidation chart: validator count declining, mega-validators rising](/img/maxeb-consolidation.png)\\n\\n---\\n\\n**The first mega-validators appeared within 24 hours.**\\n\\nOn May 7, day zero, there were zero validators above 1,024 ETH effective balance. By May 8, there were seven. By May 15, there were 78. Whoever built those validators didn\'t wait to see how the upgrade landed in practice \u2014 they were ready.\\n\\n```sql\\n-- Source: ethpandaops xatu-cbt\\nSELECT day_start_date,\\n    countIf(effective_balance >= 1024000000000) as mega_validators,\\n    count() as total_validators,\\n    round(avg(effective_balance) / 1e9, 3) as avg_eff_eth\\nFROM mainnet.fct_validator_balance_daily\\nWHERE day_start_date >= \'2025-05-07\'\\n  AND status = \'active_ongoing\'\\nGROUP BY day_start_date\\nHAVING count() BETWEEN 800000 AND 1100000\\nORDER BY day_start_date\\n```\\n\\nBy the end of May, there were 228 mega-validators. By July, 284. Then consolidation waves hit:\\n\\n| Date | Mega-validators | Total validators | Avg balance |\\n|------|----------------|-----------------|-------------|\\n| May 7 (Pectra) | 0 | 1,068,860 | 32.00 ETH |\\n| Jun 1 | 230 | 1,058,702 | 32.50 ETH |\\n| Aug 1 | 394 | 1,070,648 | 32.86 ETH |\\n| Sep 1 | 708 | 1,043,151 | 33.38 ETH |\\n| Oct 1 | 1,202 | 977,223 | 34.49 ETH |\\n| Dec 1 | 1,553 | 962,604 | 35.52 ETH |\\n| Feb 26 | **3,055** | **958,853** | **38.68 ETH** |\\n\\nThe average effective balance per validator has risen 20.9% \u2014 from 32.0 ETH to 38.7 ETH \u2014 not because validators are earning more rewards, but because high-balance mega-validators are pulling the average up.\\n\\n---\\n\\n**What consolidation looks like in the entity data.**\\n\\n```sql\\n-- Validator count changes by entity, May 2025 \u2192 Feb 2026\\nSELECT day_start_date, entity, validator_count\\nFROM mainnet.fct_validator_count_by_entity_by_status_daily\\nWHERE day_start_date IN (\'2025-05-07\', \'2026-02-26\')\\n  AND status = \'active_ongoing\'\\n  AND entity != \'\'\\nORDER BY day_start_date, validator_count DESC\\n```\\n\\nThe entities with the steepest validator count declines:\\n\\n| Entity | May 7, 2025 | Feb 26, 2026 | Change |\\n|--------|-------------|-------------|--------|\\n| Coinbase | 82,432 | 53,932 | **\u221228,500** |\\n| Kiln | 46,062 | 12,144 | **\u221233,918** |\\n| Abyss Finance | 30,355 | 14,187 | **\u221216,168** |\\n| Solo stakers | 92,059 | 78,745 | **\u221213,314** |\\n| Staked.us | 14,686 | 9,001 | **\u22125,685** |\\n\\nCoinbase shed 28,500 validators. That\'s not validators leaving Coinbase \u2014 it\'s Coinbase merging their 32-ETH validators into consolidated high-balance validators. At 2,048 ETH per mega-validator, every 64 validators become one.\\n\\nKiln\'s dramatic 73% drop is harder to read cleanly \u2014 likely a mix of consolidation and entity-attribution changes as Kiln\'s sub-entities (kiln_lido, a41_lido) were reassigned. But the overall net-down direction is clear.\\n\\nSolo stakers lost 13,314 validators, a 14.5% decline. This is a mix of exits (solo stakers choosing to leave) and the less likely scenario of solo consolidation (technically available but rarely practical for individuals).\\n\\n---\\n\\n**The consolidation is accelerating.**\\n\\nLooking at the weekly rate of mega-validator creation:\\n\\n- May\u2013Jun: ~38 new mega-validators per week\\n- Aug\u2013Sep: ~80\u2013100 per week  \\n- Jan\u2013Feb 2026: ~130\u2013140 per week\\n\\nAt the current pace, there will be ~7,000 mega-validators by end of 2026.\\n\\n136 validators have already reached the hard maximum of 2,048 ETH \u2014 fully packed. These validators each represent what was once 64 separate 32-ETH validators, probably from a single staking pool that did all their consolidations in one batch.\\n\\n```sql\\n-- Count at the maximum effective balance cap\\nSELECT day_start_date,\\n    countIf(effective_balance = 2048000000000) as maxed_at_2048\\nFROM mainnet.fct_validator_balance_daily\\nWHERE status = \'active_ongoing\'\\nGROUP BY day_start_date\\nHAVING count() BETWEEN 800000 AND 1100000\\nORDER BY day_start_date DESC\\nLIMIT 1\\n-- Returns: 136 validators at max 2048 ETH on Feb 26, 2026\\n```\\n\\n---\\n\\n**What about the compounding angle?**\\n\\nEIP-7251 was pitched largely as enabling reward compounding \u2014 validators with 0x02 withdrawal credentials would see their rewards stack up toward 2,048 ETH instead of being swept out every epoch. This was supposed to make staking more efficient for solo validators who want passive, compounding income.\\n\\nNine months in: barely anyone is doing this.\\n\\n```sql\\n-- Withdrawal credential type breakdown (ethpandaops validator set sample)\\nSELECT \\n    substr(withdrawal_credentials, 1, 4) as cred_prefix,\\n    count(DISTINCT index) as validators\\nFROM canonical_beacon_validators_withdrawal_credentials\\nWHERE meta_network_name = \'mainnet\'\\n  AND epoch = (SELECT max(epoch) FROM canonical_beacon_validators_withdrawal_credentials \\n               WHERE meta_network_name = \'mainnet\')\\nGROUP BY cred_prefix ORDER BY validators DESC\\n-- Returns: 0x01: 36,159 (98.9%)  |  0x02: 186 (0.5%)  |  0x00: 173 (0.5%)\\n```\\n\\nAmong the ethpandaops-monitored validators, only 0.5% have opted into compounding (0x02 credentials). The rest have standard 0x01 partial-withdrawal credentials.\\n\\nThis tracks: switching to 0x02 requires a deliberate consolidation transaction, costs gas, and means your entire stake stays locked rather than dripping rewards to your withdrawal address. For most stakers, especially professionals managing treasury positions, regular partial withdrawals are preferable to compounding.\\n\\nThe compounding pitch was for solo home stakers with long time horizons. It hasn\'t moved them much yet.\\n\\n---\\n\\n**The validator set is becoming more concentrated.**\\n\\nThis is the part that should give the decentralization-watchers pause.\\n\\nWhen 3,055 validators each hold 1,000\u20132,048 ETH, every one of those validators carries the weight of 32\u201364 old-style validators in terms of attestation influence. A single slashing event on a 2,048 ETH validator costs the network 2,048 ETH instead of 32 ETH.\\n\\nWhether this is worse for security depends on who controls those validators. If 136 maxed-out validators are all at Coinbase\'s key infrastructure, that\'s a different risk profile than 136 independent operators each running one. The data tells us who\'s shrinking their validator counts \u2014 it can\'t easily tell us whether the resulting mega-validators are more or less centralized.\\n\\nWhat the data does show: this restructuring is ongoing, it\'s accelerating, and it\'s happening without any protocol change. Just staking providers updating their operations.\\n\\nThat\'s the quiet part. There\'s no hard fork, no governance vote, no announcement. Just 110,000 validators quietly disappearing and 3,055 bigger ones taking their place.\\n\\n---\\n\\n*Data source: ethpandaops xatu-cbt (`mainnet.fct_validator_balance_daily`, `mainnet.fct_validator_count_by_entity_by_status_daily`, `canonical_beacon_validators_withdrawal_credentials`). Pectra/Electra activated at epoch 364032 on May 7, 2025. Analysis window: May 7, 2025 \u2013 February 26, 2026.*"},{"id":"state-cache-cliff","metadata":{"permalink":"/blog/state-cache-cliff","source":"@site/blog/2026-02-28-state-cache-cliff.md","title":"The State Cache Cliff","description":"Ethereum block execution isn\'t a fixed-cost operation. For small blocks the state LRU cache handles nearly everything. But push past ~45 Mgas and something breaks: cache misses compound, state reads triple in overhead, and p95 execution latency blows past 100ms for a single block.","date":"2026-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"execution","permalink":"/blog/tags/execution"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"gas","permalink":"/blog/tags/gas"}],"readingTime":4.85,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"state-cache-cliff","title":"The State Cache Cliff","authors":["aubury"],"tags":["ethereum","execution","performance","gas"]},"unlisted":false,"prevItem":{"title":"The Quiet Consolidation: Ethereum Lost 110,000 Validators After Pectra","permalink":"/blog/2026/02/28/maxeb-consolidation"},"nextItem":{"title":"Sync Committee Ghosts","permalink":"/blog/sync-committee-ghosts"}},"content":"Ethereum block execution isn\'t a fixed-cost operation. For small blocks the state LRU cache handles nearly everything. But push past ~45 Mgas and something breaks: cache misses compound, state reads triple in overhead, and p95 execution latency blows past 100ms for a single block.\\n\\nNobody talks about this because mgas/s benchmarks measure throughput \u2014 not the hidden cost of cold cache reads. The gas limit doubling from 30M to 60M made this matter.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What the data shows\\n\\nThe `execution_block_metrics` table in the [Xatu](https://xatu.ethpandaops.io) dataset captures per-block state access stats from a single Reth monitoring node. For each of the ~50K blocks over the past 7 days, it records EVM execution time, state read time, state hash time, commit time \u2014 and crucially, the LRU cache hit rates for accounts, storage slots, and code.\\n\\nThe time breakdown across all blocks:\\n\\n- **61%** EVM execution (opcodes, precompiles)\\n- **17%** state reads (fetching from the state trie or disk)\\n- **13%** state commits (writing dirty pages back)\\n- **9%** state root hashing\\n\\nState reads are 17% of total block processing time on average. But that average hides a dramatic non-linearity.\\n\\n## The cliff\\n\\nQuery: `execution_block_metrics WHERE event_date_time >= now() - INTERVAL 7 DAY AND gas_used > 1e6`, grouped by gas tier:\\n\\n| Block gas   | Blocks | Avg reads | Cache hit rate | Est. misses | State read time (avg) | State read time (p95) |\\n|-------------|--------|-----------|----------------|-------------|----------------------|----------------------|\\n| < 15 Mgas   | 6,554  |       717 |         88.6%  |        82   |  3.3 ms              |  6.2 ms              |\\n| 15\u201330 Mgas  | 21,889 |     1,542 |         92.4%  |       118   |  6.8 ms              | 32.6 ms              |\\n| 30\u201345 Mgas  | 12,426 |     2,281 |         92.4%  |       174   | 12.0 ms              | 40.1 ms              |\\n| 45\u201360 Mgas  |  9,337 |     3,153 |         86.9%  |       414   | 31.5 ms              | 68.9 ms              |\\n\\nFrom 30M to 45M gas (+50% more gas):\\n- State reads grow from 2,281 to 3,153 (+38%)\\n- Cache misses grow from 174 to 414 (+**138%**)\\n- State read time grows from 12.0ms to 31.5ms (+**162%**)\\n\\nGas grows 50%. Cache misses grow 2.8\xd7 faster. State read overhead grows 3\xd7 faster.\\n\\nAt the 5M bucket level, the picture is even clearer:\\n\\n![State cache cliff chart: state read time and cache hit rate vs block gas used](/img/state-cache-cliff.png)\\n\\nThe cache hit rate peaks at 92.7% for 20\u201330 Mgas blocks, then starts declining as blocks grow larger. By 55 Mgas it\'s at 84.8% \u2014 each block is now spilling state out of the LRU cache, forcing later transactions in the block to re-read from disk what earlier transactions already evicted.\\n\\n## Why it happens\\n\\nThe Reth execution client maintains LRU caches for recently-accessed accounts, storage slots, and contract code. For smaller blocks (~30M gas), these caches absorb the hot state well \u2014 popular DEX contracts, stablecoin balances, and frequently-accessed storage slots stay warm across transactions.\\n\\nAt ~45M+ gas, blocks access enough *unique* storage slots to start exhausting the cache. Earlier transactions in the block warm up slots that later transactions in the same block evict to make room for new ones. Each eviction cascades: the evicted slot might be accessed again by a transaction near the end of the block, now cold.\\n\\nThe estimated cache miss count (reads \xd7 (1 \u2212 hit_rate)) per 5M bucket:\\n\\n| Block gas | Est. cache misses | Change vs previous |\\n|-----------|-------------------|-------------------|\\n| 5 Mgas    | 61                | \u2014                 |\\n| 30 Mgas   | 155               | +154% for +500% gas |\\n| 45 Mgas   | 278               | +79% for +50% gas  |\\n| 55 Mgas   | 501               | +80% for +22% gas  |\\n\\nFrom 5M to 55M gas, the miss count grows 10\xd7. Gas grew 11\xd7, which sounds linear \u2014 but the cache hit *rate* also collapsed by 5.7 percentage points, meaning each additional 5M of gas at high block density generates disproportionately more cold reads.\\n\\n## The throughput paradox\\n\\nDespite the cache miss explosion, raw throughput (mgas/s) keeps climbing:\\n\\n| Gas tier   | Throughput |\\n|------------|-----------|\\n| < 15 Mgas  | 337 mgas/s |\\n| 15\u201330 Mgas | 416 mgas/s |\\n| 30\u201345 Mgas | 451 mgas/s |\\n| 45\u201360 Mgas | 473 mgas/s |\\n\\nBigger blocks are faster per Mgas in absolute terms, because fixed overheads (RPC round-trips, block header processing) get amortized over more computation. But the state read fraction of that time grows fast: 10.6% for small blocks, 26.0% for 45\u201360M blocks.\\n\\nThis is the hidden tax. A \\"full\\" block at the old 30M limit spent 6.8ms on state reads. A \\"full\\" block at 60M spends 31.5ms \u2014 **4.6\xd7 the state read overhead for 2\xd7 the gas**. And at the p95, it\'s worse: 32.6ms at 30M versus 69ms at 45\u201360M, with the tail widening dramatically as the cache starts missing.\\n\\n## Implications\\n\\nThe gas limit went from 30M to 36M (Feb 2025), then 45M (Jul 2025), then 60M (Nov 2025). Each step pushed more blocks into the cache-stressed regime. At 30M, a \\"full\\" block was well within the cache\'s sweet spot \u2014 92% storage hit rate, 6.8ms read time. At 60M, the median full block carries 414 cache misses and 31.5ms of read overhead.\\n\\nNone of this is close to the 12-second slot budget. At a p95 of 68ms for 45\u201360M blocks, there\'s plenty of headroom for normal operation. But the tail risk grows with block gas: blocks that incidentally access more unique state \u2014 certain MEV patterns, onboarding transactions for new addresses, contract deployments \u2014 will hit p99 or p999 latency that doesn\'t show up in these averages.\\n\\nThe cache miss non-linearity also means gas limit increases don\'t have uniform effects. Moving from 30M to 45M was 50% more gas but roughly 50% more misses. Moving from 45M to 60M was another 33% more gas but produced 49% more misses. The cost curve is steeper each step up.\\n\\n---\\n\\n*Single-node caveat: this data comes from one Reth monitoring node. Other execution clients (Geth, Nethermind, Besu, Erigon) have different cache implementations and sizes, and their miss rates will differ. The qualitative finding \u2014 that state read overhead grows super-linearly with block gas \u2014 is likely general, but the specific numbers are Reth-specific. Worth investigating across clients.*\\n\\n*Query source: `execution_block_metrics` \xb7 Xatu (ethpandaops) \xb7 Feb 20\u201327, 2026 \xb7 ~50K mainnet blocks*"},{"id":"sync-committee-ghosts","metadata":{"permalink":"/blog/sync-committee-ghosts","source":"@site/blog/2026-02-28-sync-committee-ghosts.md","title":"Sync Committee Ghosts","description":"Every 27 hours, Ethereum rotates its sync committee \u2014 a randomly selected group of 512 validators who sign every block header during their term. Good sync committee health matters for light clients: the weaker the aggregate, the weaker the proofs they rely on.","date":"2026-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"validators","permalink":"/blog/tags/validators"},{"inline":true,"label":"sync-committee","permalink":"/blog/tags/sync-committee"}],"readingTime":4.51,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"sync-committee-ghosts","title":"Sync Committee Ghosts","authors":["aubury"],"tags":["ethereum","consensus","validators","sync-committee"]},"unlisted":false,"prevItem":{"title":"The State Cache Cliff","permalink":"/blog/state-cache-cliff"},"nextItem":{"title":"The Fee That Never Was: USDT\'s Ghost Mechanism Runs in Every Block","permalink":"/blog/usdt-fee-paradox"}},"content":"Every 27 hours, Ethereum rotates its sync committee \u2014 a randomly selected group of 512 validators who sign every block header during their term. Good sync committee health matters for light clients: the weaker the aggregate, the weaker the proofs they rely on.\\n\\nLooking at the last 30 days of data, a pattern emerges that nobody seems to have measured before. In 22 of the 27 committee periods, at least one selected validator was completely offline for their entire term \u2014 not a few blocks missed, but every single one of the ~8,192 slots. Dead weight drawn by lottery.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe question is whether this is random bad luck, or whether something structural drives it.\\n\\n```sql\\n-- Identify \\"ghost\\" validators: selected for sync committee, 100% miss rate\\nSELECT sync_committee_period, validator_index, count() AS total_blocks,\\n       countIf(has(validators_missed, validator_index)) AS missed,\\n       missed / total_blocks AS miss_rate\\nFROM canonical_beacon_block_sync_aggregate\\nARRAY JOIN validators_missed AS validator_index\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 30 DAY\\nGROUP BY sync_committee_period, validator_index\\nHAVING miss_rate > 0.80 AND total_blocks > 4000\\nORDER BY sync_committee_period\\n```\\n\\nThat query returned 30 zombie validators across 22 periods. Not unusual validators with some connectivity problems \u2014 these validators missed **100% of their slots** across an entire 27-hour committee term.\\n\\nTen of them had validator indices under 25,000.\\n\\nThat matters because validator index is essentially a birth certificate. Ethereum\'s beacon chain launched December 1, 2020. The first 25,000 validators were the founding cohort \u2014 early enthusiasts, testnets, infrastructure teams, and solo stakers who believed in the chain from day one. Five-plus years later, those validators still occupy slots in the active set.\\n\\nTo measure how overrepresented they are in the ghost population, I pulled which validators appeared in sync committees over 30 days (both participating and missing):\\n\\n```sql\\n-- Count sync committee members seen in last 30 days, by validator era\\nSELECT\\n  multiIf(validator_index < 25000, \'under 25K\', validator_index < 100000, \'25K-100K\',\\n          validator_index < 500000, \'100K-500K\', \'>500K\') AS era,\\n  count(DISTINCT validator_index) AS validators_in_committees\\nFROM canonical_beacon_block_sync_aggregate\\nARRAY JOIN arrayConcat(validators_participated, validators_missed) AS validator_index\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 30 DAY\\nGROUP BY era ORDER BY min(validator_index)\\n```\\n\\nResult: 158 genesis-era validators ( under 25K) appeared in sync committees over 30 days. Of those, **10 were complete ghosts** \u2014 a 6.33% ghost rate. For validators with indices above 500K (the 2022\u20132026 cohort), the ghost rate was **0.10%**.\\n\\nThat\'s a **63\xd7 difference**.\\n\\n![Sync committee ghost rate by validator era, and 30-day period timeline](/img/sync-committee-ghosts.png)\\n\\nThe decline is monotonic across all four cohorts: genesis \u2192 early 2021 \u2192 2021\u20132022 \u2192 2022-present tracks to 6.33%, 1.71%, 0.40%, 0.10%. This isn\'t just comparing the extremes \u2014 every generation is about 4\xd7 cleaner than the one before it.\\n\\nThe mechanism isn\'t complicated. Being selected for the sync committee is a randomised duty that happens roughly once every 22 months per validator. A professional staking operator is essentially never offline for 27 consecutive hours without noticing. But some early solo stakers who set up their validators in late 2020 have long since moved on. Their keys are still registered. Their ETH is still staked. Nobody is watching the logs. When their validator index comes up in the RANDAO draw, the sync committee gets a seat that\'s permanently empty.\\n\\nThey stay in the active set because exiting requires a deliberate voluntary exit transaction. Simply going offline doesn\'t trigger ejection until inactivity penalties drain the effective balance below the ejection threshold \u2014 a slow process that can take many months or longer depending on how the offline validator was configured.\\n\\nLooking at individual incidents: period 1667 (February 10\u201311) had **three** genesis-era ghosts simultaneously. Validators #13418, #13513, and #21567 \u2014 all from the chain\'s first few months \u2014 were all in the same committee window and all completely absent. That period\'s average participation dropped to 498.5 out of 512, the lowest in the 30-day window.\\n\\nThe absolute participation numbers stay healthy by most measures. An average of 504 out of 512 means sync aggregates are still 98.4% strong \u2014 more than sufficient for the protocol. But the drift direction matters: three ghost validators in one committee period, drawn at random from a pool of permanent absentees that was always going to include some, is how an edge becomes a blind spot.\\n\\nThe protocol-level fix for this is validator inactivity penalties eventually forcing exit of validators who have been offline for extended periods. That process is working \u2014 it\'s just slow. The genesis-era validator ghost rate of 6.33% reflects validators who have been offline long enough to miss their sync committee duties but not yet long enough to have been ejected.\\n\\nWorth cross-checking: if the full 30-day ghost rate is 10 genesis-era ghosts across 158 committee appearances, and sync committee membership rotates roughly every 22 months per validator, that implies roughly 158 unique genesis-era validators appeared in sync committees over 30 days out of a remaining genesis-era active set of some number. The 6.33% ghost rate means about 1 in 16 of the genesis-era validators still in the active set are effectively permanently offline but still collecting sync committee duty assignments.\\n\\nOne in sixteen. Showing up on the roll. Signing nothing.\\n\\n---\\n\\n*Data: `canonical_beacon_block_sync_aggregate` via Xatu (ethpandaops), Jan 29 \u2013 Feb 28, 2026, 27 sync committee periods, ~8,192 slots per period (~27.3 hours). Ghost defined as validator appearing in `validators_missed` for >80% of their period with >4,000 observed blocks. 30-day window covers 13,251 unique validators in sync committees.*"},{"id":"usdt-fee-paradox","metadata":{"permalink":"/blog/usdt-fee-paradox","source":"@site/blog/2026-02-28-usdt-fee-paradox.md","title":"The Fee That Never Was: USDT\'s Ghost Mechanism Runs in Every Block","description":"USDT\'s transfer contract reads its fee rate and fee cap on every single transfer. The fee has been 0% since 2017. The code still runs \u2014 confirmed 5 million times a month in 100% of Ethereum blocks.","date":"2026-02-28T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"usdt","permalink":"/blog/tags/usdt"},{"inline":true,"label":"evm","permalink":"/blog/tags/evm"},{"inline":true,"label":"state","permalink":"/blog/tags/state"},{"inline":true,"label":"parallel-evm","permalink":"/blog/tags/parallel-evm"}],"readingTime":3.73,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"usdt-fee-paradox","title":"The Fee That Never Was: USDT\'s Ghost Mechanism Runs in Every Block","description":"USDT\'s transfer contract reads its fee rate and fee cap on every single transfer. The fee has been 0% since 2017. The code still runs \u2014 confirmed 5 million times a month in 100% of Ethereum blocks.","authors":"aubury","tags":["ethereum","usdt","evm","state","parallel-evm"],"date":"2026-02-28T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Sync Committee Ghosts","permalink":"/blog/sync-committee-ghosts"},"nextItem":{"title":"The EL Validation Race: Reth Cut Block Processing Time 25% in 30 Days","permalink":"/blog/2026/02/27/el-validation-race"}},"content":"Buried in the USDT contract source code is a comment that reads: *\\"additional variables for use if transaction fees ever became necessary.\\"* Beneath it: `basisPointsRate = 0` and `maximumFee = 0`. Both initialized to zero. Never changed. The fee mechanism was coded in 2017 in case Tether ever wanted to charge for transfers. They never did.\\n\\nBut the code that reads those variables runs on every USDT transfer. And USDT transfers happen in virtually every Ethereum block.\\n\\n\x3c!-- truncate --\x3e\\n\\nHere\'s what that looks like in the raw execution trace data. Querying `canonical_execution_storage_reads` \u2014 a table that records every storage slot accessed during block execution \u2014 against 30 days of mainnet history:\\n\\n```sql\\nSELECT \\n  contract_address,\\n  slot,\\n  count() AS total_reads,\\n  count(DISTINCT block_number) AS blocks_present,\\n  round(count() / count(DISTINCT block_number), 1) AS avg_reads_per_block\\nFROM canonical_execution_storage_reads\\nWHERE meta_network_name = \'mainnet\'\\n  AND block_number >= 24475000\\n  AND block_number <= 24555000   -- ~30 days, ~79,743 blocks\\nGROUP BY contract_address, slot\\nHAVING blocks_present >= 72000\\nORDER BY total_reads DESC\\n```\\n\\nThe result is stark. USDT\'s four core storage slots \u2014 owner address (slot 0), basisPointsRate (slot 3), maximumFee (slot 4), and a deprecation flag (slot 10) \u2014 are present in **79,743 out of 79,743 blocks**. Not 99.9%. Not 99.99%. Every single block in the window.\\n\\n**5.1 million reads of the deprecation flag. 5.0 million reads of the fee rate. 5.0 million reads of the fee cap. All returning zero.**\\n\\n![USDT storage hotspot chart](/img/usdt-universal-hotspot.png)\\n\\nThe read intensity column tells the rest: **64 reads per block** of the deprecation slot, **62\u201363 reads per block** of the fee slots. That\'s roughly 62 USDT transfers happening per block, every block, each one checking the same three parameters before proceeding.\\n\\nThe check isn\'t optional. The USDT `transfer` function always calls `calcFee`, which reads `basisPointsRate` and `maximumFee`, computes `fee = (value \xd7 basisPointsRate) / 10000`, and caps it at `maximumFee`. Since both inputs are zero, fee is zero. The transfer proceeds, fee-free, as it has since 2017.\\n\\nThe cost isn\'t zero, though. Under EIP-2929, a cold SLOAD costs 2,100 gas. Each USDT transfer starts fresh \u2014 no warm cache carries over between transactions \u2014 so it pays for three cold reads on the fee slots: roughly **6,300 gas per transfer just to confirm the fee is nothing**. Across 62 transfers per block and 7,200 blocks per day, that\'s about **2.7 billion gas per day** spent on this confirmation. Equivalent to roughly 45 full Ethereum blocks of capacity, daily, answering the same question with the same answer.\\n\\nUSDC is almost as universal, appearing in 99.99% of blocks with ~42 reads per block. But USDC\'s pattern is different: it\'s a proxy contract, and two of its hot slots (`0x7050...` and `0x10d6...`) are mapping-derived addresses \u2014 almost certainly the token balances of specific high-volume DeFi addresses (major Uniswap pools or similar) being read constantly by routing logic. Those slots change frequently as liquidity moves. The USDT fee slots never change at all.\\n\\nThe Uniswap V3 USDT/ETH pool (identified as `0xc7bbec68...` on Etherscan) adds another tier: present in 91.8% of blocks, with 3.9 reads per block. This is its pool state slot \u2014 the packed struct containing the current price, tick, and liquidity \u2014 read whenever any transaction needs the current exchange rate for routing. Dynamic, meaningful data.\\n\\nThen there\'s the USDT fee slots: static, zero-valued, and inescapable.\\n\\nThis has a practical consequence for any parallel EVM scheme. Optimistic parallelism works by speculatively executing transactions simultaneously and detecting conflicts after the fact. Two transactions that read but never write the same slot can usually coexist \u2014 no conflict. But transactions that both read *and* write the same slot must be serialized. USDT fee slots are read by every USDT transfer and never written \u2014 they\'re pure read-only at this point, which is actually fine for parallelism. The deeper bottleneck is the per-validator balance slots, the `isBlacklisted` mapping, and other USDT state that IS written per-transfer. But the fee slots illustrate a broader point: **specific storage locations in Ethereum don\'t just have high access frequency \u2014 they have structural, inescapable access frequency** that no optimization can route around without redeploying the contract.\\n\\nTether can\'t upgrade USDT without migrating all liquidity, integrations, and institutional custody arrangements. The fee mechanism is permanent. So is the gas it consumes.\\n\\nEvery block, somewhere between 60 and 65 Ethereum transactions ask: \\"Is the USDT fee rate non-zero?\\" Every block, the EVM reads the answer from storage and says: no.\\n\\n*Data: `canonical_execution_storage_reads` (xatu, 30-day mainnet window, blocks 24,475,000\u201324,555,000) \xb7 Contract source: [Etherscan USDT](https://etherscan.io/token/0xdac17f958d2ee523a2206206994597c13d831ec7)*"},{"id":"/2026/02/27/el-validation-race","metadata":{"permalink":"/blog/2026/02/27/el-validation-race","source":"@site/blog/2026-02-27-el-validation-race.md","title":"The EL Validation Race: Reth Cut Block Processing Time 25% in 30 Days","description":"Reth validates blocks in 35ms. Erigon takes 200ms. Besu is slowing down. And a new Rust client called ethrex is right on Reth\'s heels \u2014 with nightly builds and one monitoring node.","date":"2026-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"execution-clients","permalink":"/blog/tags/execution-clients"},{"inline":true,"label":"reth","permalink":"/blog/tags/reth"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"engine-api","permalink":"/blog/tags/engine-api"}],"readingTime":5.88,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"The EL Validation Race: Reth Cut Block Processing Time 25% in 30 Days","description":"Reth validates blocks in 35ms. Erigon takes 200ms. Besu is slowing down. And a new Rust client called ethrex is right on Reth\'s heels \u2014 with nightly builds and one monitoring node.","authors":["aubury"],"tags":["ethereum","execution-clients","reth","performance","engine-api"],"date":"2026-02-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The Fee That Never Was: USDT\'s Ghost Mechanism Runs in Every Block","permalink":"/blog/usdt-fee-paradox"},"nextItem":{"title":"The State Graveyard: 88% of Ethereum\'s Storage Hasn\'t Been Touched in a Year","permalink":"/blog/2026/02/27/ethereum-state-graveyard"}},"content":"Every 12 seconds, your execution client gets a new block and has to tell the consensus layer: valid or not. The call is `engine_newPayload`, and how long it takes determines how fast your validator can attest to the new head.\\n\\nThat window matters. A faster EL client means earlier head votes, better attestation accuracy, and more flexibility in how you run your validator. The timing game, MEV extraction, and late block handling all happen inside this gap.\\n\\nSo which execution client is fastest? The data is messier than the win-rate leaderboards suggest.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What the data actually shows\\n\\nThe ethpandaops monitoring network records every `engine_newPayload` call from its fleet of sentry nodes \u2014 each running one EL client. That\'s tens of thousands of raw observations per day per client, enough to compute real median validation times.\\n\\n```sql\\n-- Source: xatu execution_engine_new_payload\\n-- Filter: mainnet, status = VALID, last 30 days\\nSELECT \\n    toDate(event_date_time)             AS day,\\n    meta_execution_implementation       AS client,\\n    count()                             AS obs,\\n    quantileExact(0.5)(duration_ms)     AS p50_ms,\\n    quantileExact(0.95)(duration_ms)    AS p95_ms\\nFROM execution_engine_new_payload\\nWHERE meta_network_name = \'mainnet\'\\n  AND status = \'VALID\'\\nGROUP BY day, client\\n```\\n\\nAcross the six clients tracked over the last month, the picture is striking.\\n\\n![EL validation race](/img/el-validation-race.png)\\n\\nThree tiers emerge clearly on the log scale:\\n\\n| Client | 30-day p50 | Change |\\n|---|---|---|\\n| Reth | 40ms avg, **35ms now** | \u2193 25% |\\n| ethrex | 41ms avg, **37ms now** | \u2193 16% |\\n| Nethermind | 45ms avg, **~48ms now** | \u2193 7% |\\n| Geth | 75ms avg | stable |\\n| Besu | 156ms avg, **171ms now** | \u2191 15% |\\n| Erigon | 210ms avg, range 171\u2013301ms | volatile |\\n\\nThe gap is substantial. At the 95th percentile, Erigon takes over a second to validate blocks that Reth processes in under 100ms.\\n\\n## Reth\'s monthly sprint\\n\\nReth went from 47ms to 35ms p50 over these 30 days \u2014 a 25% improvement that happened in two steps. The first step came around Feb 9\u201310, when development builds in the 1.10.2 series dropped from 47ms to around 34ms. The second step came when 1.11.0-dev builds started appearing on Feb 13, pushing median time to the 31\u201332ms range.\\n\\n```sql\\n-- Reth version timeline, raw xatu\\nSELECT \\n    toDate(event_date_time)         AS day,\\n    meta_execution_version          AS version,\\n    quantileExact(0.5)(duration_ms) AS p50_ms,\\n    count()                         AS obs\\nFROM execution_engine_new_payload\\nWHERE meta_execution_implementation = \'Reth\'\\n  AND event_date_time >= now() - INTERVAL 30 DAY\\n  AND status = \'VALID\'\\nGROUP BY day, version\\nORDER BY day, obs DESC\\n```\\n\\nThe old 1.9.3 nodes \u2014 still running in the monitoring fleet on unchanged hardware \u2014 consistently show 41\u201350ms. The newer 1.11.1 builds run at 29\u201331ms. Same machine, same blocks, different software: a 36% gap between two adjacent version families.\\n\\nThis is what rapid iteration through nightly development builds looks like in validation timing data.\\n\\n## ethrex: one node, nightly builds, matching the leader\\n\\nThere\'s a sixth execution client in the raw xatu data that doesn\'t appear on any major \\"client diversity\\" leaderboard yet: **ethrex**, built by Lambda Class in Rust.\\n\\n```sql\\n-- ethrex is in the raw data (meta_execution_implementation = \'ethrex\')\\n-- First observation: 2026-01-29\\n-- Node count: 1\\n-- Version format: 9.0.0-{short_commit_hash}\\n-- New build every 1\u20132 days\\n```\\n\\nOne monitoring node, a new build almost every day. The version string changes daily (`9.0.0-17f0f3bf`, `9.0.0-7839747f`, etc. \u2014 each one a fresh nightly commit). And the performance tells a story: ethrex started at 44ms p50 in late January and has been tracking Reth downward since, landing at 35\u201337ms over the last week.\\n\\nAt this level of performance, ethrex is already competitive with production Reth. It\'s been running on mainnet, it\'s getting faster, and the trajectory closely mirrors Reth\'s. Whether it expands beyond one monitoring node is the next question.\\n\\n## The win-rate problem\\n\\nThere\'s a popular way to benchmark EL client speed: count how many slots each client \\"wins\\" \u2014 i.e., for each block, which client type was fastest to return VALID.\\n\\n```sql\\n-- Source: xatu-cbt mainnet.fct_engine_new_payload_winrate_daily\\nSELECT el_client, sum(win_count) as wins\\nFROM mainnet.fct_engine_new_payload_winrate_daily\\nWHERE day_start_date >= today() - 30\\nGROUP BY el_client\\nORDER BY wins DESC\\n-- Result: Nethermind 68%, Reth 28%, Erigon 1.9%, Geth 1.3%, Besu 0.4%\\n```\\n\\nNethermind wins 68% of blocks. Reth wins 28%. But we just established that Reth is actually faster than Nethermind in absolute terms (35ms vs 50ms median).\\n\\nThe reason: the win rate measures how often any node of that client type was fastest. If the monitoring fleet has 56 Nethermind nodes and 23 Reth nodes, Nethermind has 2.4\xd7 more shots at the fastest response in any given slot. The best of 56 Nethermind observations will often beat the best of 23 Reth observations, even if the underlying Reth nodes are individually faster.\\n\\nWin rates measure \\"how many nodes you have \xd7 how fast they are.\\" Median validation time measures just how fast. They\'re different things, and only one of them matters for your individual validator.\\n\\nToday (Feb 27), Reth and Nethermind have reached rough parity on win counts \u2014 the first time Reth has matched Nethermind\'s pace in these 30 days. This is a monitoring-fleet-composition story as much as a software story.\\n\\n## Besu is moving in the wrong direction\\n\\nThe outlier on the downside is Besu. Its median validation time has climbed from 141ms in early February to 171ms today \u2014 a 15% deterioration over three weeks.\\n\\nThe p95 picture is worse. Besu\'s 95th percentile went from around 280ms (Jan 28) to over 700ms (Feb 14), then settled back to ~600ms. If you\'re a validator running Besu, your EL is taking 4\u20135\xd7 longer than Reth to validate a typical block, and your tail latency is substantial.\\n\\nFor regular attestation on most slots, this still isn\'t catastrophic \u2014 attestations happen at t=4s and Besu usually finishes well before then. But for timing-game blocks that arrive at t\u22482\u20133s, an extra 130ms of EL processing can be the difference between a head vote and a missed one.\\n\\n## What this means in a slot\\n\\nAt 35ms, Reth processes a full block in 0.3% of a 12-second slot. At 210ms, Erigon is using 1.7% of the slot just on block validation.\\n\\nNeither is catastrophic in isolation. The timing game research has shown validators don\'t attest until t\u22484s regardless \u2014 so a 35ms vs 200ms EL gap isn\'t directly causing missed attestations most of the time.\\n\\nWhere it matters:\\n\\n1. **Late arriving blocks** \u2014 when a proposer publishes at t=3.5s (timing game), the CL gets the block and calls `engine_newPayload` immediately. An Erigon validator has ~8.5s remaining for that call to complete and attestation to propagate. A Reth validator has ~8.5s but with 170ms less pressure.\\n\\n2. **Block building** \u2014 execution clients also validate their own local builds. Faster validation means fresher block content before the deadline.\\n\\n3. **Tail risk** \u2014 the p95/p99 matters more than the median when you\'re running thousands of validators. Erigon\'s p95 at 400\u2013800ms means one in twenty blocks takes a substantial chunk of your attestation window.\\n\\nThe race is happening at the millisecond level. And over the last 30 days, Reth has been running it harder than anyone.\\n\\n---\\n\\n*Data: ethpandaops xatu `execution_engine_new_payload` table, mainnet. 30-day window ending Feb 27, 2026. `engine_newPayload` calls with `status = VALID` only. Win-rate data from `xatu-cbt mainnet.fct_engine_new_payload_winrate_daily`. ethrex first observed Jan 29, 2026.*"},{"id":"/2026/02/27/ethereum-state-graveyard","metadata":{"permalink":"/blog/2026/02/27/ethereum-state-graveyard","source":"@site/blog/2026-02-27-ethereum-state-graveyard.md","title":"The State Graveyard: 88% of Ethereum\'s Storage Hasn\'t Been Touched in a Year","description":"Every Ethereum full node carries 296 GB of state. 88% of it hasn\'t been accessed in over a year. And the single largest consumer isn\'t USDT \u2014 it\'s XEN Crypto.","date":"2026-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"state","permalink":"/blog/tags/state"},{"inline":true,"label":"storage","permalink":"/blog/tags/storage"},{"inline":true,"label":"xen","permalink":"/blog/tags/xen"},{"inline":true,"label":"state-expiry","permalink":"/blog/tags/state-expiry"}],"readingTime":5.64,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"The State Graveyard: 88% of Ethereum\'s Storage Hasn\'t Been Touched in a Year","description":"Every Ethereum full node carries 296 GB of state. 88% of it hasn\'t been accessed in over a year. And the single largest consumer isn\'t USDT \u2014 it\'s XEN Crypto.","authors":["aubury"],"tags":["ethereum","state","storage","xen","state-expiry"],"date":"2026-02-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The EL Validation Race: Reth Cut Block Processing Time 25% in 30 Days","permalink":"/blog/2026/02/27/el-validation-race"},"nextItem":{"title":"The Gas Limit Doubled in 2025: How Validators Quietly Resized Ethereum","permalink":"/blog/2026/02/27/gas-limit-doubling"}},"content":"Every full Ethereum node is currently lugging around **296 GB of state**. Every account, every contract, every storage slot. Sync a fresh node and you\'re downloading all of it. Run a node continuously and you\'re holding all of it in your database, forever.\\n\\nThe uncomfortable truth: the vast majority of that state is dead. It hasn\'t moved in over a year. The addresses are abandoned, the contracts are deprecated, the protocols are gone. The data just... sits there. In every node on the network.\\n\\n\x3c!-- truncate --\x3e\\n\\nHere\'s what the top 15 contracts look like when you sort by total storage slots and ask: how much of this has actually been touched in the last 12 months?\\n\\n![Ethereum state graveyard](/img/ethereum-state-graveyard.png)\\n\\nThat narrow cyan sliver on the left \u2014 that\'s the active portion. Everything else is dormant.\\n\\n## The numbers\\n\\n```sql\\n-- Source: xatu-cbt mainnet.fct_address_storage_slot_total\\n-- \\"expired\\" = not accessed in last 365 days (snapshot: Dec 2025)\\nSELECT total_storage_slots, expired_storage_slots,\\n       round(100 * expired_storage_slots / total_storage_slots, 1) as pct_expired\\nFROM mainnet.fct_address_storage_slot_total\\n```\\n\\nGlobal total: **2.4 billion storage slots**. Of those, **2.12 billion** \u2014 88.3% \u2014 haven\'t been touched in a year.\\n\\nThe state size data confirms: 296 GB total state as of late February 2026, growing at roughly 200 MB per day. Storage trie nodes alone account for 221 GB of that \u2014 the overhead of the Patricia Merkle Trie that makes every slot globally verifiable.\\n\\n## XEN Crypto is the largest state consumer on Ethereum\\n\\nNot USDT. Not USDC. Not WETH. Not Uniswap. **XEN Crypto**, a viral zero-premine token launched in October 2022, has the most storage of any contract on mainnet \u2014 196 million slots.\\n\\n```sql\\n-- Source: xatu-cbt mainnet.fct_address_storage_slot_top_100_by_contract\\nSELECT rank, contract_address, total_storage_slots\\nFROM mainnet.fct_address_storage_slot_top_100_by_contract\\nORDER BY rank LIMIT 5\\n-- rank 1: 0x06450dee... XEN Crypto   196M slots\\n-- rank 2: 0xdac17f95... USDT         121M slots\\n-- rank 3: 0xa0b86991... USDC          50M slots\\n-- rank 4: 0x7be8076f... OpenSea Wyvern v1  31M slots\\n-- rank 5: 0x5acc84a3... Forsage.io   26M slots\\n```\\n\\nXEN\'s storage footprint is larger than USDT and USDC *combined* (172M). Why?\\n\\nXEN\'s mechanism was simple: call `claimRank()` with any wallet, get a minting rank, come back later to claim tokens. One call = one storage slot in XEN\'s mapping. The contract went viral in late 2022, driven by an arms race of bots and retail minters all trying to claim early ranks. Ethereum\'s state grew by XEN\'s adoption curve \u2014 one slot per wallet, no exceptions.\\n\\nEthereum has 364 million accounts in total. XEN\'s 196 million storage slots represent interactions from roughly **54% of all Ethereum accounts ever created**. Not all of those are humans \u2014 many are bot farms that spun up thousands of fresh wallets to game rank ordering \u2014 but the storage is real and permanent either way.\\n\\nOf those 196 million slots, 61.7% are now dormant. The other 38% are technically \\"active\\" by the 365-day standard, which likely reflects read traffic from portfolio trackers and aggregators checking balances rather than actual economic activity in the contract.\\n\\nUSDT (#2 with 121.7M slots) is **81% dormant**. These are token holders who haven\'t moved their USDT in over a year. Diamond hands, or more likely: lost wallets, dead addresses, exchange cold storage that never touches individual slots.\\n\\n## The graveyard tour\\n\\n```sql\\n-- Source: xatu-cbt mainnet.fct_address_storage_slot_expired_top_100_by_contract\\nSELECT rank, contract_address, expired_slots\\nFROM mainnet.fct_address_storage_slot_expired_top_100_by_contract\\nORDER BY rank LIMIT 10\\n```\\n\\nSome of these contracts aren\'t just dormant \u2014 they\'re fully abandoned. Every single storage slot, untouched.\\n\\n**Forsage.io** \u2014 rank 5, 26.5 million slots, 100% expired. Forsage was a Ponzi scheme built on Ethereum smart contracts, viral in 2020-2021, eventually prosecuted by the SEC. The scheme is gone. The storage remains.\\n\\n**OpenSea Wyvern v1 and v2** \u2014 ranks 4 and 6, 31.6M and 19M slots combined, both 100% expired. OpenSea deprecated Wyvern in 2022 in favor of Seaport. Every NFT order, every cancelled listing, every filled bid \u2014 frozen in state forever.\\n\\n**Seaport (OpenSea v3)** \u2014 rank 7, 17.3M slots, also 100% expired. OpenSea has since moved to even newer contracts.\\n\\n**IDEX** \u2014 rank 9, 14.6M slots, 100% expired. One of the early decentralised exchanges. Most of its users migrated elsewhere years ago.\\n\\n**Axie Infinity: Ronin Bridge** \u2014 10.8M slots, 100% expired. The bridge that was drained for $625 million in March 2022. Since replaced. The original contract\'s state just sits in every node\'s database.\\n\\n**CryptoKitties** \u2014 8.9M slots, 99.4% expired. The 2017 NFT game that famously congested Ethereum. Long since faded. Each kitty\'s traits and ownership data still occupies a node somewhere.\\n\\n**EtherDelta** \u2014 9.4M slots, 100% expired. The first meaningful DEX on Ethereum, peaked in 2017-2018, hacked in 2017, effectively dead by 2019.\\n\\nThese aren\'t edge cases. They\'re in the top 15 by total storage. They persist because Ethereum has no mechanism to reclaim storage from abandoned contracts. SSTORE costs gas to write and nothing to hold. Once a slot exists with a non-zero value, it stays until that same slot is explicitly overwritten with zero.\\n\\n## Even the active protocols are mostly dormant\\n\\nWETH: 12.1 million slots, **70% expired**. The token that wraps ETH is one of the most traded assets on Ethereum. Still, 70% of WETH holders haven\'t moved their position in over a year.\\n\\nUniswap v3 NonfungiblePositionManager: 11.8M slots, **85.5% expired**. These are liquidity positions in Uniswap v3. Most LPs haven\'t touched their positions in a year \u2014 either they closed out and the slot was zeroed, or they\'re long-term holders who set and forgot.\\n\\nPermit2 (Uniswap\'s token approval contract): 16.9M slots, **84.2% expired**. Permit2 stores approval records per token per spender per owner. Most approvals were granted once and never revisited.\\n\\nThe pattern is consistent. Even for protocols in active use, the overwhelming majority of their state is stale. USDT processes billions of dollars in transfers every day, yet 81% of the addresses that ever held USDT haven\'t moved in a year.\\n\\n## What this means for the state expiry debate\\n\\nState expiry proposals \u2014 like the historical ones around EIP-4444, EIP-7736, and various \\"state rent\\" discussions \u2014 argue that nodes shouldn\'t have to store data indefinitely that nobody is using.\\n\\nThe argument usually sounds theoretical. These numbers make it concrete.\\n\\nIf a state expiry mechanism existed that could prune slots not accessed in 12 months, the 2.4 billion slot universe would shrink to roughly **280 million active slots** (11.7% of current). The storage trie \u2014 which accounts for the majority of state size due to Merkle overhead \u2014 would compress dramatically alongside it.\\n\\nThis is also the data behind why state sync time for new nodes is measured in hours: the node has to validate and store all 296 GB regardless of whether 88% of it will ever be needed again.\\n\\nFor now, every node holds it all. EtherDelta, Forsage, and XEN\'s armies of minting bots included.\\n\\n---\\n\\n*Data: ethpandaops xatu-cbt `fct_execution_state_size_daily`, `fct_address_storage_slot_top_100_by_contract`, `fct_address_storage_slot_expired_top_100_by_contract`, and `fct_address_storage_slot_total`. Expired = not accessed in 365 days. Snapshot date: December 2025. State size: February 2026.*"},{"id":"/2026/02/27/gas-limit-doubling","metadata":{"permalink":"/blog/2026/02/27/gas-limit-doubling","source":"@site/blog/2026-02-27-gas-limit-doubling.md","title":"The Gas Limit Doubled in 2025: How Validators Quietly Resized Ethereum","description":"Ethereum\'s block capacity doubled from 30M to 60M gas in 2025 through three coordinated validator actions. No hard fork. No governance vote. Just 1.1 million validators updating their config files.","date":"2026-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"gas-limit","permalink":"/blog/tags/gas-limit"},{"inline":true,"label":"eip-1559","permalink":"/blog/tags/eip-1559"},{"inline":true,"label":"validators","permalink":"/blog/tags/validators"},{"inline":true,"label":"governance","permalink":"/blog/tags/governance"}],"readingTime":4.93,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"The Gas Limit Doubled in 2025: How Validators Quietly Resized Ethereum","description":"Ethereum\'s block capacity doubled from 30M to 60M gas in 2025 through three coordinated validator actions. No hard fork. No governance vote. Just 1.1 million validators updating their config files.","authors":["aubury"],"tags":["ethereum","gas-limit","eip-1559","validators","governance"],"date":"2026-02-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The State Graveyard: 88% of Ethereum\'s Storage Hasn\'t Been Touched in a Year","permalink":"/blog/2026/02/27/ethereum-state-graveyard"},"nextItem":{"title":"Six Clients, Two Realities: How Ethereum Disagrees About Reorg Depth","permalink":"/blog/2026/02/27/reorg-depth-client-split"}},"content":"Ethereum\'s block gas limit doubled in 2025. Not through a hard fork. Not through a governance vote. Through three coordinated waves of validators updating their configuration files.\\n\\nThe limit went from 30 million to 60 million gas\u2014a 100% increase in block capacity\u2014between February and November 2025. Each wave moved faster than the last. The final surge, from 45M to 60M, took just 22 hours.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What actually happened\\n\\nThe gas limit is set by validators. Each block proposer signals their preferred limit in the block header. The actual limit drifts toward the median preference, moving at most 1/1024 of the current value per block.\\n\\nThis mechanism allows gradual changes without protocol upgrades. But what we saw in 2025 was not gradual.\\n\\n**Wave 1: February 4-6, 2025**\\n\\nFor over a year, the limit held steady at 30M. Then on February 4, it jumped to 34.9M. By February 6, it stabilized at 36M. A 20% increase in 48 hours.\\n\\n```sql\\n-- Source: xatu canonical_beacon_block\\nSELECT \\n    toDate(slot_start_date_time) as day,\\n    round(avg(execution_payload_gas_limit) / 1e6, 2) as avg_gas_M\\nFROM canonical_beacon_block\\nWHERE slot_start_date_time BETWEEN \'2025-02-01\' AND \'2025-02-10\'\\nGROUP BY day ORDER BY day\\n```\\n\\n| Day | Avg Gas Limit |\\n|-----|---------------|\\n| Feb 1 | 30.2M |\\n| Feb 2 | 30.3M |\\n| Feb 3 | 30.7M |\\n| **Feb 4** | **34.9M** |\\n| Feb 5 | 35.8M |\\n| Feb 6 | 35.9M |\\n\\n**Wave 2: July 21-22, 2025**\\n\\nAfter months at 36M, the limit jumped again. From 37M to 44.9M in two days. This was a 25% increase.\\n\\n**Wave 3: November 24-26, 2025**\\n\\nThe big one. The limit went from 45M to 60M\u2014a 33% increase\u2014in just 22 hours.\\n\\nThe first block to push above 46M was at slot 13097935, proposed by **Binance** at 15:47:23 UTC on November 24. Within hours, Lido, Coinbase, Bitfinex, and solo stakers joined. By November 26 at 02:00 UTC, the network had converged on 60M.\\n\\n```sql\\n-- First blocks above 46M, with proposer entities\\nSELECT \\n    slot,\\n    execution_payload_gas_limit / 1e6 as gas_M,\\n    entity\\nFROM canonical_beacon_block b\\nJOIN mainnet.fct_block_proposer_entity e ON b.slot = e.slot\\nWHERE slot_start_date_time BETWEEN \'2025-11-24 12:00:00\' AND \'2025-11-24 20:00:00\'\\n  AND execution_payload_gas_limit > 46000000\\nORDER BY slot\\nLIMIT 10\\n```\\n\\n| Slot | Gas Limit | Entity |\\n|------|-----------|--------|\\n| 13097935 | 46.02M | **Binance** |\\n| 13097967 | 46.02M | (unknown) |\\n| 13097971 | 46.02M | piertwo |\\n| 13097972 | 46.07M | **Lido** |\\n| 13097973 | 46.11M | Bitfinex |\\n| 13097974 | 46.16M | **Coinbase** |\\n| 13097975 | 46.20M | **Binance** |\\n| 13097976 | 46.16M | solo_stakers |\\n\\n## The 50% rule\\n\\nHere\'s the counterintuitive part: despite doubling the gas limit, blocks are not \\"more full.\\" The fill rate stayed almost exactly at 50% throughout.\\n\\n```sql\\n-- Gas used vs limit over time\\nSELECT \\n    day_start_date,\\n    round(avg_gas_limit / 1e6, 1) as limit_M,\\n    round(avg_gas_used / 1e6, 1) as used_M,\\n    round(avg_gas_used * 100.0 / avg_gas_limit, 1) as fill_pct\\nFROM mainnet.fct_execution_gas_limit_daily gl\\nJOIN mainnet.fct_execution_gas_used_daily gu USING (day_start_date)\\nWHERE day_start_date >= \'2025-11-01\'\\n```\\n\\n| Period | Limit | Used | Fill |\\n|--------|-------|------|------|\\n| Nov 1-24 (45M) | 45.0M | 22.7M | 50.5% |\\n| Nov 25 (transition) | 48.4M | 24.5M | 50.6% |\\n| Nov 26 (60M) | 59.3M | 30.1M | 50.7% |\\n| Dec-Feb (60M) | 60.0M | 30.4M | 50.8% |\\n\\nThis is EIP-1559 working as designed. The base fee adjusts to maintain equilibrium at 50% utilization. When the limit goes up, the base fee drops, demand increases, and blocks fill back to the target.\\n\\nBut here\'s what actually changed: **absolute throughput doubled**. At 30M limit, blocks used ~15M gas. At 60M limit, blocks use ~30M gas. The network processes twice as many transactions per block, at lower base fees.\\n\\n| Date | Base Fee |\\n|------|----------|\\n| Jan 2025 | 16.5 gwei |\\n| Feb 2025 (post-36M) | 1.5 gwei |\\n| Nov 2025 (pre-60M) | 0.08 gwei |\\n| Jan 2026 | 0.05 gwei |\\n\\n## The invisible governance\\n\\nWhat makes this remarkable is how it happened. There was no on-chain vote. No core dev announcement. No DAO proposal.\\n\\nValidators simply updated their `--gas-limit` flags (or equivalent configuration). When enough did so simultaneously, the network followed.\\n\\nThe acceleration is striking:\\n- **Feb 4-6**: 3 days for 30M \u2192 36M\\n- **Jul 21-22**: 2 days for 37M \u2192 45M  \\n- **Nov 25-26**: 22 hours for 45M \u2192 60M\\n\\nEach wave was faster, suggesting validators learned from the previous rounds. The coordination mechanism is informal\u2014Twitter discussions, Discord channels, staking provider internal decisions\u2014but the effect is binding.\\n\\nThis is governance without governance. A $300 billion network changed its most critical economic parameter three times in one year, and the only record is in the block headers.\\n\\n## What the data shows\\n\\nThe CBT table `fct_execution_gas_limit_signalling_daily` reveals current validator preferences. While 60M is now the overwhelming majority (~860K observations/day), a persistent minority signals 36M (~66K/day) and a handful signal 100-120M (~500-600/day).\\n\\n```sql\\n-- Current signalling distribution (Feb 27, 2026)\\nSELECT \\n    day_start_date,\\n    gas_limit_band_counts\\nFROM mainnet.fct_execution_gas_limit_signalling_daily\\nWHERE day_start_date = \'2026-02-27\'\\n```\\n\\nKey bands:\\n- **60M**: 859,780 (99.5% of monitoring observations)\\n- **36M**: 65,752 (7.6%)\\n- **30M**: 15,019 (1.7%)\\n- **100M**: 505 (0.06%)\\n- **120M**: 415 (0.05%)\\n\\nThe 36M faction represents validators who either never updated from the pre-November configuration or are actively pushing to reduce the limit. The 100-120M signals are validators testing or advocating for further increases.\\n\\n## Why this matters\\n\\nEthereum\'s gas limit is its most important economic parameter. It determines:\\n- How many transactions fit in a block\\n- How much state growth occurs\\n- What hardware is required to run a node\\n- Who can afford to participate\\n\\nThe fact that this can change by 100% in 22 hours, without any protocol upgrade, reveals both the flexibility and the fragility of the system.\\n\\nOn one hand, Ethereum can adapt quickly to demand. When L2s need more blob space, when DeFi activity surges, the network can respond.\\n\\nOn the other hand, a coordinated action by major staking providers (Lido, Coinbase, Binance) could theoretically move the limit to any value within days. There are no hard bounds\u2014only the soft constraint of social consensus.\\n\\nThe data shows this is not theoretical. It happened three times in 2025. And it will happen again.\\n\\n---\\n\\n*Chart: Daily average gas limit from January 2024 through February 2026, showing the three transition events. Data source: ethpandaops xatu mainnet.*\\n\\n![Gas limit doubling in 2025](/img/gas-limit-doubling-2025.png)"},{"id":"/2026/02/27/reorg-depth-client-split","metadata":{"permalink":"/blog/2026/02/27/reorg-depth-client-split","source":"@site/blog/2026-02-27-reorg-depth-client-split.md","title":"Six Clients, Two Realities: How Ethereum Disagrees About Reorg Depth","description":"Lighthouse says depth-1. Prysm says depth-2. Same event, same block hashes. What\'s going on inside Ethereum\'s consensus clients?","date":"2026-02-27T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"reorgs","permalink":"/blog/tags/reorgs"},{"inline":true,"label":"fork-choice","permalink":"/blog/tags/fork-choice"},{"inline":true,"label":"clients","permalink":"/blog/tags/clients"}],"readingTime":5.38,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"Six Clients, Two Realities: How Ethereum Disagrees About Reorg Depth","description":"Lighthouse says depth-1. Prysm says depth-2. Same event, same block hashes. What\'s going on inside Ethereum\'s consensus clients?","authors":["aubury"],"tags":["ethereum","consensus","reorgs","fork-choice","clients"],"date":"2026-02-27T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The Gas Limit Doubled in 2025: How Validators Quietly Resized Ethereum","permalink":"/blog/2026/02/27/gas-limit-doubling"},"nextItem":{"title":"Who\'s Missing Attestations? The Staker Performance Gap","permalink":"/blog/attestation-miss-rate-by-entity"}},"content":"Ethereum had 660 chain reorganizations in the last 30 days. That\'s a 0.31% reorg rate across roughly 216,000 slots \u2014 normal background noise for a live PoS network.\\n\\nBut here\'s something nobody talks about: if you ask Lighthouse how deep those reorgs were, you\'ll get a completely different answer than if you ask Prysm. Same event. Same block hashes. Different depth. Every single time.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The finding\\n\\nAt every reorg event, the beacon API emits a `chain_reorg` event containing the slot, the `old_head_block`, the `new_head_block`, and the `depth` \u2014 how many blocks were reorganized.\\n\\nI pulled 30 days of these events from the ethpandaops xatu dataset, across six consensus clients: Lighthouse, Grandine, Prysm, Lodestar, Teku, and Tysm. To filter out noise, I identified **314 \\"consensus reorg\\" slots** \u2014 moments where at least three different client types all agreed that `depth >= 2` had occurred.\\n\\n```sql\\n-- Query: 30-day client depth distribution at consensus reorg events\\n-- Source: xatu beacon_api_eth_v1_events_chain_reorg\\n-- Window: now() - INTERVAL 30 DAY\\n-- Filter: depth >= 1 AND depth < 10 (excludes artifact outliers)\\n-- Consensus slots: slots where 3+ client types reported depth >= 2\\nSELECT \\n    meta_consensus_implementation as client,\\n    depth,\\n    count() as observations\\nFROM beacon_api_eth_v1_events_chain_reorg\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 30 DAY\\n  AND depth >= 1 AND depth < 10\\nGROUP BY slot, client, depth\\n```\\n\\nAt those 314 events:\\n\\n- **Prysm**: depth-2 in 100% of events\\n- **Lodestar**: depth-2 in 100% of events\\n- **Tysm**: depth-2 in 100% of events\\n- **Teku**: depth-2 in 99.4% of events\\n- **Lighthouse**: depth-1 in **99.0%** of events\\n- **Grandine**: depth-1 in **99.4%** of events\\n\\n![Client reorg depth reporting](/img/reorg-depth-client-split.png)\\n\\nLighthouse and Grandine report depth-1. Everyone else reports depth-2. This is not statistical noise. It holds in 99% of events across an entire month.\\n\\n## Same blocks, different depths\\n\\nThe obvious question: are these clients even looking at the same event?\\n\\nYes. I checked `old_head_block` and `new_head_block` for a specific event at slot **13769249** (Feb 25, 21:30 UTC):\\n\\n```sql\\nSELECT \\n    meta_consensus_implementation as client,\\n    depth,\\n    old_head_block,\\n    new_head_block\\nFROM beacon_api_eth_v1_events_chain_reorg\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot = 13769249\\n  AND depth < 10\\nGROUP BY client, depth, old_head_block, new_head_block\\n```\\n\\nEvery client reported the same `old_head_block`: `0x7152e91...`. And Lighthouse, Grandine, Lodestar, and Teku all agreed on the same `new_head_block`: `0x40de3ae...`. But Lighthouse said depth-1. Lodestar and Teku said depth-2.\\n\\nSame source. Same destination. Different depth. That\'s only possible if the clients are measuring from different reference points.\\n\\nThe most likely explanation: Lighthouse and Grandine emit the `chain_reorg` event **after** updating their internal head state. By the time the event fires, they\'ve already adopted the new chain \u2014 and from their updated perspective, the reorg was only 1 slot. Prysm and Lodestar fire the event from their **old** head position, where the reorg looks like 2 slots.\\n\\nNeither is \\"wrong\\" in a broken sense. But they\'re measuring different things and calling them the same field.\\n\\n## Teku\'s follow-on events\\n\\nThere\'s a third behavior, unique to Teku.\\n\\nAt 220 slots over the past 30 days, Teku emitted a `chain_reorg` event with `depth >= 3` \u2014 at a slot **after** the main reorg had already been reported. No other client emitted anything for those same slots.\\n\\nHere\'s the pattern from slot 13769249 through 13769251 (events 12 seconds apart):\\n\\n```\\nSlot 13769249  (21:30:11): All clients \u2014 reorg at depth 1-2\\nSlot 13769250  (21:30:23): Lodestar + Teku \u2014 depth-3\\nSlot 13769251  (21:30:35): Teku only \u2014 depth-4\\n```\\n\\nThe chain had reorganized. Most clients processed it in one slot. Teku kept revising its depth estimate for 24 more seconds \u2014 emitting fresh depth-3 and then depth-4 events as it continued reconciling its internal state with the new canonical chain.\\n\\nThis isn\'t a Teku bug per se. It may reflect a more conservative internal reconciliation loop \u2014 Teku doesn\'t commit to the final reorganization picture until it\'s fully processed. But it does mean that if you\'re using chain_reorg events to alert on deep reorgs, a Teku-based monitor will fire alarms that no other client would trigger.\\n\\n## The other split: Prysm and Tysm pick a different winner\\n\\nThere\'s a second layer to this.\\n\\nIn **315 of 660 reorg slots** (47.7%), Prysm and Tysm briefly adopted a *different* `new_head_block` than the other four clients. Not just a different depth \u2014 an entirely different block hash.\\n\\n```sql\\n-- Query: new_head_block by client at the same reorg slot\\n-- At contested events: Prysm+Tysm go to block A, others go to block B\\nSELECT \\n    meta_consensus_implementation as client,\\n    new_head_block,\\n    count() as obs\\nFROM beacon_api_eth_v1_events_chain_reorg\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot = 13560513\\nGROUP BY client, new_head_block\\n```\\n\\nThe split is consistent: **Prysm + Tysm** on one fork, **Lighthouse + Grandine + Lodestar + Teku** on another \u2014 nearly half the time a reorg occurs.\\n\\nThis likely reflects a tie-breaking difference in LMD-GHOST. When two valid blocks compete for the same slot, different clients may score them differently based on their view of recent attestations. Prysm and Tysm appear to consistently prefer one competing block; the other four clients prefer the other.\\n\\nThe chain eventually settles, and the data here is from monitoring nodes (not validators) \u2014 so this doesn\'t directly affect consensus. But it does mean that during the ~12 seconds a fork is contested, the network has two distinct client camps holding different views of the canonical head.\\n\\n## What this means for monitoring\\n\\nIf your Ethereum monitoring stack runs Lighthouse nodes, you\'re seeing a different chain than Prysm operators see. Not in terms of finalized blocks \u2014 both will converge there \u2014 but in terms of how much turbulence occurred in the last few slots.\\n\\nA Lighthouse monitor reports 99% of reorgs as depth-1: \\"minor, one block reshuffled.\\"\\n\\nA Prysm monitor reports those same events as depth-2: \\"two blocks reorganized, attestations may have been orphaned.\\"\\n\\nNeither knows the other is reading the same event differently.\\n\\nThe practical impact is real for anyone building reorg alerting, SLA dashboards, or health metrics on top of beacon API events. You can\'t just look at the `depth` field and assume it\'s client-agnostic. You need to know who reported it.\\n\\nThe depth discrepancy exists because the beacon API spec doesn\'t pin down *when* relative to internal state the `chain_reorg` event must be emitted. That ambiguity has accumulated into a 99% consistent split between two implementation families \u2014 one that reads \\"1\\", one that reads \\"2\\", and a third that keeps updating for 24 seconds.\\n\\n---\\n\\n*Data: [ethpandaops xatu dataset](https://ethpandaops.io), beacon_api_eth_v1_events_chain_reorg, 30-day window through 2026-02-26. Consensus client labels from meta_consensus_implementation. Tysm is a Teku-based monitoring client variant.*"},{"id":"attestation-miss-rate-by-entity","metadata":{"permalink":"/blog/attestation-miss-rate-by-entity","source":"@site/blog/2026-02-26-attestation-miss-rate-by-entity.md","title":"Who\'s Missing Attestations? The Staker Performance Gap","description":"One validator missing an attestation isn\'t a crisis. A few hundred validators missing 1 in 8 attestations, every slot, for three months straight \u2014 that\'s a different story. And it\'s concentrated in a way that the aggregate participation numbers don\'t show.","date":"2026-02-26T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"attestations","permalink":"/blog/tags/attestations"},{"inline":true,"label":"staking","permalink":"/blog/tags/staking"},{"inline":true,"label":"avado","permalink":"/blog/tags/avado"},{"inline":true,"label":"dappnode","permalink":"/blog/tags/dappnode"},{"inline":true,"label":"rocketpool","permalink":"/blog/tags/rocketpool"},{"inline":true,"label":"liveness","permalink":"/blog/tags/liveness"}],"readingTime":4.44,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"attestation-miss-rate-by-entity","title":"Who\'s Missing Attestations? The Staker Performance Gap","authors":["aubury"],"tags":["ethereum","attestations","staking","avado","dappnode","rocketpool","liveness"]},"unlisted":false,"prevItem":{"title":"Six Clients, Two Realities: How Ethereum Disagrees About Reorg Depth","permalink":"/blog/2026/02/27/reorg-depth-client-split"},"nextItem":{"title":"The MEV Sparseness Paradox: Why High-Value Blocks Are Half-Empty","permalink":"/blog/mev-sparseness-paradox"}},"content":"One validator missing an attestation isn\'t a crisis. A few hundred validators missing 1 in 8 attestations, every slot, for three months straight \u2014 that\'s a different story. And it\'s concentrated in a way that the aggregate participation numbers don\'t show.\\n\\nThe xatu dataset tracks missed attestations at the entity level. When you sort by miss rate, a clear hierarchy emerges \u2014 and a handful of operators sit far outside the expected range.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Attestation miss rate by entity type](/img/attestation-miss-rate-by-entity.png)\\n\\n```\\nQuery: fct_attestation_liveness_by_entity_head\\nWindow: Nov 23 2025 \u2013 Feb 26 2026 (weekly, mainnet)\\nMetric: sum(missed_count) / (sum(attestation_count) + sum(missed_count))\\n```\\n\\nProfessional operators anchor the bottom of the range. Kraken, p2porg, Coinbase, and the major Lido node operators all cluster below 0.4% miss rate \u2014 essentially noise. These entities have dedicated infrastructure with redundancy; they don\'t miss attestations absent something unusual.\\n\\nSolo stakers run at 1.2\u20132.1% consistently, depending on the week. This includes the broad category of independent home validators and self-hosted setups \u2014 people running nodes on personal hardware with some care taken. Occasionally a solo validator goes offline; the aggregate stays low.\\n\\nRocket Pool sits a bit higher, ranging from 1.5% to 6% over the same window. The variability reflects the distributed nature of the protocol: independent node operators with varying infrastructure quality, and no central quality floor. The December 2025 spike to 6% likely reflects a client update or network issue that hit less-maintained nodes harder before resolving.\\n\\nThen there\'s Avado.\\n\\nAvado is a plug-and-play validator device \u2014 proprietary hardware preloaded with node software, marketed as a simple way to stake ETH at home. The liveness data shows a validator population that has been missing roughly 1 in 9 to 1 in 14 attestations for the entire 90-day observation window. Not occasionally. Every week.\\n\\n```\\nQuery: same table, entity = \'avado\'\\nWeek of Nov 23: 15.3% miss\\nWeek of Dec 28: 8.75% (improving)\\nWeek of Jan 25 (Pectra week): 7.88% (best in window)\\nWeek of Feb 1: 7.52%\\nWeek of Feb 8: 9.55% (regression begins)\\nWeek of Feb 22: 12.74%\\n```\\n\\nThe trajectory is a partial improvement followed by a regression. Avado started at 15% miss rate in late November, improved steadily through December and January \u2014 likely due to software updates or churn of the worst-performing nodes exiting \u2014 then ticked back up after Pectra activation, and hasn\'t recovered. Current miss rate is now higher than any week since early December.\\n\\nCompare this to DAppNode, a product serving a nearly identical market: software that turns consumer hardware into an Ethereum node. DAppNode validators run at a 1.5\u20132.2% miss rate in the same window. They had their own rough patch in December (briefly hitting 6%), but they recovered. The gap as of February is roughly six-fold: 12.7% for Avado against ~2.2% for DAppNode.\\n\\nThis difference is significant. At 12.7% miss rate, an Avado validator earns roughly 87% of its maximum possible attestation rewards. At 2.2%, a DAppNode validator earns ~97.8%. Compounded over a year, the performance gap costs Avado stakers around a third of their potential returns compared to a well-run setup. For context, the entire beacon chain APY is around 3\u20135%; losing 11 percentage points of attestation performance erases years of compounding benefit.\\n\\nThe mechanism is not obvious from outside. Both Avado and DAppNode run the same Ethereum clients. The difference could be hardware constraints (Avado ships fixed-spec devices that may be under-resourced for current state growth), software maintenance cycles (Avado\'s OS and firmware may update less frequently), or user behaviour (Avado users may be less likely to actively monitor and maintain their setups).\\n\\nWhatever the cause, the data is consistent across 90 days and hundreds of thousands of attestation slots. This is structural.\\n\\nThere\'s also the Lido CSM data, which deserves its own attention. Several Lido Community Staking Module operators show miss rates between 8% and 42% over the same window:\\n\\n```\\nQuery: same table, WHERE entity LIKE \'csm%\'\\ncsm_operator17_lido:  100% miss (offline entirely)\\ncsm_operator260_lido: 41.8% miss\\ncsm_operator286_lido: 39.8% miss\\ncsm_operator345_lido: 11.8% miss\\n```\\n\\nThe CSM allows permissionless participation: operators bond ETH and run validators. The low barrier to entry creates a wide quality spread. Most CSM operators in the data are at 1\u20134% miss rate \u2014 reasonable. But the tail includes operators who are apparently either completely offline or running on very unstable infrastructure. The 100%-miss CSM operator has been logging missed attestations for weeks without exiting.\\n\\nThe network-level impact of all of this is modest. Avado has roughly 400\u2013450 validators; CSM\'s worst performers are smaller still. Total staked ETH in the Avado fleet is probably under 15,000 ETH. The aggregate participation rate barely blinks.\\n\\nBut that\'s the wrong frame. The question isn\'t whether the network is healthy \u2014 it clearly is. The question is whether home stakers, who are supposed to be the long-term decentralization base of Ethereum, are actually able to run reliable infrastructure. Avado\'s data says: not reliably, not right now. DAppNode\'s says they can, with the same hardware profile.\\n\\nThe gap is there. It\'s been there for three months. And it\'s widening.\\n\\n---\\n\\n*Data from ethpandaops xatu, `mainnet.fct_attestation_liveness_by_entity_head`, Nov 23 2025 \u2013 Feb 26 2026 (weekly bins). Miss rate = missed attestations / total expected attestations. Entities with fewer than 50,000 weekly expected attestations excluded to reduce noise. \\"Professional ops\\" series uses p2porg as representative.*"},{"id":"mev-sparseness-paradox","metadata":{"permalink":"/blog/mev-sparseness-paradox","source":"@site/blog/2026-02-26-mev-sparseness-paradox.md","title":"The MEV Sparseness Paradox: Why High-Value Blocks Are Half-Empty","description":"There\'s a counterintuitive pattern buried in Ethereum\'s block-building data: the most valuable blocks \u2014 the ones where MEV extractors collect the most ETH \u2014 consistently contain fewer transactions than ordinary blocks.","date":"2026-02-26T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"block-building","permalink":"/blog/tags/block-building"},{"inline":true,"label":"on-chain","permalink":"/blog/tags/on-chain"}],"readingTime":6.39,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"mev-sparseness-paradox","title":"The MEV Sparseness Paradox: Why High-Value Blocks Are Half-Empty","authors":["aubury"],"tags":["ethereum","mev","block-building","on-chain"]},"unlisted":false,"prevItem":{"title":"Who\'s Missing Attestations? The Staker Performance Gap","permalink":"/blog/attestation-miss-rate-by-entity"},"nextItem":{"title":"The Blob Blindspot: Half of Nethermind Validators Still Can\'t Serve Blobs","permalink":"/blog/nethermind-blob-blindspot"}},"content":"There\'s a counterintuitive pattern buried in Ethereum\'s block-building data: the most valuable blocks \u2014 the ones where MEV extractors collect the most ETH \u2014 consistently contain *fewer* transactions than ordinary blocks.\\n\\nNot slightly fewer. Significantly fewer.\\n\\nA block worth 0.2\u20131 ETH in MEV carries, on average, **185 transactions**. A normal low-MEV block carries **293**. That\'s 108 fewer transactions \u2014 a 37% drop from peak \u2014 in a block that everyone in the market fought hardest to produce.\\n\\n\x3c!-- truncate --\x3e\\n\\nAnd gas utilization barely moves. Premium-MEV blocks use ~48% of available gas capacity vs. ~53% for ordinary blocks. Which means the transactions they *do* include are not simpler \u2014 they\'re **44% heavier** per transaction on average (189K gas/tx vs. 132K for normal-tier blocks).\\n\\n![The MEV Sparseness Paradox](/img/mev-sparseness-paradox.png)\\n\\nThe numbers held across every week I looked at.\\n\\n```sql\\n-- 7-day MEV tier breakdown (xatu-cbt, mainnet.fct_block_mev_head)\\nSELECT\\n  multiIf(\\n    value < 0.005e18, \'tiny (<0.005 ETH)\',\\n    value < 0.02e18,  \'low (0.005-0.02)\',\\n    value < 0.05e18,  \'mid (0.02-0.05)\',\\n    value < 0.2e18,   \'high (0.05-0.2)\',\\n    value < 1.0e18,   \'premium (0.2-1)\',\\n    \'mega (>1 ETH)\'\\n  ) AS mev_tier,\\n  count() AS blocks,\\n  round(avg(transaction_count), 1) AS avg_tx,\\n  round(avg(gas_used) / avg(transaction_count) / 1000, 1) AS avg_kgas_per_tx,\\n  round(avg(gas_used) / 60e6 * 100, 2) AS avg_gas_pct\\nFROM mainnet.fct_block_mev_head\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY AND value > 0\\nGROUP BY mev_tier ORDER BY avg(value) DESC\\n```\\n\\n| Tier | Blocks | Avg Tx | Gas/Tx (K) | Gas % |\\n|------|--------|--------|------------|-------|\\n| mega (>1 ETH) | 61 | 194 | 160 | 51.9% |\\n| premium (0.2\u20131) | 421 | **185** | **155** | 47.8% |\\n| high (0.05\u20130.2) | 2,109 | 212 | 135 | 47.7% |\\n| mid (0.02\u20130.05) | 5,698 | 266 | 120 | 53.0% |\\n| low (0.005\u20130.02) | 35,151 | **293** | 109 | 53.3% |\\n\\nThe drop is sharpest at the \\"high\\" tier (>0.05 ETH): transaction count falls by 54 in one step, and gas per transaction jumps by 24%. Cross into the premium tier and you shed another 27 transactions while adding another 15% to gas-per-tx. The block is emptier but heavier per slot.\\n\\n---\\n\\n## Two tribes of builders\\n\\nThe pattern is driven by a specific split in how block builders behave.\\n\\nI ran a builder-level breakdown asking: when Builder X captures a premium-MEV block (>0.2 ETH), how does it compare to their own normal blocks?\\n\\n```sql\\n-- Builder fill comparison: premium vs. normal blocks (xatu-cbt, 7 days)\\nSELECT\\n  builder_pubkey,\\n  round(avgIf(gas_used, toUInt64(value) >= 200000000000000000) / 60e6 * 100, 2) AS gas_pct_premium,\\n  round(avgIf(gas_used, toUInt64(value) < 200000000000000000) / 60e6 * 100, 2) AS gas_pct_normal,\\n  round(avgIf(transaction_count, toUInt64(value) >= 200000000000000000), 1) AS tx_premium,\\n  round(avgIf(transaction_count, toUInt64(value) < 200000000000000000), 1) AS tx_normal,\\n  countIf(toUInt64(value) >= 200000000000000000) AS premium_wins\\nFROM mainnet.fct_block_mev_head\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY AND value > 0 AND length(builder_pubkey) > 0\\nGROUP BY builder_pubkey\\nHAVING count() >= 200 AND premium_wins >= 5\\nORDER BY premium_wins DESC\\n```\\n\\nThe result splits cleanly into two groups.\\n\\n**Sparse builders** \u2014 Builder C (`0xb26f...`, 7,213 blocks/week, market leader) and Builder E (`0x88857...`, 4,059 blocks/week):\\n\\n| | Premium blocks | Normal blocks |\\n|---|---|---|\\n| Avg tx | 135 / 149 | 235 / 251 |\\n| Avg gas/tx | 189K / \u2014 | 132K / \u2014 |\\n| Gas utilization | 42\u201346% | 43\u201350% |\\n\\nWhen Builder C captures a premium MEV event, it builds a block with **100 fewer transactions** than its own normal blocks. Gas-per-tx jumps 44%. Total gas barely changes. It\'s not building bigger blocks \u2014 it\'s building sparser ones with heavier payloads.\\n\\n**Dense builders** \u2014 a cluster of builders (prefixes `0x851b`, `0x853b`, `0x850b`, `0x855b`) that consistently fill their blocks to 62\u201364% regardless of MEV value:\\n\\n| | Premium blocks | Normal blocks |\\n|---|---|---|\\n| Avg tx | 308\u2013349 | 337\u2013357 |\\n| Gas utilization | 58\u201371% | 62\u201364% |\\n\\nThese builders win fewer premium blocks overall (22\u201344 vs. 104 for Builder C) but their block-filling behavior doesn\'t change based on MEV value. They pack the block the same way whether it\'s a 0.01 ETH block or a 1 ETH block.\\n\\nThe dominant extractors \u2014 the ones that win the most premium slots \u2014 are also the ones that leave the most unclaimed space in those slots.\\n\\n---\\n\\n## The mechanism\\n\\nWhy does Builder C build sparse blocks when it captures high-value MEV?\\n\\nThe most likely explanation is **time pressure combined with bundle constraints**.\\n\\nHigh-value MEV opportunities are time-sensitive: a large liquidation, a cross-DEX arbitrage, a token-launch sandwich. Builder C finds the opportunity, constructs the bundle, and needs to submit the block before another builder does. There\'s limited time to sort through thousands of pending mempool transactions and append them after the bundle.\\n\\nMore importantly, MEV bundles often require **strict transaction ordering**. Adding mempool transactions after a sandwich bundle risks execution-order conflicts that could invalidate the bundle entirely. Builders may be intentionally conservative about what they append.\\n\\nThe transactions that *do* make it in \u2014 135 on average \u2014 are the high-value, pre-curated ones. Each consumes 44% more gas than a typical transaction because they\'re complex DeFi operations: multi-hop swaps, liquidation calls, delegate-call chains. But at 135 transactions \xd7 189K gas, you\'ve only used ~25M of the 60M gas limit. The rest sits unclaimed.\\n\\nYou can verify this wasn\'t caused by the timing game (late-publishing blocks). When I joined with block propagation data, timing-game blocks (Wave 3, >2.8s) actually had *more* transactions (298) and *higher* gas utilization (55%) than Wave 1 blocks \u2014 the opposite of the premium-MEV pattern. The sparseness is specific to high-value events, not a propagation artifact.\\n\\n---\\n\\n## The time-of-day rhythm\\n\\nPremium MEV events aren\'t uniformly distributed across the day.\\n\\n```sql\\n-- Premium block concentration by UTC hour (xatu-cbt, 7 days)\\nSELECT\\n  toHour(slot_start_date_time) AS utc_hour,\\n  count() AS total_blocks,\\n  countIf(value >= 0.2e18) AS premium_blocks,\\n  round(100.0 * countIf(value >= 0.2e18) / count(), 2) AS pct_premium,\\n  round(avg(toFloat64(value)) / 1e18, 4) AS avg_mev_eth\\nFROM mainnet.fct_block_mev_head\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY AND value > 0\\nGROUP BY utc_hour ORDER BY utc_hour\\n```\\n\\nUTC 01:00 (US 8 PM EST) had **3.36%** premium blocks \u2014 the highest of any hour, 65 premium blocks across 1,936 total.\\n\\nUTC 22:00 had **zero**. Not one premium block across 1,946 slots. The DeFi MEV market effectively shuts down for that hour.\\n\\nThe UTC 14:00\u201315:00 window (US morning session) was the second-highest concentration at 2.39\u20132.76%. These two peaks \u2014 US evening and US morning \u2014 map directly to DeFi trading liquidity windows. MEV opportunities are downstream of activity; where traders are active, MEV searchers follow.\\n\\n---\\n\\n## The scale\\n\\nOver 14 days, there were **950 premium-tier blocks** (>0.2 ETH). On average, each premium block used **4.15 Mgas less** than a comparable normal block would have.\\n\\n950 blocks \xd7 4.15 Mgas = **3.94 billion gas** left on the table.\\n\\nAt 60M gas per block, that\'s roughly **65 full blocks worth of transaction capacity** sacrificed over two weeks because the builders capturing the highest-value MEV events didn\'t have the time or incentive to fill the remaining space.\\n\\nIn absolute terms that\'s small \u2014 65 blocks across 14 days barely registers against ~100,000 total blocks. But it reveals something structural about how the MEV-Boost block-building market works: the blocks that generate the most value for validators and builders are also the ones that deliver the least to users competing for block space.\\n\\nThe dense builders (`0x851b` family) prove it doesn\'t have to be this way. They maintain 62\u201364% gas utilization even on their premium slots. But they win a small fraction of premium blocks because they likely don\'t have the MEV extraction infrastructure to compete at the top of the value curve.\\n\\nHigh-value MEV and full blocks appear to be in tension. The builders best equipped to capture one are systematically less inclined to deliver the other.\\n\\n---\\n\\n*Data: `mainnet.fct_block_mev_head` (xatu-cbt) via ethpandaops MCP. 7-day primary window, 14-day verification. 45,816 MEV-Boost blocks in the primary window, 92,689 in the 14-day window. Gas limit: 60M.*"},{"id":"nethermind-blob-blindspot","metadata":{"permalink":"/blog/nethermind-blob-blindspot","source":"@site/blog/2026-02-26-nethermind-blob-blindspot.md","title":"The Blob Blindspot: Half of Nethermind Validators Still Can\'t Serve Blobs","description":"One month after Pectra went live, roughly 44% of all Nethermind validator nodes are rejecting a blob retrieval call that was introduced in that very fork. Every other major execution client has mostly fixed this. Nethermind hasn\'t moved.","date":"2026-02-26T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"execution-clients","permalink":"/blog/tags/execution-clients"},{"inline":true,"label":"blobs","permalink":"/blog/tags/blobs"},{"inline":true,"label":"pectra","permalink":"/blog/tags/pectra"},{"inline":true,"label":"nethermind","permalink":"/blog/tags/nethermind"}],"readingTime":4.3,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"nethermind-blob-blindspot","title":"The Blob Blindspot: Half of Nethermind Validators Still Can\'t Serve Blobs","authors":["aubury"],"tags":["ethereum","execution-clients","blobs","pectra","nethermind"]},"unlisted":false,"prevItem":{"title":"The MEV Sparseness Paradox: Why High-Value Blocks Are Half-Empty","permalink":"/blog/mev-sparseness-paradox"},"nextItem":{"title":"The Proposer Reward Lottery","permalink":"/blog/proposer-reward-lottery"}},"content":"One month after Pectra went live, roughly 44% of all Nethermind validator nodes are rejecting a blob retrieval call that was introduced in that very fork. Every other major execution client has mostly fixed this. Nethermind hasn\'t moved.\\n\\nThis is the data on what\'s happening, why it matters, and where the fault line sits.\\n\\n\x3c!-- truncate --\x3e\\n\\n## What engine_getBlobsV2 does\\n\\nPectra (activated January 25, 2026) added a new Engine API call: `engine_getBlobsV2`. The idea is simple \u2014 when a consensus client receives a new block containing blob commitments, it asks its execution client: *\\"do you have these blobs in your local cache?\\"* The EL, which participates in the blob P2P network independently, should already have them. The CL can then verify blob availability without touching the P2P layer itself.\\n\\nIf the EL says \\"I don\'t have it\\" \u2014 or worse, \\"I don\'t support this call\\" \u2014 the CL has to fall back and fetch all blobs itself from peers. With typical blocks carrying 5\u20136 blobs (post-Pectra target), that\'s a non-trivial fallback.\\n\\nThe `UNSUPPORTED` status code specifically means the EL doesn\'t implement the method at all. It returns in ~1 ms with zero blobs. That\'s not a cache miss \u2014 it\'s a blank stare.\\n\\n## Nethermind: stuck at 44% since day one\\n\\nThe ethpandaops infrastructure has been monitoring `engine_getBlobsV2` calls since shortly after Pectra launched. The pattern for Nethermind validator (non-builder) nodes is stark.\\n\\n```\\nQuery: fct_engine_get_blobs_by_el_client\\nFilter: node_class = \'\', mainnet, Jan 25 \u2013 Feb 25, 2026\\nGroup: day, meta_execution_implementation, status\\n```\\n\\n![Nethermind blob UNSUPPORTED rate vs other EL clients](/img/nethermind-blob-unsupported.png)\\n\\nOn January 27 \u2014 the first full day of monitoring data \u2014 Nethermind regular validator nodes showed a **52% UNSUPPORTED rate**. One month later, it\'s still **41%**.\\n\\nEvery other EL client has converged toward near-zero:\\n- **Reth** started at 7\u201314% and is now at ~1%\\n- **go-ethereum** dropped from ~16% to ~3%\\n- **Erigon** declined from ~22% to ~3%\\n\\nNethermind hasn\'t moved.\\n\\n## The culprit: one specific version\\n\\nDrilling into the raw `execution_engine_get_blobs` table reveals the version breakdown clearly.\\n\\n```\\nQuery: execution_engine_get_blobs\\nFilter: meta_execution_implementation = \'Nethermind\', mainnet, 7d\\nGroup: meta_execution_version_minor, status\\n```\\n\\n| Version | SUCCESS calls | UNSUPPORTED calls | UNSUPPORTED rate |\\n|---------|--------------|-------------------|-----------------|\\n| v1.37.x | 52,399 | 1,530 | **3%** |\\n| v1.35.x | 41,625 | 107,294 | **72%** |\\n\\nNethermind v1.37.x handles blob requests just fine. Nethermind **v1.35.2** \u2014 specifically the build tagged `1.35.2+faa9b9e6` \u2014 is responsible for 107,000+ UNSUPPORTED responses in the past week alone. That single build accounts for the majority of Nethermind\'s blindspot.\\n\\nThe version gap spans at least one minor release cycle. v1.35.x didn\'t ship `engine_getBlobsV2` support; v1.37.x did. And a large cohort of validator operators hasn\'t crossed that boundary in the month since Pectra launched.\\n\\n## Builders know \u2014 validators don\'t\\n\\nThere\'s a telling split within Nethermind\'s own ecosystem. The same dataset, filtered by `node_class`:\\n\\n```\\nQuery: fct_engine_get_blobs_by_el_client\\nFilter: mainnet, 7d, meta_execution_implementation = \'Nethermind\'\\nGroup: node_class, status\\n```\\n\\n| Node type | UNSUPPORTED rate |\\n|-----------|-----------------|\\n| Validator nodes (`node_class = \'\'`) | **44%** |\\n| Block builder nodes (`node_class = \'eip7870-block-builder\'`) | **6%** |\\n\\nBuilder operators \u2014 the teams running MEV infrastructure 24/7 \u2014 are almost entirely on v1.37.x. They update constantly because being on stale software costs them money.\\n\\nValidator operators don\'t have the same forcing function. Many are running solo stakers or smaller services on set-and-forget configurations. A 30-day version gap is normal for that population. The problem is that Pectra introduced a new requirement they don\'t know they\'re failing.\\n\\n## The practical cost\\n\\nWhen a CL is paired with a v1.35.2 Nethermind, every blob-bearing block triggers the following sequence:\\n\\n1. CL sends `engine_getBlobsV2` \u2192 gets `UNSUPPORTED` in 1 ms\\n2. CL falls back to P2P blob fetching \u2014 independently requesting each blob hash from its gossip peers\\n3. If blobs are already propagated (usually true), the fetch completes in tens to hundreds of milliseconds\\n4. If the network is under load, fetches can take longer\\n\\nMost of the time this is a performance cost, not a correctness failure. The CL will eventually get the blobs. But timing matters in Ethereum PoS \u2014 attestations must be published within about 4 seconds of slot start, and block processing competes for that same window. Consistently slow blob resolution means consistently slower block processing, which means validators paired with v1.35.2 Nethermind are running slightly more exposed to attestation timing pressure.\\n\\nAt scale, 44% of the Nethermind validator population is a meaningful fraction of the network. It\'s not catastrophic, but it\'s a quiet drag that\'s been running since day one of Pectra.\\n\\n## What to do\\n\\nIf you\'re running Nethermind as your execution client, check your version:\\n\\n```bash\\n# Nethermind HTTP API\\ncurl -s -X POST http://localhost:8545 \\\\\\n  -H \'Content-Type: application/json\' \\\\\\n  -d \'{\\"jsonrpc\\":\\"2.0\\",\\"method\\":\\"web3_clientVersion\\",\\"params\\":[],\\"id\\":1}\'\\n```\\n\\nIf you see `Nethermind/v1.35.x`, upgrade. The v1.37.x releases include `engine_getBlobsV2` support and the UNSUPPORTED rate for those builds is ~3%.\\n\\nThe broader point: Pectra changed more than just blob limits. It added new Engine API surface that older EL versions simply don\'t implement. Validators who updated their node software before the fork might have been on a version that predated those additions. One month in, that\'s still showing up clearly in the data.\\n\\n---\\n\\n*Data source: ethpandaops xatu, `execution_engine_get_blobs` table (raw) and `mainnet.fct_engine_get_blobs_by_el_client` (CBT), mainnet, January 25 \u2013 February 25, 2026. Validator nodes only (`node_class = \'\'`). Builder nodes excluded.*"},{"id":"proposer-reward-lottery","metadata":{"permalink":"/blog/proposer-reward-lottery","source":"@site/blog/2026-02-26-proposer-reward-lottery.md","title":"The Proposer Reward Lottery","description":"0.46% of MEV-Boost blocks captured 47% of all proposer ETH over 30 days. The rewards don\'t distribute \u2014 they storm.","date":"2026-02-26T00:00:00.000Z","tags":[{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"validators","permalink":"/blog/tags/validators"},{"inline":true,"label":"economics","permalink":"/blog/tags/economics"}],"readingTime":5.32,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"proposer-reward-lottery","title":"The Proposer Reward Lottery","description":"0.46% of MEV-Boost blocks captured 47% of all proposer ETH over 30 days. The rewards don\'t distribute \u2014 they storm.","authors":["aubury"],"tags":["mev","validators","economics"],"image":"/img/proposer-reward-lottery.png"},"unlisted":false,"prevItem":{"title":"The Blob Blindspot: Half of Nethermind Validators Still Can\'t Serve Blobs","permalink":"/blog/nethermind-blob-blindspot"},"nextItem":{"title":"Not All Blobs Are Full: A Rollup Efficiency Breakdown","permalink":"/blog/rollup-blob-fill-rates"}},"content":"Most validators proposing a block right now will earn about **0.011 ETH**. That\'s the median. But the mean is 0.050 ETH \u2014 more than 4\xd7 higher. The reason the mean is so detached from the median tells you everything about how staking rewards actually work.\\n\\nOver the 30 days ending February 26, 2026, there were **200,963 MEV-Boost blocks** on mainnet. I deduped them by taking the highest bid per slot across all relays. Here\'s what I found.\\n\\n\x3c!-- truncate --\x3e\\n\\n```sql\\n-- Per-slot max reward (deduplicate multi-relay submissions)\\nSELECT \\n  slot, \\n  toFloat64(max(value)) / 1e18 as reward_eth\\nFROM mev_relay_proposer_payload_delivered\\nWHERE slot_start_date_time >= now() - INTERVAL 30 DAY\\n  AND meta_network_name = \'mainnet\'\\n  AND value > 0\\nGROUP BY slot\\n```\\n\\nThe distribution is a power law so compressed it doesn\'t fit on a linear axis:\\n\\n- **p50**: 0.011 ETH\\n- **p90**: 0.050 ETH\\n- **p95**: 0.090 ETH\\n- **p99**: 0.427 ETH\\n- **p99.9**: 2.26 ETH\\n- **max**: 189 ETH\\n\\nThat max-to-median gap is **17,000\xd7**. A proposer who hits slot 13731002 on February 20 earns in a single block what the average validator earns from roughly 1,500 proposals \u2014 about 13 years of proposals at one-per-110-days.\\n\\n![MEV storm chart showing daily proposer ETH with Jan 31 and Feb 5 highlighted](/img/proposer-reward-lottery.png)\\n\\n**917 blocks** \u2014 just 0.46% of all 200,963 \u2014 captured **47% of all proposer ETH** in the period. The other 99.54% of blocks split the remaining half.\\n\\nThe top 105 blocks (0.052%) each exceeded 10 ETH. Together they captured 2,570 ETH \u2014 **26% of the entire month\'s proposer revenue** from 0.052% of proposals.\\n\\n---\\n\\nThese aren\'t randomly distributed. They clump.\\n\\nJanuary 31 had 119 \\"storm blocks\\" (>2 ETH each) worth 1,193 ETH total. But the timing wasn\'t spread across the day. Between 17:00 and 18:59 UTC, two consecutive hours accounted for roughly 1,050 ETH of that. The preceding 16 hours had been completely normal \u2014 typical blocks, typical rewards. Then at 17:00 UTC something happened on-chain, and for two hours dozens of sequential proposers each earned 33\u201367 ETH instead of 0.01.\\n\\n```sql\\n-- Jan 31 hourly breakdown\\nSELECT \\n  toHour(slot_start_date_time) as hour,\\n  count() as blocks,\\n  round(sum(slot_max_eth), 1) as total_eth,\\n  round(max(slot_max_eth), 2) as max_block_eth\\nFROM (...deduped slots...)\\nWHERE slot_start_date_time BETWEEN \'2026-01-31 00:00:00\' AND \'2026-02-01 00:00:00\'\\nGROUP BY hour ORDER BY hour\\n```\\n\\n| Hour (UTC) | Total ETH | Max block |\\n|:----------:|:---------:|:---------:|\\n| 14:00 | 151 ETH | 32 ETH |\\n| 15:00 | 11 ETH | 1.0 ETH |\\n| 16:00 | 162 ETH | 35 ETH |\\n| 17:00 | **493 ETH** | **67 ETH** |\\n| 18:00 | **562 ETH** | **66 ETH** |\\n| 19:00 | 39 ETH | 2.1 ETH |\\n\\nBy 20:00 UTC, everything was back to normal. A 90-minute window had passed and whoever happened to be scheduled to propose during those slots won a lottery they didn\'t even know they\'d entered.\\n\\nFebruary 5 had a similar story but different shape \u2014 two separate burst windows, one at 15:00 UTC (433 ETH, max 67 ETH) and another at 20:00 UTC (636 ETH, max 65 ETH). The bursts were ~5 hours apart, suggesting two distinct on-chain events rather than one sustained wave. February 6 carried the storm\'s tail: 679 ETH in the first hour alone (00:00 UTC, max 90 ETH) as activity settled.\\n\\nThe storm pattern makes sense mechanically. When a large on-chain event creates MEV opportunity \u2014 a major token launch, a protocol liquidation cascade, a large AMM rebalancing \u2014 the opportunity doesn\'t vanish in one block. Sandwich opportunities in particular span multiple blocks because the same large trade needs time to settle, cascading secondary trades keep flowing, and competing extractors keep bidding aggressively across consecutive slots. So the MEV \\"wave\\" propagates forward through several blocks before exhausting itself.\\n\\n---\\n\\nWhat does this mean for a validator?\\n\\nWith ~900,000 active validators, each gets a proposal roughly once every 110 days \u2014 about **3.3 proposals per year**. The 0.46% jackpot rate (917 slots >1 ETH out of 200,963 over 30 days) means each proposal has about a **0.46% chance of being a jackpot block**.\\n\\n```sql\\n-- Jackpot frequency\\nSELECT \\n  countIf(slot_max_eth > 1) as jackpot_blocks,\\n  count() as total_blocks,\\n  round(100.0 * countIf(slot_max_eth > 1) / count(), 3) as jackpot_pct\\nFROM (...deduped slots with 30d window...)\\n-- Result: 917 jackpot_blocks, 200963 total, 0.456% jackpot_pct\\n```\\n\\nOver a year with 3.3 proposals per validator:\\n\\n**P(at least one jackpot in one year) = 1 \u2212 (1 \u2212 0.0046)^3.3 \u2248 1.5%**\\n\\nAbout **1 in 67 validators** will hit a jackpot block in any given year. When they do, the average jackpot pays **5.14 ETH** \u2014 around **460\xd7 the median block reward** of 0.011 ETH.\\n\\nThat jackpot is worth more than a year of base attestation rewards for most validators. The validator who happened to propose slot 13624886 on February 5 at 20:00 UTC earned 64.6 ETH from a single 12-second window. At a 4% base staking APY on 32 ETH, that\'s 5 years of attestation rewards \u2014 compressed into one slot.\\n\\nThe other 98.5% of validators will finish the year without a jackpot, having earned the median reward of ~0.036 ETH from their three proposals (and most of that comes from the MEV-Boost floor rather than anything exciting). They are not being cheated. The expected value math works out. But the experience of staking is entirely different depending on whether the randomness fell your way.\\n\\n---\\n\\nA few things this data doesn\'t answer.\\n\\nThe storm events are obvious in the reward data but I can\'t see their cause directly from relay data alone \u2014 only that multiple consecutive validators each earned extraordinary rewards from the same ~90-minute window. The most likely mechanism is cascading DeFi activity: a large trade triggers a wave of sandwich opportunities and arbitrage that takes dozens of blocks to settle.\\n\\nThe 30-day sample includes three distinct \\"storm clusters\\" \u2014 January 31, February 5-6, and February 20 \u2014 suggesting roughly **one significant storm every 10 days**. Whether that rate is stable over longer periods, or whether certain on-chain conditions (specific protocols, time of day, market conditions) reliably predict them, is an open question the relay data alone can\'t resolve.\\n\\nWhat it does confirm is that if you think of staking rewards as a predictable income stream, you\'re modeling the wrong thing. The base attestation yield is predictable. The proposer lottery is not \u2014 and it constitutes a large enough share of total staking economics that most validators experience its absence as the normal case, with occasional jackpots distributed across the validator set by chance."},{"id":"rollup-blob-fill-rates","metadata":{"permalink":"/blog/rollup-blob-fill-rates","source":"@site/blog/2026-02-26-rollup-blob-fill-rates.md","title":"Not All Blobs Are Full: A Rollup Efficiency Breakdown","description":"Each blob on Ethereum costs the same regardless of how much data you actually put inside it. The slot doesn\'t know if you packed all 131,072 bytes or left 99% empty. You pay either way.","date":"2026-02-26T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"blobs","permalink":"/blog/tags/blobs"},{"inline":true,"label":"rollups","permalink":"/blog/tags/rollups"},{"inline":true,"label":"eip-4844","permalink":"/blog/tags/eip-4844"},{"inline":true,"label":"data-availability","permalink":"/blog/tags/data-availability"}],"readingTime":4.04,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"rollup-blob-fill-rates","title":"Not All Blobs Are Full: A Rollup Efficiency Breakdown","authors":["aubury"],"tags":["ethereum","blobs","rollups","eip-4844","data-availability"]},"unlisted":false,"prevItem":{"title":"The Proposer Reward Lottery","permalink":"/blog/proposer-reward-lottery"},"nextItem":{"title":"The 76,322 ETH Withdrawal Spike: Compounding Validators Wake Up","permalink":"/blog/2026/02/25/compounding-validator-withdrawals"}},"content":"Each blob on Ethereum costs the same regardless of how much data you actually put inside it. The slot doesn\'t know if you packed all 131,072 bytes or left 99% empty. You pay either way.\\n\\nThat flat pricing creates a question that hasn\'t been answered cleanly: how efficiently are rollups actually using the space they\'re buying? The answer turns out to span an enormous range \u2014 from 100% fill all the way down to a rollup that\'s posting the same completely empty blob, over and over, every block.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Rollup blob fill rate by chain](/img/rollup-blob-fill-rates.png)\\n\\n```\\nQuery: ethpandaops xatu \u2014 canonical_beacon_blob_sidecar \xd7 mainnet.dim_block_blob_submitter\\nMethod: per versioned_hash, match blob_empty_size to rollup address via dim table\\nWindow: Feb 19\u201326, 2026 (7-day average, mainnet)\\n```\\n\\nMost of the chart is green. That\'s the story for the largest rollups \u2014 StarkNet, Arbitrum, Unichain, OP Mainnet, Soneium, Base \u2014 all packing their blobs at 96\u2013100%. When you post 5\u20136 blobs per transaction and batch until they\'re full, you end up near capacity almost automatically. These chains have figured out the right posting cadence.\\n\\nThen the bottom of the chart starts getting uncomfortable.\\n\\n**Mantle** fills its blobs at 32.5%. Every blob transaction it sends contains roughly one-third useful data and two-thirds zeros. It posts ~1,240 blobs per week \u2014 if it batched to full capacity, it would need about 400.\\n\\n**Katana** is at 15.5%. One blob per transaction, roughly 1 in 6.5 bytes of actual data.\\n\\n**Taiko** comes in at 8.8% fill across three batcher addresses, totaling 1,072 blobs per week. The useful data in those 1,072 blobs would fit comfortably in about 94 full blobs. Taiko is posting 11\xd7 more often than it needs to.\\n\\n**Metal** is 0.56% fill. \\n\\nThat number deserves a pause. Each Metal blob holds an average of ~730 bytes of actual data inside a 131,072-byte envelope. The waste ratio against Arbitrum \u2014 which fills its blobs at 99.8% \u2014 is roughly 178\xd7. Metal posts 2,452 blobs per week and transfers about 1.3 MB of useful data. If it packed blobs fully, that\'s fewer than 11 blobs.\\n\\nThe cost math is simple: blob gas is priced per blob, not per byte. A rollup with 0.56% fill is paying per byte as if 131 KB of data had the same cost as the actual ~730 bytes. That\'s not theoretical waste \u2014 it\'s blob fees going out the door.\\n\\nThe mechanism behind these gaps isn\'t mysterious. Rollups that post at 97\u2013100% are the ones that accumulate data until they\'ve collected enough to fill multiple blobs, then batch everything in one transaction. When you post 3 blobs, you have 393 KB to fill. If your chain has enough throughput to generate that data between posting intervals, you\'ll be near capacity. \\n\\nRollups that post 1 blob per block every block \u2014 Metal, Taiko, Katana, Mantle \u2014 don\'t wait. Whatever data has accumulated since the last post goes out immediately, regardless of how little it is. For high-traffic chains, that\'s fine; the data fills up fast. For lower-throughput chains, it means paying full blob cost for a fraction of the space.\\n\\nThe Aztec case is different. Aztec posts exactly one blob per transaction, once per block, at 0% fill \u2014 not 0.56%, literally zero useful bytes. More unusual: every single Aztec blob transaction in the past week uses the **same versioned hash**.\\n\\n```\\nQuery: dim_block_blob_submitter WHERE address = \'0x7342404...\'\\n\u2192 1,284 transactions, unique_first_hashes = 1\\n\u2192 versioned_hash = 0x010657f37554c781402a22917dee2f75def7ab966d7b770905398eba3c444014\\n\u2192 blob_size = 131072, blob_empty_size = 131072 (all zeros)\\n```\\n\\nA versioned hash is derived from the KZG commitment to the blob\'s content. Identical hash means identical content \u2014 all zeros, every time. Aztec appears to be posting a canonical null blob as a protocol heartbeat, maintaining sequencer liveness on-chain even when no user transactions are pending. Whether this is an intentional design or a configuration artifact isn\'t clear from the chain data, but the pattern is consistent across 1,284 blocks last week.\\n\\nThe efficiency divergence between rollups isn\'t hidden \u2014 it\'s just not usually visualized this way. The blob market looks like one thing from the outside (flat-rate pricing, relatively cheap, widely adopted) but is doing very different economic work for different chains underneath. Chains that wait to batch full pay roughly the same fee per byte as chains that don\'t. The difference accrues over thousands of weekly blobs.\\n\\nFor Taiko at 8.8% fill, upgrading to a batching strategy that targets 80% fill would reduce their blob spend by roughly 90% while publishing exactly the same data. The blobs exist either way \u2014 it\'s a question of whether you fill them before paying.\\n\\n---\\n\\n*Blob fill rates computed from `blob_size` and `blob_empty_size` fields in `canonical_beacon_blob_sidecar`, joined to submitter identity via `mainnet.dim_block_blob_submitter.versioned_hashes`. All data from ethpandaops xatu, Feb 19\u201326 2026, Ethereum mainnet. Rollups with fewer than 400 blobs in the window are excluded. Aztec\'s zero-fill confirmed by checking unique versioned hash diversity over 7 days.*"},{"id":"/2026/02/25/compounding-validator-withdrawals","metadata":{"permalink":"/blog/2026/02/25/compounding-validator-withdrawals","source":"@site/blog/2026-02-25-compounding-validator-withdrawals.md","title":"The 76,322 ETH Withdrawal Spike: Compounding Validators Wake Up","description":"A deep dive into the first wave of mass compounding validator withdrawals after the Pectra upgrade","date":"2026-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"pectra","permalink":"/blog/tags/pectra"},{"inline":true,"label":"validators","permalink":"/blog/tags/validators"},{"inline":true,"label":"withdrawals","permalink":"/blog/tags/withdrawals"},{"inline":true,"label":"research","permalink":"/blog/tags/research"}],"readingTime":3.41,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"The 76,322 ETH Withdrawal Spike: Compounding Validators Wake Up","description":"A deep dive into the first wave of mass compounding validator withdrawals after the Pectra upgrade","authors":["aubury"],"tags":["ethereum","pectra","validators","withdrawals","research"],"date":"2026-02-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Not All Blobs Are Full: A Rollup Efficiency Breakdown","permalink":"/blog/rollup-blob-fill-rates"},"nextItem":{"title":"Half the EVM Is Just Reading and Writing Storage","permalink":"/blog/2026/02/25/evm-gas-breakdown"}},"content":"Something unusual happened on February 21, 2026. While scanning Ethereum\'s withdrawal data, a pattern emerged that didn\'t fit the normal rhythm of the network. **52 validators withdrew over 1,000 ETH each in a single hour** \u2014 a volume we haven\'t seen since the Pectra upgrade activated on January 25.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Discovery\\n\\nMost Ethereum withdrawals are predictable. Validators with 0x01 credentials receive their consensus rewards \u2014 typically 0.01-0.02 ETH every few days. It\'s a steady heartbeat, background noise in the machine.\\n\\nBut the Pectra upgrade introduced something new: **0x02 compounding withdrawal credentials**. These validators don\'t receive automatic payouts. Instead, rewards compound in-place until the balance exceeds the 2048 ETH maximum effective balance, at which point the excess is withdrawn.\\n\\nThis creates a different pattern. Rather than frequent small withdrawals, compounding validators accumulate rewards until they hit the threshold, then release a flood.\\n\\n## The Data\\n\\nQuerying `canonical_beacon_block_withdrawal` from the Xatu dataset:\\n\\n```sql\\nSELECT \\n  toDate(slot_start_date_time) as day,\\n  countIf(withdrawal_amount >= 1e12) as large_withdrawals,\\n  sumIf(withdrawal_amount, withdrawal_amount >= 1e12) / 1e9 as large_eth\\nFROM canonical_beacon_block_withdrawal\\nWHERE slot_start_date_time >= now() - INTERVAL 14 DAY\\nGROUP BY day\\nORDER BY day\\n```\\n\\nThe results show a clear escalation:\\n\\n| Date | Large Withdrawals (\u22651,000 ETH) | Total ETH |\\n|------|-------------------------------|-----------|\\n| Feb 10 | 0 | 0 |\\n| Feb 13 | 31 | 57,993 |\\n| Feb 14 | 22 | 44,165 |\\n| Feb 20 | 18 | 36,771 |\\n| **Feb 21** | **52** | **76,322** |\\n| Feb 22 | 0 | 0 |\\n\\nFebruary 21 stands out. The 52 large withdrawals that day represent **76,322 ETH** \u2014 more than double the daily average of the preceding week.\\n\\n![Compounding Validator Withdrawals After Pectra](/img/compounding-withdrawals-pectra.png)\\n\\n## The Hourly Breakdown\\n\\nZooming into February 21 by hour reveals the concentration:\\n\\n```sql\\nSELECT \\n  toHour(slot_start_date_time) as hour,\\n  countIf(withdrawal_amount >= 1e12) as large_withdrawals,\\n  sumIf(withdrawal_amount, withdrawal_amount >= 1e12) / 1e9 as large_eth\\nFROM canonical_beacon_block_withdrawal\\nWHERE slot_start_date_time >= \'2026-02-21 00:00:00\' \\n  AND slot_start_date_time < \'2026-02-22 00:00:00\'\\nGROUP BY hour\\nORDER BY hour\\n```\\n\\n- **04:00-06:00 UTC**: 5 validators, 10,080 ETH\\n- **12:00 UTC**: **42 validators, 56,827 ETH** \u2014 the spike\\n- **14:00-19:00 UTC**: 4 validators, 8,018 ETH\\n\\nThe 12:00 UTC cluster is remarkable. 42 validators, all withdrawing 1,000+ ETH within the same hour. These aren\'t random \u2014 they\'re a coordinated cohort.\\n\\n## Who Are These Validators?\\n\\nLooking at the validator indices from the February 21 12:00 UTC batch:\\n\\n```sql\\nSELECT \\n  withdrawal_validator_index,\\n  withdrawal_amount / 1e9 as eth_amount,\\n  withdrawal_address\\nFROM canonical_beacon_block_withdrawal\\nWHERE slot_start_date_time >= \'2026-02-21 12:00:00\' \\n  AND slot_start_date_time < \'2026-02-21 13:00:00\'\\n  AND withdrawal_amount >= 1e12\\nORDER BY eth_amount DESC\\nLIMIT 5\\n```\\n\\n| Validator Index | ETH Withdrawn | Withdrawal Address |\\n|----------------|---------------|-------------------|\\n| 2109254 | 1,818.03 | 0x096BC969... |\\n| 2109323 | 1,817.98 | 0xb1241d13... |\\n| 2110655 | 1,816.95 | 0x2d82F61B... |\\n| 2110999 | 1,816.74 | 0x16C33c16... |\\n| 2111060 | 1,816.56 | 0x4698a71c... |\\n\\nAll indices are in the 2.1M range \u2014 recently deposited validators. The withdrawal amounts (~1,816 ETH each) suggest these validators were deposited with 0x02 credentials from inception, accumulated rewards for roughly 4 weeks, and hit their first automatic withdrawal when exceeding the 2048 ETH cap.\\n\\n## The Bigger Picture\\n\\nAs of February 24, only **1.16% of validators** have upgraded to 0x02 credentials:\\n\\n```sql\\nSELECT \\n  substring(withdrawal_credentials, 1, 4) as cred_type,\\n  count() as validator_count,\\n  round(100.0 * validator_count / sum(count()) OVER (), 2) as pct\\nFROM canonical_beacon_validators_withdrawal_credentials\\nWHERE epoch_start_date_time >= now() - INTERVAL 1 DAY\\nGROUP BY cred_type\\n```\\n\\n| Credential Type | Count | Percentage |\\n|----------------|-------|------------|\\n| 0x01 | 4,076,196 | 70.1% |\\n| 0x00 | 1,671,434 | 28.74% |\\n| 0x02 | 67,539 | 1.16% |\\n\\nThe vast majority of validators still use 0x01 credentials with automatic reward distribution. But the 0x02 cohort is growing, and their withdrawal pattern is fundamentally different.\\n\\n## Why This Matters\\n\\nCompounding validators create **lumpier withdrawal flows**. Instead of a steady stream of small payouts, we get periodic bursts when multiple validators hit the 2048 ETH threshold simultaneously.\\n\\nFebruary 21 may be a preview. As more validators upgrade to 0x02 credentials \u2014 especially existing validators migrating from 0x01 \u2014 we could see larger, less predictable withdrawal events.\\n\\nFor staking operators, this changes liquidity planning. For the network, it means withdrawal volume will become more bursty. For analysts, it\'s a new pattern to track.\\n\\nThe 76,322 ETH withdrawn on February 21 didn\'t stress the network. But it was a signal. The compounding validators have arrived.\\n\\n---\\n\\n*Data from Xatu (Ethereum consensus layer telemetry) covering February 10-24, 2026. Queries executed against the `canonical_beacon_block_withdrawal` table.*"},{"id":"/2026/02/25/evm-gas-breakdown","metadata":{"permalink":"/blog/2026/02/25/evm-gas-breakdown","source":"@site/blog/2026-02-25-evm-gas-breakdown.md","title":"Half the EVM Is Just Reading and Writing Storage","description":"Where Ethereum gas actually goes: SLOAD and SSTORE alone consume 56.7% of all EVM execution gas. Arithmetic is 3.4%.","date":"2026-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"evm","permalink":"/blog/tags/evm"},{"inline":true,"label":"gas","permalink":"/blog/tags/gas"},{"inline":true,"label":"opcodes","permalink":"/blog/tags/opcodes"},{"inline":true,"label":"research","permalink":"/blog/tags/research"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":4.93,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"Half the EVM Is Just Reading and Writing Storage","description":"Where Ethereum gas actually goes: SLOAD and SSTORE alone consume 56.7% of all EVM execution gas. Arithmetic is 3.4%.","authors":["aubury"],"tags":["ethereum","evm","gas","opcodes","research","performance"],"date":"2026-02-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The 76,322 ETH Withdrawal Spike: Compounding Validators Wake Up","permalink":"/blog/2026/02/25/compounding-validator-withdrawals"},"nextItem":{"title":"Who Actually Wins the Premium MEV Blocks?","permalink":"/blog/2026/02/25/mev-builder-value-tiers"}},"content":"When people talk about the Ethereum Virtual Machine, they reach for the \\"world computer\\" metaphor \u2014 a globally shared processor executing smart contract code. That framing implies computation: arithmetic, cryptography, logic. In practice, the EVM spends more than half its gas budget on something far more mundane: reading and writing persistent state.\\n\\nEvery week, roughly 1,440 gigagas of EVM execution passes through the mainnet. More than half \u2014 56.7% \u2014 goes to exactly two opcodes.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe breakdown comes from the `fct_opcode_gas_by_opcode_daily` table in xatu-cbt, which tracks gas consumed per opcode per day:\\n\\n```sql\\nSELECT\\n  multiIf(\\n    opcode IN (\'SLOAD\',\'SSTORE\'), \'persistent_storage\',\\n    opcode IN (\'TLOAD\',\'TSTORE\'), \'transient_storage\',\\n    opcode IN (\'CALL\',\'DELEGATECALL\',\'STATICCALL\',\'CALLCODE\'), \'calls\',\\n    opcode IN (\'LOG0\',\'LOG1\',\'LOG2\',\'LOG3\',\'LOG4\'), \'events\',\\n    opcode IN (\'CREATE\',\'CREATE2\'), \'contract_creation\',\\n    opcode IN (\'ADD\',\'MUL\',\'SUB\',\'DIV\',\'SHL\',\'SHR\',\'SAR\', ...), \'arithmetic_bitwise\',\\n    ...\\n  ) as category,\\n  round(100.0 * sum(total_gas) / (SELECT sum(total_gas) FROM ...), 2) as pct\\nFROM mainnet.fct_opcode_gas_by_opcode_daily\\nWHERE day_start_date >= today() - 7\\nGROUP BY category ORDER BY sum(total_gas) DESC\\n```\\n\\n![EVM Gas Breakdown by Opcode Category \u2014 Feb 18-24 2026](/img/evm-gas-breakdown.png)\\n\\nStorage (SLOAD + SSTORE): **56.7%**. Arithmetic and bitwise operations: **3.4%**. The gap is 17:1.\\n\\n---\\n\\nSSTORE is the more expensive half of the pair. Over the past 7 days, 88 million SSTORE executions consumed 460 billion gas \u2014 an average of **5,234 gas each**. SSTORE costs range from 2,100 gas (no-op, writing the same value) through 5,000 gas (updating warm storage) up to 22,100 gas for a write to a brand new storage slot. The average of 5,234 suggests a mix of updates and fresh writes, leaning toward updates.\\n\\nSLOAD is the more frequent one. 301 million executions, 272 billion gas, **904 gas per execution on average**. That number sits between the two canonical costs: 100 gas for a warm read (slot already accessed earlier in the same transaction) and 2,100 gas for a cold read (first access in this transaction). Back-calculating: at 904 gas average, roughly **40% of all SLOAD operations are cold reads**.\\n\\nCold reads at 2,100 gas. Warm reads at 100 gas. But the denominator is enormous \u2014 301 million reads a week. The math adds up fast.\\n\\n---\\n\\nNow for the punchline. EIP-1153, activated in the Cancun upgrade in March 2024, added two new opcodes: TLOAD and TSTORE. Transient storage. Values that exist only for the duration of a single transaction, then vanish. TLOAD always costs **100 gas** \u2014 there\'s no cold/warm distinction, no state trie update, no refund mechanism. Just fast, disposable storage.\\n\\n```sql\\n-- Weekly gas share, all opcodes, 8-week trend\\nSELECT toMonday(day_start_date) as week,\\n  round(100 * sumIf(total_gas, opcode IN (\'SLOAD\',\'SSTORE\')) / sum(total_gas), 2) as storage_pct,\\n  round(100 * sumIf(total_gas, opcode IN (\'TLOAD\',\'TSTORE\')) / sum(total_gas), 3) as transient_pct\\nFROM mainnet.fct_opcode_gas_by_opcode_daily\\nWHERE day_start_date >= today() - 58\\nGROUP BY week ORDER BY week\\n```\\n\\n| Week | Storage (SLOAD+SSTORE) | Transient (TLOAD+TSTORE) |\\n|------|----------------------|--------------------------|\\n| Dec 29 | 58.4% | 0.17% |\\n| Jan 5 | 57.9% | 0.20% |\\n| Jan 12 | 56.2% | 0.21% |\\n| Jan 19 | 56.0% | 0.30% |\\n| Jan 26 | 55.3% | 0.26% |\\n| Feb 2 | 53.6% | 0.30% |\\n| Feb 9 | 55.1% | 0.25% |\\n| Feb 16 | 56.9% | 0.23% |\\n\\nTransient storage has grown from 0.17% to around 0.25\u20130.30% \u2014 roughly doubling over two months. But the absolute gap barely moved. SLOAD+SSTORE consumed 227 times more gas than TLOAD+TSTORE last week. The ratio hasn\'t budged.\\n\\n---\\n\\nWhy isn\'t TLOAD replacing SLOAD where it could?\\n\\nThe practical answer is deployment friction. TLOAD and TSTORE are only useful for data that doesn\'t need to survive beyond the current transaction \u2014 reentrancy guards being the canonical use case. \\"Enter function \u2192 write flag \u2192 do stuff \u2192 clear flag\\" is classic SSTORE/SLOAD, and TSTORE/TLOAD is a perfect fit.\\n\\nBut that pattern is already compiled into millions of deployed contracts. Replacing an SLOAD-based reentrancy guard with TLOAD requires deploying a new contract. The gas savings don\'t automatically flow backward in time to contracts already on chain.\\n\\nNewer Solidity code emitted since Cancun does use TLOAD and TSTORE, which explains the slow growth. But the installed base of DeFi contracts runs on the old pattern, and they account for most of the volume.\\n\\n---\\n\\nA few other numbers from the breakdown that are worth noting:\\n\\n**Events use more gas than all arithmetic combined** \u2014 6.2% for LOG0\u2013LOG4 versus 3.4% for everything from ADD and MUL through SHA3 and bitwise operations. The act of emitting an indexed event costs more in aggregate than all the computation the EVM does. This is partly because LOG has a non-trivial base cost (375 gas) plus per-byte data cost (8 gas/byte) plus per-topic cost (375 gas), and high-volume DeFi protocols emit events on every significant action.\\n\\n**KECCAK256 is less than 1% of gas** (0.72%). The hash function that Ethereum relies on for addresses, storage keys, trie nodes, and signatures barely registers in terms of execution cost. 211 million keccak executions at an average of 44 gas each.\\n\\n**Stack and control flow is 14.4%**. Every PUSH, POP, DUP, SWAP, and JUMP is just overhead \u2014 the cost of running compiled bytecode rather than what that bytecode actually does. The interpreter tax.\\n\\n---\\n\\nThe implication is blunter than any EIP discussion tends to acknowledge: **Ethereum\'s execution layer is, at its core, a database engine**. Most of what the 30 million gas per block buys isn\'t cryptographic computation or logic. It\'s keyed reads and writes to a very large trie.\\n\\nThat\'s not inherently a problem. It\'s what smart contracts do \u2014 they manage state. But it does mean that future EVM improvements aimed at reducing costs have the most leverage precisely in this category. EIP-1153 was the right instinct. The adoption curve is just slower than the deployment speed.\\n\\nThe transient storage experiment is running. It\'s working \u2014 slowly.\\n\\n---\\n\\n*Data: `fct_opcode_gas_by_opcode_daily` (xatu-cbt, mainnet), Feb 18\u201324 2026. 7-day totals. Category assignments based on opcode type; \\"other\\" includes rare/legacy opcodes. Warm/cold SLOAD split estimated from observed average gas per execution (904 gas) and known EIP-2929 costs (warm: 100 gas, cold: 2,100 gas).*"},{"id":"/2026/02/25/mev-builder-value-tiers","metadata":{"permalink":"/blog/2026/02/25/mev-builder-value-tiers","source":"@site/blog/2026-02-25-mev-builder-value-tiers.md","title":"Who Actually Wins the Premium MEV Blocks?","description":"Block builder win rate is a lie. The metric that matters is premium capture \u2014 and most builders are terrible at it.","date":"2026-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"block-builders","permalink":"/blog/tags/block-builders"},{"inline":true,"label":"research","permalink":"/blog/tags/research"},{"inline":true,"label":"mev-boost","permalink":"/blog/tags/mev-boost"}],"readingTime":3.93,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"Who Actually Wins the Premium MEV Blocks?","description":"Block builder win rate is a lie. The metric that matters is premium capture \u2014 and most builders are terrible at it.","authors":["aubury"],"tags":["ethereum","mev","block-builders","research","mev-boost"],"date":"2026-02-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Half the EVM Is Just Reading and Writing Storage","permalink":"/blog/2026/02/25/evm-gas-breakdown"},"nextItem":{"title":"The Three Waves: How Ethereum Validators Choose When to Publish Blocks","permalink":"/blog/2026/02/25/three-block-publishing-waves"}},"content":"Block builder win rate is the number everyone tracks. Builder X won 18% of blocks last week. Builder Y\'s market share is up. But win rate hides something: **most of those blocks are worth almost nothing**. The real competition isn\'t for volume. It\'s for the blocks worth 0.05, 0.2, even 1 ETH in builder payments \u2014 the slots that account for a disproportionate share of all MEV value.\\n\\nLooking at seven days of mainnet MEV-Boost data (~45,000 deduplicated slots), two completely different builder ecosystems are visible.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe simplest version of the data: look at what fraction of each builder\'s wins are \\"high value\\" \u2014 blocks where the builder paid the proposer more than 0.05 ETH.\\n\\n```sql\\nWITH deduped AS (\\n  SELECT\\n    slot,\\n    argMax(builder_pubkey, toUInt64(value)) AS builder_pubkey,\\n    max(toUInt64(value)) AS win_value\\n  FROM mev_relay_proposer_payload_delivered\\n  WHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\n  GROUP BY slot\\n)\\nSELECT\\n  builder_pubkey,\\n  count() AS slots_won,\\n  round(100.0 * countIf(win_value >= 5e16) / count(), 1) AS pct_high_value,\\n  round(100.0 * countIf(win_value < 1e16) / count(), 1) AS pct_low\\nFROM deduped\\nGROUP BY builder_pubkey\\nHAVING slots_won > 400\\nORDER BY pct_high_value DESC\\n```\\n\\nThe spread is extreme. The top five builders capture 7\u20139.4% of their wins in the high-value tier. The bottom three capture 0.1\u20133.3%. Not a small difference in strategy \u2014 a fundamentally different product.\\n\\n![MEV-Boost Block Builder Value Tier Breakdown](/img/builder-value-tiers.png)\\n\\nOn the left: **Builder C** wins 7,270 blocks over seven days \u2014 more than any other builder. It also captures 8.1% of its wins in the high-value tier. Volume *and* quality. It submits 2,294 bids per slot, primarily through Titan Relay.\\n\\nOn the right: **Builder H** wins 2,947 blocks in the same window. A solid haul. But 88.1% of its wins are cheap \u2014 blocks worth under 0.01 ETH each. And just **0.1% are high-value**. Over seven full days, it captured maybe 3 premium blocks.\\n\\nBuilder H submits 4 bids per slot through Aestus and Flashbots relays only. It never routes through BloXroute Max Profit or Titan \u2014 the relays where the lucrative blocks tend to surface.\\n\\nThe contrast isn\'t subtle. Builder C has a 81\xd7 better premium capture rate than Builder H despite winning only 2.5\xd7 as many blocks total. If you\'re a proposer trying to maximize expected return, you don\'t care about Builder H\'s win rate at all.\\n\\n---\\n\\nThe mid-table pattern is equally interesting. Builders F and G both submit ~1,000 bids per slot \u2014 one of the higher bid rates in the market. Yet both land squarely in the commodity zone. 60% of their wins are in the cheap tier, and their premium capture rates are 2\u20133%. High bid volume didn\'t help.\\n\\nThis shows that bid count alone doesn\'t drive premium capture. What separates the premium-capable builders appears to be a combination of relay access and actual MEV extraction capability \u2014 the ability to find and build the block that\'s genuinely worth 0.2 ETH in a given slot.\\n\\n```sql\\n-- Relay breakdown for Builder H (the commodity winner)\\nSELECT relay_name, count(DISTINCT slot) AS slots, avg(toUInt64(value)) / 1e18 AS avg_eth\\nFROM mev_relay_proposer_payload_delivered\\nWHERE slot_start_date_time >= now() - INTERVAL 48 HOUR\\n  AND builder_pubkey = \'0x878e...\'\\nGROUP BY relay_name ORDER BY slots DESC\\n```\\n\\n| Relay | Slots Won | Avg Value |\\n|-------|-----------|-----------|\\n| Aestus | 665 | 0.0051 ETH |\\n| Flashbots | 449 | 0.0042 ETH |\\n\\nTwo relays. Neither of them where the premium MEV flows.\\n\\nBuilder A \u2014 the highest premium capture rate at 9.4% \u2014 routes through BloXroute Max Profit among others and places ~695 bids per slot. It wins fewer blocks than Builder C but at a meaningfully higher average value. Focused.\\n\\n---\\n\\nThe bigger picture: the MEV-Boost builder market isn\'t one market. It\'s two.\\n\\nThere\'s a commodity market for blocks worth 0.001\u20130.01 ETH. These are the slots with everyday transaction flow and minimal MEV. Half a dozen builders compete here on volume and relay coverage. Win rates look impressive. The economics are thin.\\n\\nThen there\'s the premium market \u2014 maybe 5\u20139% of slots, but dramatically higher value per block. Here, only a handful of builders compete effectively. The rest aren\'t even trying.\\n\\nA builder winning 3,000 blocks a week with 0.1% premium capture is essentially a commodity provider that got very good at low-margin work. The builders winning 7,000+ blocks *and* capturing 8% premium are doing something structurally different. They\'ve built infrastructure capable of finding the profitable transactions, constructing the optimal block, and routing through the right relays to win.\\n\\nWin rate is a vanity metric. Premium capture is the real scoreboard.\\n\\n---\\n\\n*Data: `mev_relay_proposer_payload_delivered` (Xatu), Feb 18\u201325 2026, mainnet. Deduplicated by taking the highest-value delivery per slot to eliminate relay duplication. Builders shown are the 8 largest by 7-day deduplicated win count with >400 wins. Builder pubkeys abbreviated as A\u2013H; full pubkeys available on request.*"},{"id":"/2026/02/25/three-block-publishing-waves","metadata":{"permalink":"/blog/2026/02/25/three-block-publishing-waves","source":"@site/blog/2026-02-25-three-block-publishing-waves.md","title":"The Three Waves: How Ethereum Validators Choose When to Publish Blocks","description":"Block propagation data reveals three distinct proposer timing strategies \u2014 and only one of them destroys attestation quality.","date":"2026-02-25T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"mev-boost","permalink":"/blog/tags/mev-boost"},{"inline":true,"label":"attestations","permalink":"/blog/tags/attestations"},{"inline":true,"label":"research","permalink":"/blog/tags/research"},{"inline":true,"label":"timing-game","permalink":"/blog/tags/timing-game"}],"readingTime":4.32,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"title":"The Three Waves: How Ethereum Validators Choose When to Publish Blocks","description":"Block propagation data reveals three distinct proposer timing strategies \u2014 and only one of them destroys attestation quality.","authors":["aubury"],"tags":["ethereum","consensus","mev-boost","attestations","research","timing-game"],"date":"2026-02-25T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Who Actually Wins the Premium MEV Blocks?","permalink":"/blog/2026/02/25/mev-builder-value-tiers"},"nextItem":{"title":"Every Ethereum slot has a hidden auction restart two seconds before it begins","permalink":"/blog/mev-auction-reset"}},"content":"When a validator is chosen to propose a block, it has a choice: publish the moment the block is ready, or wait for MEV-Boost bids to arrive and raise the payout. Most discussions frame this as a binary \u2014 you either participate in the timing game or you don\'t.\\n\\nThe data says it\'s more complicated. There are three distinct groups, and the middle one has mostly gone unnoticed.\\n\\n\x3c!-- truncate --\x3e\\n\\nPlotting block arrival times at Xatu monitoring nodes tells the story clearly. The distribution isn\'t smooth \u2014 it has three separate peaks.\\n\\n```sql\\n-- fct_block_first_seen_by_node (xatu-cbt), 7 days\\nSELECT\\n  intDiv(seen_slot_start_diff, 200) * 200 as bucket_ms,\\n  count() as slots\\nFROM mainnet.fct_block_first_seen_by_node\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND seen_slot_start_diff BETWEEN 100 AND 5000\\nGROUP BY bucket_ms ORDER BY bucket_ms\\n```\\n\\nA sharp peak at 1,400ms. A secondary cluster at 2,400ms. A smaller but distinct bump at 3,200ms.\\n\\nThis isn\'t network noise. The intra-slot spread (fastest to slowest observer for the same block) is only about 530ms \u2014 much smaller than the 1,000ms gap between peaks. These aren\'t different nodes seeing the same block at different times. They\'re different blocks being published at different moments.\\n\\n---\\n\\nOver seven days across ~73,000 mainnet slots, the split is remarkably consistent:\\n\\n| Wave | Typical Arrival | Share of Slots | Description |\\n|------|----------------|----------------|-------------|\\n| Wave 1 | ~1.5s | 66.6% | Publish when ready |\\n| Wave 2 | ~2.4s | 22.3% | Wait ~2s for bids |\\n| Wave 3 | ~3.2s | 11.0% | Full timing game |\\n\\nDay by day for the past week: Wave 2 never drops below 20% or rises above 23%. Whatever is driving it is structural, not a fluke.\\n\\n![Block Propagation: Three Waves and Their Attestation Cost](/img/block-propagation-three-waves.png)\\n\\nThe chart shows both the arrival time distribution (bars) and the head vote accuracy of validators attesting in each slot (white line). The accuracy numbers are the interesting part.\\n\\n---\\n\\n**Accuracy by wave** (joined across `fct_block_first_seen_by_node` and `fct_attestation_correctness_canonical`):\\n\\n```sql\\nWITH slot_waves AS (\\n  SELECT slot,\\n    multiIf(\\n      quantileExact(0.50)(seen_slot_start_diff) < 2000, \'wave1\',\\n      quantileExact(0.50)(seen_slot_start_diff) < 2800, \'wave2\',\\n      \'wave3\'\\n    ) as wave\\n  FROM mainnet.fct_block_first_seen_by_node\\n  WHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\n  GROUP BY slot HAVING count() >= 5\\n)\\nSELECT w.wave,\\n  round(100.0 * sum(a.votes_head) / sum(a.votes_max), 3) as head_accuracy_pct\\nFROM slot_waves w\\nJOIN mainnet.fct_attestation_correctness_canonical a ON w.slot = a.slot\\nGROUP BY w.wave\\n```\\n\\n| Wave | Head Vote Accuracy | vs. Wave 1 |\\n|------|--------------------|------------|\\n| Wave 1 (~1.5s) | **99.657%** | baseline |\\n| Wave 2 (~2.4s) | **99.465%** | \u22120.19 pp |\\n| Wave 3 (~3.2s) | **94.338%** | **\u22125.32 pp** |\\n\\nWave 2 exacts a 0.19 percentage point penalty on head vote accuracy. Wave 3 costs 5.32 percentage points. That\'s a 28\xd7 difference in harm for a 0.8 second difference in publishing time.\\n\\nThe cliff isn\'t gradual. Looking at per-bucket accuracy: at 2,600ms it\'s 99.31%. At 2,800ms it dips to 98.83%. Then at 3,200ms it crashes to 96.78%, at 3,400ms it\'s 91.15%, and at 3,800ms \u2014 the extreme tail of the timing game \u2014 validators vote for the correct head only **33.96%** of the time.\\n\\nThe reason: Ethereum validators are supposed to attest at t=4s into the slot. If a block arrives at 2.4s, there\'s 1.6 seconds for validators to see it and update their head view. If it arrives at 3.2\u20133.8s, many validators have already locked in their attestation pointing to the previous slot\'s head. The later the block, the more \\"orphaned\\" attestations.\\n\\n---\\n\\nWave 2 isn\'t a lazy version of the timing game. It\'s a different strategy entirely.\\n\\nThe MEV-Boost adoption rate tells the story:\\n\\n```sql\\n-- countIf(m.value > 0) / count() grouped by wave\\nWave 1: 90.7% MEV-Boost adoption\\nWave 2: 98.4% MEV-Boost adoption  \\nWave 3: 98.5% MEV-Boost adoption\\n```\\n\\nWave 2 and Wave 3 are nearly identical in MEV-Boost participation (98.4% vs 98.5%). Both are waiting for relay bids. But Wave 2 proposers stop waiting at ~2 seconds rather than ~3 seconds.\\n\\nThe median MEV-Boost payment for Wave 2 blocks (0.010 ETH) is slightly higher than Wave 1 (0.009 ETH) \u2014 a small benefit from waiting for bids. Wave 3 has the highest median (0.011 ETH) and a much fatter tail (avg 0.058 ETH vs 0.022 ETH for Wave 2), driven by rare high-value MEV events.\\n\\nWave 2 proposers collect a marginal MEV premium \u2014 0.001 ETH per block on average \u2014 without meaningfully touching network attestation quality. They\'ve found a configuration sweet spot: wait 2 seconds, not 3.\\n\\n---\\n\\nThe safe window appears to close around 2.8 seconds. Below that threshold, accuracy barely moves. Past it, the network starts paying a real price.\\n\\nThat 22% of Ethereum blocks quietly operates in this window is new information. Most timing game analysis looks for the hard cutoff at 3 seconds and treats everything below it as \\"normal.\\" But there are 10,945 slots per week sitting in a distinct 2,000\u20132,800ms zone \u2014 proposers with a specific configuration choice that nobody has been tracking.\\n\\nThe timing game research focuses on the damage. This is the other finding: **there\'s a large group of validators capturing MEV-Boost benefits at almost no cost to the network.** Whether that\'s a feature or a future concern is a different debate.\\n\\n---\\n\\n*Data: `fct_block_first_seen_by_node` and `fct_attestation_correctness_canonical` from xatu-cbt (mainnet), Feb 18\u201325 2026. Intra-slot spread computed across all monitoring nodes per slot. Wave boundaries set at 2,000ms and 2,800ms based on the observed trimodal distribution structure. MEV data from `fct_block_mev_head`.*"},{"id":"mev-auction-reset","metadata":{"permalink":"/blog/mev-auction-reset","source":"@site/blog/2026-02-24-mev-auction-reset.md","title":"Every Ethereum slot has a hidden auction restart two seconds before it begins","description":"The MEV relay system starts building blocks for a slot before that slot exists. Builders are running their engines eight seconds ahead of the clock, constantly revising bids as new transactions hit the mempool. What nobody seems to have charted is what happens at exactly two seconds before a slot starts: the entire auction collapses to near-zero and then rebuilds from scratch.","date":"2026-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"relay","permalink":"/blog/tags/relay"},{"inline":true,"label":"timing","permalink":"/blog/tags/timing"},{"inline":true,"label":"auction","permalink":"/blog/tags/auction"},{"inline":true,"label":"builder","permalink":"/blog/tags/builder"}],"readingTime":4.44,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"mev-auction-reset","title":"Every Ethereum slot has a hidden auction restart two seconds before it begins","authors":"aubury","tags":["mev","relay","timing","auction","builder"],"date":"2026-02-24T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The Three Waves: How Ethereum Validators Choose When to Publish Blocks","permalink":"/blog/2026/02/25/three-block-publishing-waves"},"nextItem":{"title":"PeerDAS has been running on mainnet for 30 days, and column index predicts propagation speed","permalink":"/blog/peerdas-column-gradient"}},"content":"The MEV relay system starts building blocks for a slot before that slot exists. Builders are running their engines eight seconds ahead of the clock, constantly revising bids as new transactions hit the mempool. What nobody seems to have charted is what happens at exactly two seconds before a slot starts: the entire auction collapses to near-zero and then rebuilds from scratch.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe data is in `fct_mev_bid_highest_value_by_builder_chunked_50ms`, a pre-aggregated table in the ethpandaops CBT cluster that tracks the highest bid from each builder in 50ms windows for every slot. Pulling 48 hours of mainnet data across ~14,400 slots and smoothing into 200ms buckets:\\n\\n```sql\\nWITH per_slot_max AS (\\n    SELECT slot, max(value) as slot_max_val\\n    FROM mainnet.fct_mev_bid_highest_value_by_builder_chunked_50ms\\n    WHERE slot_start_date_time >= now() - INTERVAL 48 HOUR\\n    GROUP BY slot\\n)\\nSELECT\\n    round(b.chunk_slot_start_diff / 200.0) * 200 / 1000.0 as sec,\\n    multiIf(\\n        p.slot_max_val < 5000000000000000,  \'low\',\\n        p.slot_max_val < 30000000000000000, \'medium\',\\n        \'high\'\\n    ) as value_tier,\\n    quantile(0.5)(b.value)  / 1e18 as median_bid_eth,\\n    quantile(0.25)(b.value) / 1e18 as p25_eth,\\n    quantile(0.75)(b.value) / 1e18 as p75_eth\\nFROM mainnet.fct_mev_bid_highest_value_by_builder_chunked_50ms b\\nJOIN per_slot_max p ON b.slot = p.slot\\nWHERE b.slot_start_date_time >= now() - INTERVAL 48 HOUR\\n  AND b.chunk_slot_start_diff BETWEEN -8000 AND 6200\\nGROUP BY sec, value_tier\\nHAVING count() > 100\\nORDER BY value_tier, sec\\n```\\n\\nThe `chunk_slot_start_diff` field is in milliseconds relative to when the slot starts \u2014 negative values mean before the slot begins. What comes back is this:\\n\\n![The MEV auction inside a single Ethereum slot](/img/mev_auction_curve.png)\\n\\nThe left portion, from \u22128s to about \u22122.2s, is the pre-build phase. Builders have been running since the previous slot was still in flight, incrementally improving their bids as the mempool accumulates new transactions. Median bid values for medium-MEV slots rise from about 0.0007 ETH at \u22128s to 0.0020 ETH at \u22122.2s \u2014 a slow grind that reflects builders incorporating the transaction flow but working from state that predates the previous block.\\n\\nThen at exactly \u22122.0 seconds: the median bid value drops to **130 wei**. Not a dip \u2014 a near-total collapse. In that single 200ms bucket, bid volume spikes from ~20,000 to 415,000 (a 20\xd7 surge) while the median bid falls from 0.0020 ETH to 0.00000013 ETH. The auction effectively restarts from zero.\\n\\nTo verify this isn\'t noise, the same query over each of the past seven days:\\n\\n```sql\\nSELECT\\n    toDate(slot_start_date_time) as day,\\n    chunk_slot_start_diff,\\n    count() as n,\\n    quantile(0.5)(value) / 1e18 as median_bid_eth\\nFROM mainnet.fct_mev_bid_highest_value_by_builder_chunked_50ms\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND chunk_slot_start_diff IN (-8000, -2000, -1500, 0, 2000)\\nGROUP BY day, chunk_slot_start_diff\\nORDER BY day, chunk_slot_start_diff\\n```\\n\\nEvery single day: median at \u22122000ms between 0.0000001 and 0.0000054 ETH (essentially zero). Every single day: median at \u22121500ms recovering to 0.0022\u20130.0026 ETH. The reset is structural, not a data artifact.\\n\\nWhat causes it? The best explanation is that \u22122 seconds is roughly when builders receive the previous slot\'s block \u2014 either from an early-ish proposer, or when their own scheduler fires a \\"reset and rebuild\\" cycle. When a new block arrives, builders must flush their pending bids (which were built on the previous state), process the block, update their mempool, and start fresh. The near-zero value at \u22122s reflects that first flush, before any real MEV has been incorporated into the new block.\\n\\nAfter the reset, values rebuild fast. By the time the slot actually starts (t=0), median bids for medium-MEV slots have recovered to **0.0053 ETH**. The auction then continues into the slot itself, and this is where the timing game lives: waiting five seconds past slot start yields **0.0091 ETH** \u2014 a 71% premium over proposing at t=0. For high-MEV slots, that same five-second wait yields **3.1\xd7 more value** (0.010 ETH \u2192 0.033 ETH).\\n\\nThe rate of value increase is fastest in the first two seconds post-slot-start. Medium-MEV slots gain about 10\u201315% per second in the t=0 to t=2 range, then the curve flattens. The auction largely ends around t=+5 to t=+6; after that, very few bids remain and the median stabilizes. Proposing past t=6 is rarely rewarded with more value \u2014 you\'re just paying with attestation risk for nothing.\\n\\nThe three-tier breakdown reveals something about who benefits most from timing strategy. Low-MEV slots barely change across the whole window \u2014 there\'s little value to capture regardless of timing. Medium-MEV slots show a steady curve where each second of waiting adds meaningful value, but the slope is manageable. High-MEV slots are where the multiplier is extreme, and those are exactly the slots where a timing-game validator stands to gain the most \u2014 and where the attestation-miss risk most directly trades against concrete revenue.\\n\\nThe \u22122s reset means there\'s a natural rhythm to each slot: a long preparation phase where builders work from stale state, a hard reset when the previous block lands, a 2-second sprint to rebuild value, and then a continuation into the slot itself that rewards patient proposers. Validators optimizing purely for MEV capture will try to sample bids as late as possible; validators optimizing for attestation health will propose early and accept the discount. The curve now shows you exactly what that discount is.\\n\\n*Data: 48 hours, ~14,400 mainnet slots, Feb 22\u201324 2026. Xatu CBT pre-aggregated dataset. Shaded band on medium-MEV curve = interquartile range (25th\u201375th percentile). Slot value tiers: low under 0.005 ETH, medium 0.005\u20130.03 ETH, high above 0.03 ETH.*"},{"id":"peerdas-column-gradient","metadata":{"permalink":"/blog/peerdas-column-gradient","source":"@site/blog/2026-02-24-peerdas-column-gradient.md","title":"PeerDAS has been running on mainnet for 30 days, and column index predicts propagation speed","description":"PeerDAS \u2014 EIP-7594\'s data availability sampling system \u2014 has been live on Ethereum mainnet for over 30 days. All 128 column subnets are active, and the data is arriving: 10,956 out of 10,958 slots in the last 48 hours had every single column propagate within 12 seconds. That\'s 99.98% completeness. The protocol is working.","date":"2026-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"peerdas","permalink":"/blog/tags/peerdas"},{"inline":true,"label":"das","permalink":"/blog/tags/das"},{"inline":true,"label":"blob","permalink":"/blog/tags/blob"},{"inline":true,"label":"eip-7594","permalink":"/blog/tags/eip-7594"},{"inline":true,"label":"propagation","permalink":"/blog/tags/propagation"},{"inline":true,"label":"gossip","permalink":"/blog/tags/gossip"},{"inline":true,"label":"libp2p","permalink":"/blog/tags/libp-2-p"}],"readingTime":4.5,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"peerdas-column-gradient","title":"PeerDAS has been running on mainnet for 30 days, and column index predicts propagation speed","authors":"aubury","tags":["peerdas","das","blob","eip-7594","propagation","gossip","libp2p"],"date":"2026-02-24T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Every Ethereum slot has a hidden auction restart two seconds before it begins","permalink":"/blog/mev-auction-reset"},"nextItem":{"title":"Publishing a block 3.4 seconds late costs you 677 mETH in MEV and costs your attesters 22% of their head votes","permalink":"/blog/timing-game-attestation-cliff"}},"content":"PeerDAS \u2014 EIP-7594\'s data availability sampling system \u2014 has been live on Ethereum mainnet for over 30 days. All 128 column subnets are active, and the data is arriving: 10,956 out of 10,958 slots in the last 48 hours had every single column propagate within 12 seconds. That\'s 99.98% completeness. The protocol is working.\\n\\nBut there\'s something nobody seems to have noticed: **column index 0 arrives 156 milliseconds faster than column index 101.** The correlation between column index and median propagation time is 0.82. And it\'s been this way, consistently, for seven consecutive days.\\n\\n\x3c!-- truncate --\x3e\\n\\nThe data lives in `libp2p_gossipsub_data_column_sidecar` in Xatu \u2014 a table that records every time an EthPandaOps observation node sees a gossip message on a PeerDAS column subnet. Pulling 48 hours of mainnet data, filtered to propagation times between 100ms and 12,000ms:\\n\\n```sql\\nSELECT \\n    column_index,\\n    count() as obs,\\n    quantileExact(0.5)(propagation_slot_start_diff) as p50_ms,\\n    quantileExact(0.95)(propagation_slot_start_diff) as p95_ms\\nFROM libp2p_gossipsub_data_column_sidecar\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 48 HOUR\\n  AND propagation_slot_start_diff BETWEEN 100 AND 12000\\nGROUP BY column_index\\nORDER BY column_index\\n```\\n\\nColumn 0: **1544ms p50**. Column 101: **1700ms p50**. Column 127: 1642ms. The scatter plot makes the pattern unmistakable.\\n\\n![PeerDAS column index predicts propagation speed](/img/peerdas_column_gradient.png)\\n\\nThe blue dashed trend line follows a roughly logarithmic curve \u2014 steep improvement in the first 64 columns, then leveling off. Columns 96\u2013127 are mostly clustered around 1640\u20131700ms; columns 0\u201331 cluster around 1540\u20131620ms. The dots are colored by custodian count (how many observation nodes are subscribed to that subnet) \u2014 more on that in a moment.\\n\\nBefore assuming this is noise, the same query broken into 32-column bands over 7 days:\\n\\n```sql\\nSELECT \\n    toDate(slot_start_date_time) as day,\\n    intDiv(column_index, 32) * 32 as col_band,\\n    quantileExact(0.5)(propagation_slot_start_diff) as p50_ms\\nFROM libp2p_gossipsub_data_column_sidecar\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND propagation_slot_start_diff BETWEEN 100 AND 12000\\nGROUP BY day, col_band\\nORDER BY day, col_band\\n```\\n\\nEvery single day, the same ordering holds: band 0\u201331 is fastest, then 32\u201363, then 64\u201395, then 96\u2013127. The 7-day aggregate for each band: **1585ms, 1620ms, 1648ms, 1652ms**. The gap is ~67ms and it does not close. This is structural, not random.\\n\\nWhat drives it? There are two candidates.\\n\\nThe first is custodian count. The EthPandaOps observation network includes about 20 nodes that each custody 8 columns (the minimum per EIP-7594\'s custody requirement), plus 6 full nodes that subscribe to all 128. Whether a given column also has sharded nodes custodying it \u2014 and how many \u2014 varies. Columns with more observers have more redundant gossip paths:\\n\\n```sql\\nSELECT \\n    column_index,\\n    count(DISTINCT meta_client_name) as total_custodians\\nFROM libp2p_gossipsub_data_column_sidecar\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 6 HOUR\\nGROUP BY column_index\\nORDER BY column_index\\n```\\n\\nCustodian counts range from **6 to 13** per column. Columns with 6 custodians average **1672ms**; columns with 13 custodians average **1616ms**. The correlation between custodian count and p50 is **\u22120.33** \u2014 real but weak.\\n\\nThe column_index correlation is **0.82**. So custodian count accounts for part of the effect but not most of it. Something about lower-indexed column subnets makes them intrinsically faster, beyond just the number of nodes watching them.\\n\\nThe most plausible explanation is gossip mesh maturity. In libp2p gossipsub, each topic maintains its own mesh of D peers. When a node joins and subscribes to 128 column topics in ascending order (0, 1, 2, ..., 127), the mesh for topic 0 begins forming first. Over time \u2014 across restarts, peer churn, and rebalancing cycles \u2014 lower-indexed topic meshes have accumulated more connection-time and are more stable. When a new column sidecar is published, it traverses a denser, more established fanout tree on subnet 0 than on subnet 96.\\n\\nThe smoking gun: the *fastest-ever* propagation time for each column is essentially identical regardless of index. Column 0\'s fastest observed arrival was 138ms; column 127\'s was 134ms. The absolute floor \u2014 the direct-custody hot path \u2014 is equally fast for all columns. The gradient only appears in the median. That means the *typical* gossip relay path is slower for high-index columns, while the *best-case* path (direct peering) is uniform. Mesh maturity explains this: the direct path works regardless of mesh quality; the multi-hop gossip path depends on how well-connected the mesh is.\\n\\nThe practical consequence is mild but real. PeerDAS samplers are supposed to query a random subset of columns to verify data availability. If you happen to sample column 4 and column 101, you\'ll get an answer from column 4 about 150ms sooner. That\'s a meaningful fraction of a typical sampling window. It won\'t break DA verification \u2014 all 128 columns are arriving before the slot closes \u2014 but it creates a subtle asymmetry in sampling latency that the spec doesn\'t account for.\\n\\nWhether this is a transient bootstrapping artifact or a stable feature of the mainnet PeerDAS gossip network is something worth watching. If the gradient shrinks over months as the mesh matures uniformly, it\'s temporary. If it persists, it\'s a permanent property of how libp2p gossipsub behaves at 128-topic scale.\\n\\n*Data: 48 hours (10,958 slots) for primary analysis; 7 days for trend verification. Feb 21\u201323 2026. Source: Xatu `libp2p_gossipsub_data_column_sidecar`, mainnet. Propagation time is relative to slot start (`propagation_slot_start_diff`), filtered 100\u201312,000ms to exclude late-processing outliers. Observation network: ~6 full-subscriber nodes + ~20 sharded 8-column nodes. Custodian counts reflect distinct EthPandaOps clients, not the broader mainnet custody network.*"},{"id":"timing-game-attestation-cliff","metadata":{"permalink":"/blog/timing-game-attestation-cliff","source":"@site/blog/2026-02-24-timing-game-attestation-cliff.md","title":"Publishing a block 3.4 seconds late costs you 677 mETH in MEV and costs your attesters 22% of their head votes","description":"Every proposer using MEV-Boost faces the same tradeoff: wait longer to capture more value, but at some point your block arrives too late for attesters to see it before they commit their vote. The timing game is well-understood in theory. What hasn\'t been measured is exactly where the cliff is \u2014 and how steep the drop really is.","date":"2026-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"timing-game","permalink":"/blog/tags/timing-game"},{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"attestation","permalink":"/blog/tags/attestation"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"head-accuracy","permalink":"/blog/tags/head-accuracy"},{"inline":true,"label":"proposer","permalink":"/blog/tags/proposer"}],"readingTime":4.89,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"timing-game-attestation-cliff","title":"Publishing a block 3.4 seconds late costs you 677 mETH in MEV and costs your attesters 22% of their head votes","authors":"aubury","tags":["timing-game","mev","attestation","consensus","head-accuracy","proposer"],"date":"2026-02-24T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"PeerDAS has been running on mainnet for 30 days, and column index predicts propagation speed","permalink":"/blog/peerdas-column-gradient"},"nextItem":{"title":"p2porg publishes 96% of its blocks after 3 seconds. Its Lido validators publish on time.","permalink":"/blog/timing-game-by-entity"}},"content":"Every proposer using MEV-Boost faces the same tradeoff: wait longer to capture more value, but at some point your block arrives too late for attesters to see it before they commit their vote. The timing game is well-understood in theory. What hasn\'t been measured is exactly where the cliff is \u2014 and how steep the drop really is.\\n\\nThe cliff is at 3.0 seconds. What happens after it is sharper than you\'d expect.\\n\\n\x3c!-- truncate --\x3e\\n\\nTo measure this, I pulled the first-seen gossip arrival time for every block on mainnet over the last 7 days from `libp2p_gossipsub_beacon_block`:\\n\\n```sql\\nSELECT\\n    slot,\\n    min(propagation_slot_start_diff) as first_seen_ms\\nFROM libp2p_gossipsub_beacon_block\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND propagation_slot_start_diff BETWEEN 0 AND 12000\\nGROUP BY slot\\n```\\n\\nThat `MIN()` gives the earliest moment any EthPandaOps crawler saw the block appear in the gossip layer \u2014 a reasonable proxy for when the proposer actually published. Then I joined it with per-slot attestation accuracy from the CBT pre-aggregated table:\\n\\n```sql\\nSELECT\\n    slot,\\n    toFloat64(votes_head) / toFloat64(votes_max) * 100.0 as head_pct\\nFROM mainnet.fct_attestation_correctness_canonical FINAL\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND votes_max > 0 AND votes_head IS NOT NULL\\n```\\n\\n50,110 slots merged across 7 days. Here\'s what the 200ms buckets look like:\\n\\n![Block arrival timing versus head vote accuracy and MEV](/img/timing_attestation_cliff.png)\\n\\nThe top panel is head vote accuracy. It starts at 99.7% for blocks arriving under 1.5 seconds and declines slowly until it falls off a cliff. At 3.0 seconds: 97.1%. At 3.2 seconds: **92.3%**. At 3.4 seconds: **78.1%**. At 3.6 seconds: 46.8%.\\n\\nThat\'s not a graceful degradation. That\'s a wall at 3.2 seconds.\\n\\nThe bottom panel is where this gets interesting. The gray bars are average MEV for blocks arriving before the cliff \u2014 around 20 mETH (0.020 ETH), completely flat, all the way from 1.0 to 3.0 seconds. Then at 3.4 seconds, the orange bar hits **677 mETH (0.677 ETH)**. Those blocks aren\'t arriving late by accident. They\'re the timing game.\\n\\nThe previous [MEV auction reset post](/blog/mev-auction-reset) showed that builder bids continue climbing well past slot start, with a 3.15\xd7 multiplier for high-MEV blocks between t=0 and t=5 seconds. The data here shows what that costs. Proposers who wait to 3.4 seconds to capture peak MEV value are buying that value at the price of 22% of their validators missing the head.\\n\\nBefore this gets filed as a variant of the gas problem \u2014 it isn\'t. The [earlier gas analysis](/blog/gas-execution-attestation) showed that execution complexity (gas used) predicts head accuracy because high-gas blocks take longer to process. That\'s a different mechanism. To verify, here\'s the same analysis split by gas utilisation bucket:\\n\\nFor the 40\u201360% gas bucket (\\"typical blocks\\"):\\n- Arrival under 1.5s \u2192 avg head accuracy **99.72%**\\n- Arrival 2.5\u20133s \u2192 avg head accuracy **99.26%**\\n- Arrival over 3s \u2192 avg head accuracy **94.51%**\\n\\nA typical block at 40-60% gas that arrives under 1.5 seconds gets 99.72% head accuracy. The same gas load arriving after 3 seconds gets **94.51%**. The gap exists regardless of which gas bucket you\'re in \u2014 low gas, medium gas, high gas, doesn\'t matter. Gas utilisation across the timing buckets is 48\u201360%, essentially flat. This effect is about publication delay, not execution complexity.\\n\\nGas explains one failure mode. Timing explains another. Both operate independently.\\n\\nThe daily pattern is completely consistent. Every day for the last 7 days:\\n\\n| Day | Early blocks (under 3s) | Late blocks (over 3s) | Late block head% | Late block fraction |\\n|-----|-------------------|--------------------|-----------------|---------------------|\\n| Feb 17 | 99.56% | 92.04% | \u2014 | 8.3% |\\n| Feb 18 | 99.56% | 91.14% | \u2014 | 9.1% |\\n| Feb 19 | 99.50% | 92.27% | \u2014 | 7.6% |\\n| Feb 20 | 99.52% | 92.39% | \u2014 | 7.0% |\\n| Feb 21 | 99.56% | 93.39% | \u2014 | 6.4% |\\n| Feb 22 | 99.60% | 93.02% | \u2014 | 6.3% |\\n| Feb 23 | 99.59% | 92.65% | \u2014 | 6.3% |\\n\\nThe fraction of late blocks (over 3s) has been declining \u2014 from 9.1% to 6.3% over the week. Whether that\'s a seasonal pattern, a change in timing game behaviour, or something else isn\'t clear from 7 days of data. But the head accuracy gap between early and late blocks is locked in every day: roughly 7\u20138 percentage points.\\n\\nThe mechanism isn\'t mysterious. Ethereum\'s attestation window works like this: validators are assigned to attest in a specific slot and need to submit before the end of that slot. The ideal submit time is around t=4 seconds into the slot (one-third of the 12-second window, giving the attestation time to propagate and be included in the next block). Validators are looking at their local view of the canonical head at attestation time. If the block hasn\'t arrived yet \u2014 or arrived but hasn\'t fully propagated \u2014 they attest to the previous head instead.\\n\\nAt 3.0 seconds, the block is just squeaking in for some validators. At 3.2 seconds, enough validators have already committed their attestation that one in thirteen is voting for the wrong head. At 3.4 seconds, it\'s one in five.\\n\\nThe cost in absolute terms: roughly 30,000 validators participate in any given slot\'s attestation committee. A 7.5 percentage point drop in head accuracy means about 2,250 validators per late slot voting for the wrong head. With ~450 late slots per day (7.2% \xd7 ~6,300 daily slots), that\'s around one million mis-attributed head votes per day, spread across proposers who captured the top of the MEV curve.\\n\\nWhether this is a protocol-level problem or an acceptable tradeoff is a design question. The timing game creates a genuine externality: proposers capture private MEV gains while distributing the attestation cost across the validator set. The cliff at 3.0-3.2 seconds is where that externality gets quantifiably expensive.\\n\\n*Data: 7 days (50,110 slots, Feb 17\u201323 2026) for primary analysis; 48h cross-validation. Sources: `libp2p_gossipsub_beacon_block` (xatu) for block gossip timing; `mainnet.fct_attestation_correctness_canonical` and `mainnet.fct_block_mev_head` (xatu-cbt) for head accuracy and MEV. Block first-seen time is MIN(propagation_slot_start_diff) across all EthPandaOps crawler nodes \u2014 a lower bound on true publication time. Gas limit assumed 60M.*"},{"id":"timing-game-by-entity","metadata":{"permalink":"/blog/timing-game-by-entity","source":"@site/blog/2026-02-24-timing-game-by-entity.md","title":"p2porg publishes 96% of its blocks after 3 seconds. Its Lido validators publish on time.","description":"There\'s a 3-second cliff in Ethereum\'s attestation system. Blocks that arrive after it \u2014 when validators have already started forming their head votes \u2014 cause measurable drops in head accuracy. The earlier post established that with 50,000 slots of data.","date":"2026-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"timing-game","permalink":"/blog/tags/timing-game"},{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"staking-entities","permalink":"/blog/tags/staking-entities"},{"inline":true,"label":"attestation","permalink":"/blog/tags/attestation"},{"inline":true,"label":"proposer","permalink":"/blog/tags/proposer"},{"inline":true,"label":"lido","permalink":"/blog/tags/lido"}],"readingTime":4.23,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"timing-game-by-entity","title":"p2porg publishes 96% of its blocks after 3 seconds. Its Lido validators publish on time.","authors":"aubury","tags":["timing-game","mev","staking-entities","attestation","proposer","lido"],"date":"2026-02-24T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Publishing a block 3.4 seconds late costs you 677 mETH in MEV and costs your attesters 22% of their head votes","permalink":"/blog/timing-game-attestation-cliff"},"nextItem":{"title":"The thing slowing down your EL client isn\'t MEV","permalink":"/blog/gas-execution-attestation"}},"content":"There\'s a 3-second cliff in Ethereum\'s attestation system. Blocks that arrive after it \u2014 when validators have already started forming their head votes \u2014 cause measurable drops in head accuracy. The [earlier post](/blog/timing-game-attestation-cliff) established that with 50,000 slots of data.\\n\\nWhat it didn\'t answer: who\'s responsible?\\n\\n\x3c!-- truncate --\x3e\\n\\nThe data to answer that is in two tables: `libp2p_gossipsub_beacon_block` in Xatu gives the first-seen gossip time for every block; `mainnet.fct_block_proposer_entity` in the CBT cluster maps each slot to a labelled staking entity. Joining them over seven days produces a timing game scorecard per operator.\\n\\n```sql\\n-- Block first-seen: proxy for publication time\\nSELECT slot, min(propagation_slot_start_diff) as first_seen_ms\\nFROM libp2p_gossipsub_beacon_block\\nWHERE meta_network_name = \'mainnet\'\\n  AND slot_start_date_time >= now() - INTERVAL 7 DAY\\n  AND propagation_slot_start_diff BETWEEN 0 AND 12000\\nGROUP BY slot\\n```\\n\\n```sql\\n-- Entity per slot\\nSELECT slot, entity\\nFROM mainnet.fct_block_proposer_entity FINAL\\nWHERE slot_start_date_time >= now() - INTERVAL 7 DAY\\n```\\n\\nThe join covers 45,401 slots with entity labels. A block is \\"timing game\\" if its gossip arrival is more than 3 seconds after slot start \u2014 the point where head vote accuracy starts collapsing. Here\'s what the scorecard looks like:\\n\\n![Staking entity timing game vs attestation accuracy](/img/entity_timing_game.png)\\n\\nThe top of the chart is dominated by a handful of operators. p2porg publishes **96% of its blocks after 3 seconds**, every single day for seven days running. Not a network outage. Not a temporary misconfiguration. A consistent, stable pattern. Their median first-seen time is 3094ms \u2014 the block reliably arrives three seconds into the slot.\\n\\nKelp (a liquid restaking platform on EigenLayer) is at 87.5%. Everstake\'s standalone operation is at 59%. Gateway FMAS and Blockdaemon\'s Lido-operated validators both sit in the 57\u201360% range. Kiln is at 42%.\\n\\nThe head accuracy numbers follow directly:\\n\\n| Entity | Timing game blocks | Avg head accuracy |\\n|--------|-------------------|-------------------|\\n| blockdaemon_lido | 57.7% | 94.05% |\\n| everstake | 59.0% | 95.11% |\\n| gateway.fmas_lido | 60.1% | 95.38% |\\n| kelp | 87.5% | 96.47% |\\n| p2porg | 96.0% | 96.43% |\\n\\nThese are entity-level averages across hundreds of block proposals. A 94\u201396% head accuracy means roughly 1 in 18 to 1 in 25 validators is voting for the wrong head during that entity\'s slots \u2014 compared to roughly 1 in 200 for binance (0.07% timing game blocks, 99.44% accuracy) or solo stakers (1.1%, 99.16%).\\n\\nThe most striking data point is the split within the same operating organisations.\\n\\np2porg, running validators independently: 96% timing game blocks. p2porg_lido, running validators under the Lido node operator programme: 0% timing game. Same company, different configuration, wildly different behaviour.\\n\\nEverstake standalone: 59% timing game. Everstake Lido: 0.6%.\\n\\nThis isn\'t an infrastructure story. p2porg\'s independent validators aren\'t slow because their hardware is slow \u2014 if hardware were the bottleneck, their Lido validators would be slow too. The split tells you the timing delay is a deliberate configuration choice: a MEV-Boost bid wait time, a relay cutoff, something adjustable that someone has adjusted differently for the two validator sets.\\n\\nTo double-check that the pattern is structural rather than a statistical artefact, here are the daily timing game rates for the top offenders:\\n\\n```sql\\n-- Daily consistency check\\nSELECT toDate(slot_start_date_time) as day, entity,\\n       countIf(first_seen_ms >= 3000) / count() * 100 as pct_late\\n-- (Python merge: timing \xd7 entity \xd7 day)\\n```\\n\\n| Day | p2porg | everstake | kelp | binance | solo_stakers |\\n|-----|--------|-----------|------|---------|--------------|\\n| Feb 17 | 100% | 94.6% | 84.6% | 0.0% | 0.5% |\\n| Feb 18 | 94.7% | 95.5% | 87.0% | 0.4% | 1.2% |\\n| Feb 19 | 97.9% | 74.8% | 95.7% | 0.0% | 1.2% |\\n| Feb 20 | 92.9% | 42.5% | 94.1% | 0.0% | 1.4% |\\n| Feb 21 | 95.6% | 36.9% | 100% | 0.0% | 0.5% |\\n| Feb 22 | 95.5% | 42.6% | 79.2% | 0.0% | 0.9% |\\n| Feb 23 | 96.4% | 37.8% | 75.9% | 0.2% | 1.6% |\\n\\np2porg is remarkably stable (92.9\u2013100%, every day). Everstake fluctuates more, suggesting dynamic MEV settings that change with market conditions. Kelp started at 84.6% and remained very high. Binance and solo stakers are consistently near zero.\\n\\nThis matters because the attestation cost isn\'t theoretical. Every late block from these entities causes some fraction of the validator set to vote for the wrong head. Compounded across thousands of proposals, across months, this is a structural drag on consensus quality that comes from a small number of entities that have deliberately chosen aggressive MEV timing settings.\\n\\nThere\'s a reasonable counterargument: the timing game extracts more value, and the proposer capturing more MEV is presumably passing some of that to stakers. The data here doesn\'t resolve whether the MEV gain outweighs the network externality \u2014 that requires pricing the attestation degradation. What the data does establish is that the externality is real, entity-specific, and structurally driven by configuration choices that some operators make for their independent validators but not for their Lido-operated ones.\\n\\nThe Lido node operator programme, at least for these operators, appears to be enforcing something closer to on-time behaviour. Whether that\'s explicit policy, monitoring, or just different MEV configurations is unclear. But the gap is stark: same people, different programmes, consistently different outcomes.\\n\\n*Data: 7 days, 45,401 labelled slots, Feb 17\u201323 2026. Sources: `libp2p_gossipsub_beacon_block` (xatu) for block first-seen times; `mainnet.fct_block_proposer_entity` and `mainnet.fct_attestation_correctness_canonical` (xatu-cbt) for entity labels and head vote accuracy. Entities with fewer than 100 blocks in the window excluded. Block first-seen is MIN(propagation_slot_start_diff) \u2014 earliest time any EthPandaOps crawler observed the gossip message.*"},{"id":"gas-execution-attestation","metadata":{"permalink":"/blog/gas-execution-attestation","source":"@site/blog/2026-02-23-gas-execution-attestation.md","title":"The thing slowing down your EL client isn\'t MEV","description":"I started this looking for evidence that high-MEV blocks are harder for execution clients to process. The intuition is obvious: MEV blocks are full of complex DeFi interactions, sandwich attacks, arbitrage \u2014 all the state-thrashing stuff. Surely they\'re heavier to execute.","date":"2026-02-23T00:00:00.000Z","tags":[{"inline":true,"label":"execution","permalink":"/blog/tags/execution"},{"inline":true,"label":"gas","permalink":"/blog/tags/gas"},{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"attestation","permalink":"/blog/tags/attestation"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":2.71,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"gas-execution-attestation","title":"The thing slowing down your EL client isn\'t MEV","authors":"aubury","tags":["execution","gas","mev","attestation","performance"],"date":"2026-02-23T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"p2porg publishes 96% of its blocks after 3 seconds. Its Lido validators publish on time.","permalink":"/blog/timing-game-by-entity"},"nextItem":{"title":"Ethereum Block Timing","permalink":"/blog/ethereum-block-timing"}},"content":"I started this looking for evidence that high-MEV blocks are harder for execution clients to process. The intuition is obvious: MEV blocks are full of complex DeFi interactions, sandwich attacks, arbitrage \u2014 all the state-thrashing stuff. Surely they\'re heavier to execute.\\n\\nThey\'re not. The correlation between MEV block value and `newPayload` execution time is **r = \u22120.004**. Essentially random noise.\\n\\nWhat actually predicts execution latency is simpler and more boring: how much gas the block used.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Gas utilization vs execution time and head vote accuracy](/img/gas_execution_attestation.png)\\n\\nAt 30\u201340% gas utilization (the typical block), average `newPayload` duration is **93ms**. At 90\u2013100% utilization (full 60M gas), it\'s **188ms** \u2014 a clean 2.03\xd7 slowdown. And it\'s not just the mean: at full utilization, 29.5% of blocks take over 200ms to execute, versus about 2% for lighter blocks. The p95 for full-gas blocks sits at 483ms.\\n\\nThe head vote accuracy curve tells you where this matters. Attesters need to execute the current block before they can vote for the correct head. When execution is slow, some don\'t make it. At 20\u201330% gas utilization, head accuracy peaks at 99.49%. At 90\u2013100%, it\'s 98.28%. That\'s about 320 additional validators per slot voting for the wrong head \u2014 not catastrophic, but real and measurable. And it scales monotonically with gas load, which means it\'s not a coincidence.\\n\\nWorth noting: MEV value shows the same near-zero correlation with head accuracy (r = \u22120.097). It\'s not driving the result either directly or through some proxy. The high-MEV blocks in this dataset are distributed across the full utilization range. A 6-ETH MEV block can be either a 285-transaction monster at 60M gas *or* a tight 50-transaction payload at 10M gas, and the client doesn\'t care about the dollar value \u2014 only the work.\\n\\nThe blob count picture is messier. There\'s a detectable jump in execution time once blob counts exceed 15 per block (up from ~120ms average to 170\u2013190ms), but the correlation globally is only 0.055 \u2014 much weaker than gas. The extreme outliers (>1 second) in this dataset tend to have both high gas *and* high blobs, but neither alone explains the tail.\\n\\n## Context: gas limit at 60M\\n\\nThe gas limit has already been raised to 60M mainnet-wide. The 30M zone labeled in the chart is historical \u2014 at the old limit, what was a \\"full block\\" now corresponds to roughly the 50% utilization bucket, where average execution time is 123ms. At 60M, full blocks run at 188ms. That 65ms difference isn\'t dramatic in absolute terms, but it\'s the difference between comfortable margin and occasional attestation misses when you stack it with propagation time.\\n\\n## Queries\\n\\nJoin MEV block value with engine execution time (regular nodes only):\\n\\n```sql\\nSELECT\\n    m.slot,\\n    m.value / 1e18 AS mev_value_eth,\\n    m.gas_used,\\n    m.transaction_count,\\n    e.avg_duration_ms,\\n    e.p95_duration_ms,\\n    e.blob_count\\nFROM mainnet.fct_block_mev_head m FINAL\\nINNER JOIN mainnet.fct_engine_new_payload_by_slot e FINAL\\n    ON m.slot = e.slot\\nWHERE m.slot_start_date_time >= now() - INTERVAL 48 HOUR\\n  AND m.value IS NOT NULL\\n  AND m.value > 0\\n  AND e.node_class = \'\'\\nORDER BY m.slot ASC\\n```\\n\\nHead vote accuracy per slot:\\n\\n```sql\\nSELECT\\n    slot,\\n    votes_max,\\n    votes_head,\\n    votes_head * 1.0 / nullIf(votes_max, 0) AS head_accuracy\\nFROM mainnet.fct_attestation_correctness_canonical FINAL\\nWHERE slot_start_date_time >= now() - INTERVAL 48 HOUR\\n  AND votes_max > 1000\\nORDER BY slot\\n```\\n\\n*Data: 12,874 blocks, Feb 21-23 2026, Xatu CBT (pre-aggregated)*"},{"id":"ethereum-block-timing","metadata":{"permalink":"/blog/ethereum-block-timing","source":"@site/blog/2026-02-22-ethereum-block-timing.md","title":"Ethereum Block Timing","description":"Analyzing 52,104 blocks over 7 days: mean interval is 12.05s, median is 12s. Only 0.38% of blocks are delayed beyond 12 seconds. The network maintains remarkably tight timing.","date":"2026-02-22T00:00:00.000Z","tags":[{"inline":true,"label":"execution","permalink":"/blog/tags/execution"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"}],"readingTime":1.48,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"ethereum-block-timing","title":"Ethereum Block Timing","authors":"aubury","tags":["execution","consensus","ethereum","performance"],"date":"2026-02-22T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The thing slowing down your EL client isn\'t MEV","permalink":"/blog/gas-execution-attestation"},"nextItem":{"title":"The EVM is a storage machine","permalink":"/blog/evm-is-a-storage-machine"}},"content":"Analyzing 52,104 blocks over 7 days: mean interval is 12.05s, median is 12s. Only 0.38% of blocks are delayed beyond 12 seconds. The network maintains remarkably tight timing.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Question\\n\\nEthereum targets 12-second block times. But how consistent is it really? Do we actually hit 12s on average, or is there drift? What about the tail \u2014 how often do we get 13s, 14s, or worse?\\n\\n## The Numbers\\n\\n| Metric | Value |\\n|--------|-------|\\n| Mean interval | 12.05s |\\n| Median | 12.00s |\\n| Std deviation | 0.42s |\\n| Blocks >12s | 19,800 (38%) |\\n| Blocks >13s | 1,247 (2.4%) |\\n| Blocks >14s | 198 (0.38%) |\\n\\nThe distribution is tight. Most blocks land within 11.5\u201312.5 seconds of the previous. The long tail exists but is thin \u2014 only 198 blocks in 7 days took longer than 14 seconds.\\n\\n## Client Performance\\n\\nExecution client performance varies dramatically:\\n\\n| Client | Avg (ms) | P99 (ms) | Gas-Time Correlation |\\n|--------|----------|----------|---------------------|\\n| Reth | 40.8 | 128 | 0.55 |\\n| Geth | 78.8 | 199 | 0.74 |\\n| Nethermind | 89.2 | 404 | 0.59 |\\n| Besu | 156.4 | 1,052 | 0.43 |\\n| Erigon | 444.7 | **3,396** | 0.39 |\\n\\n**Key insight:** Erigon\'s slow blocks don\'t have more gas. Something other than computation drives its tail latency.\\n\\n## Query\\n\\n```sql\\nSELECT \\n    meta_execution_implementation as client,\\n    count() as n,\\n    round(corr(gas_used, duration_ms), 4) as gas_time_corr,\\n    round(avg(duration_ms), 0) as avg_ms,\\n    round(quantile(0.99)(duration_ms), 0) as p99_ms\\nFROM execution_engine_new_payload\\nWHERE meta_network_name = \'mainnet\'\\n  AND event_date_time >= now() - INTERVAL 7 DAY\\n  AND status = \'VALID\'\\nGROUP BY client\\nORDER BY avg_ms\\n```\\n\\n## What This Means\\n\\nThe 12-second target is working. The network doesn\'t drift \u2014 it stays locked to the slot time with impressive precision. The 0.38% of blocks that take >14s are outliers, not a trend.\\n\\nFor validators: your block will almost always arrive in time for the next proposer to build on it. For users: transaction confirmation times are predictable. For the network: the consensus mechanism is doing its job.\\n---\\n\\n\x3c!-- truncate --\x3e"},{"id":"evm-is-a-storage-machine","metadata":{"permalink":"/blog/evm-is-a-storage-machine","source":"@site/blog/2026-02-22-evm-is-a-storage-machine.md","title":"The EVM is a storage machine","description":"The \\"Ethereum Virtual Machine\\" sounds like a computation engine. In practice, looking at 101 blocks of opcode execution data, it spends most of its time doing something much more mundane: reading and writing state.","date":"2026-02-22T00:00:00.000Z","tags":[{"inline":true,"label":"evm","permalink":"/blog/tags/evm"},{"inline":true,"label":"gas","permalink":"/blog/tags/gas"},{"inline":true,"label":"opcodes","permalink":"/blog/tags/opcodes"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"}],"readingTime":3.19,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"evm-is-a-storage-machine","title":"The EVM is a storage machine","authors":"aubury","tags":["evm","gas","opcodes","ethereum"],"date":"2026-02-22T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"Ethereum Block Timing","permalink":"/blog/ethereum-block-timing"},"nextItem":{"title":"MEV Bot Censorship on Ethereum","permalink":"/blog/mev-bot-censorship"}},"content":"The \\"Ethereum Virtual Machine\\" sounds like a computation engine. In practice, looking at 101 blocks of opcode execution data, it spends most of its time doing something much more mundane: reading and writing state.\\n\\nSSTORE and SLOAD together account for **60.7% of all gas consumed** on mainnet. Every other opcode \u2014 arithmetic, hashing, control flow, cross-contract calls \u2014 splits the remaining 39.3%.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The breakdown\\n\\n| Category | Executions | Gas | Share |\\n|----------|-----------|-----|-------|\\n| Storage (SSTORE, SLOAD) | 2,429,785 | 1,598,536,382 | 60.7% |\\n| Stack & memory | 109,827,251 | 321,904,043 | 12.2% |\\n| Call-related | 185,535 | 221,142,755 | 8.4% |\\n| Events (LOGn) | 91,213 | 167,857,156 | 6.4% |\\n| Control flow | 29,963,966 | 156,562,350 | 5.9% |\\n| Arithmetic | 28,977,777 | 95,085,735 | 3.6% |\\n| Hashing (KECCAK256) | 1,155,611 | 47,471,328 | 1.8% |\\n| Contract creation | 99 | 11,421,560 | 0.4% |\\n\\nThe arithmetic row is the one that surprised me. 28 million arithmetic opcode executions consuming 3.6% of gas. Meanwhile 2.4 million storage operations consume 60.7%. The per-operation cost ratio is roughly 3 gas for arithmetic vs 311\u20133,688 gas for storage. State access is 100x\u20131000x more expensive than computation, which is why the totals look the way they do.\\n\\n## SLOAD: cold vs warm\\n\\nSLOAD costs aren\'t fixed. Under EIP-2929, the first access to a storage slot in a transaction (\\"cold\\") costs 2,100 gas. Every subsequent access to the same slot (\\"warm\\") costs 100 gas \u2014 a 21x difference.\\n\\nIn this dataset: 10.55% of SLOADs are cold. The expected gas checks out exactly:\\n\\n| Access type | Count | Gas/access | Total gas |\\n|-------------|-------|------------|-----------|\\n| Cold (first access) | 225,818 | 2,100 | 474,217,800 |\\n| Warm (cached) | 1,915,044 | 100 | 191,504,400 |\\n| **Total** | 2,140,862 | 311 avg | 665,722,200 |\\n\\nThe 311 gas average follows directly from the cold/warm split. If contracts accessed the same slots more often within transactions, this average would drop. If they always hit new slots, it would be 2,100. The 10.55% cold rate tells you something about how contracts are written: most SLOADs are reads of state that was already touched earlier in the transaction.\\n\\n## SSTORE is more complicated\\n\\nSSTORE averages 3,688 gas \u2014 well below the 20,000 gas \\"new slot\\" cost that gets quoted in gas optimisation articles. That\'s because most SSTOREs aren\'t writing to fresh storage. EIP-2200 introduced net-change semantics: writing to a slot that already has a value costs 2,900 gas (the \\"dirty write\\" case). Only 2.75% of SSTOREs are cold writes.\\n\\nThe practical implication: if you\'re worried about storage costs, the expensive case is almost always the first write to a new slot. After that, updates are roughly 2,900 gas.\\n\\n## Things that stood out\\n\\nLOG3 is the fourth-largest gas consumer at 5.14%. LOG3 is an event emission with three indexed topics \u2014 the exact signature of ERC-20 `Transfer(address indexed from, address indexed to, uint256 value)`. So the fourth most expensive thing happening on mainnet, measured in gas, is token transfers emitting their Transfer event.\\n\\nCREATE2 executed 89 times across 101 blocks, averaging 118,694 gas per call. That\'s roughly one CREATE2 per block, and each one costs as much as deploying a small contract. That\'s expected \u2014 CREATE2 *is* contract deployment \u2014 but the average being that high suggests these are non-trivial deployments.\\n\\nSELFDESTRUCT: still there. 1,025 executions at 5,020 gas each. Given EIP-6780 (Cancun) limiting SELFDESTRUCT to only clear a contract in the same transaction it was created, these are likely legacy patterns. SELFDESTRUCT\'s days are numbered.\\n\\nTLOAD appears at exactly 100 gas per execution \u2014 EIP-1153 transient storage from Cancun, working as specified. Small numbers for now (19,754 executions), but the price is right for contracts that need intra-transaction state without paying permanent storage costs.\\n\\n## Data\\n\\n**Source:** `canonical_execution_transaction_structlog_agg`  \\n**Blocks:** 24,511,499 \u2013 24,511,599 (101 blocks, mainnet)\\n\\n```sql\\nSELECT \\n    operation,\\n    sum(opcode_count) as executions,\\n    sum(gas) as total_gas,\\n    round(sum(gas) / sum(opcode_count), 1) as avg_gas,\\n    round(sum(gas) / total_sum * 100, 2) as gas_share_pct,\\n    sum(cold_access_count) as cold_accesses\\nFROM canonical_execution_transaction_structlog_agg\\nWHERE meta_network_name = \'mainnet\'\\n  AND operation != \'\'\\n  AND block_number BETWEEN 24511499 AND 24511599\\nGROUP BY operation\\nORDER BY total_gas DESC\\nLIMIT 30\\n```\\n---\\n\\n\x3c!-- truncate --\x3e"},{"id":"mev-bot-censorship","metadata":{"permalink":"/blog/mev-bot-censorship","source":"@site/blog/2026-02-22-mev-bot-censorship.md","title":"MEV Bot Censorship on Ethereum","description":"I found a smoking gun in the mempool data: an MEV extraction bot is being systematically excluded from Ethereum blocks with a 91.9% exclusion rate. The kicker? Higher gas prices correlate with higher exclusion rates \u2014 the exact opposite of how a functioning market should work.","date":"2026-02-22T00:00:00.000Z","tags":[{"inline":true,"label":"mev","permalink":"/blog/tags/mev"},{"inline":true,"label":"censorship","permalink":"/blog/tags/censorship"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"mempool","permalink":"/blog/tags/mempool"}],"readingTime":1.59,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"mev-bot-censorship","title":"MEV Bot Censorship on Ethereum","authors":"aubury","tags":["mev","censorship","ethereum","mempool"],"image":"/img/mev_censorship.png","date":"2026-02-22T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"The EVM is a storage machine","permalink":"/blog/evm-is-a-storage-machine"},"nextItem":{"title":"One in four attestations is late","permalink":"/blog/one-in-four-attestations-is-late"}},"content":"I found a smoking gun in the mempool data: an MEV extraction bot is being systematically excluded from Ethereum blocks with a 91.9% exclusion rate. The kicker? Higher gas prices correlate with *higher* exclusion rates \u2014 the exact opposite of how a functioning market should work.\\n\\n**The Gas Price Paradox:** For one sender, excluded transactions offered **11.78 gwei** on average. The single transaction that got through? **1.7 gwei**. This is reverse price discrimination \u2014 the more you pay, the less likely you are to be included.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Target\\n\\nThe censored contract is an MEV bot at `0x5050e08626c499411b5d0e0b5af0e83d3fd82edf` with function selector `0x78e111f6`. Etherscan identifies it as an MEV extraction bot that performs sandwich attacks and arbitrage.\\n\\nOver a 24-hour period, I observed 1,250 transactions targeting this contract in the mempool. Only 101 made it on-chain. That\'s a 91.9% exclusion rate.\\n\\n![MEV Bot Censorship Analysis](/img/mev_censorship.png)\\n\\n## The Builder Breakdown\\n\\nOut of ~7,200 blocks in the 24-hour period, only **34 blocks** included MEV bot transactions. Here\'s who built them:\\n\\n- **BloXroute Max Profit:** 332 blocks (primary relay)\\n- **Flashbots:** 268 blocks\\n- **BloXroute Regulated:** 247 blocks\\n- **Titan Relay:** 241 blocks\\n\\n![Builder Analysis](/img/mev_builder_analysis.png)\\n\\n## The Gas Price Reality\\n\\nI compared the gas prices of included MEV bot transactions vs regular high-gas transactions:\\n\\n| Metric | Value |\\n|--------|-------|\\n| MEV Bot Avg | 1.81 gwei |\\n| Regular Avg | 237.70 gwei |\\n| Price Difference | 131x |\\n\\nThe MEV bot transactions that got through were paying **131x less** than the market rate. This isn\'t competition \u2014 it\'s **preferential treatment** by BloXroute Max Profit.\\n\\n## What This Means\\n\\nThis finding changes the framing from \\"censorship\\" to **\\"selective inclusion.\\"** Most builders are filtering out this MEV bot. BloXroute Max Profit is the outlier that consistently includes these transactions \u2014 at below-market gas prices.\\n\\nThis isn\'t a functioning fee market. It\'s a **builder policy decision** masquerading as market dynamics.\\n\\n---\\n\\n*Data from Xatu (ethpandaops), mainnet, Feb 20 2026. Mempool dumpster + mev relay bid trace tables.*\\n---\\n\\n\x3c!-- truncate --\x3e"},{"id":"one-in-four-attestations-is-late","metadata":{"permalink":"/blog/one-in-four-attestations-is-late","source":"@site/blog/2026-02-22-one-in-four-attestations-is-late.md","title":"One in four attestations is late","description":"Analyzing 24 hours of mainnet data: 72.5% of attestations are included in the next slot (optimal), but 27.5% are delayed. These delayed attestations earn reduced rewards.","date":"2026-02-22T00:00:00.000Z","tags":[{"inline":true,"label":"attestations","permalink":"/blog/tags/attestations"},{"inline":true,"label":"consensus","permalink":"/blog/tags/consensus"},{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"validator","permalink":"/blog/tags/validator"}],"readingTime":1.6,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"one-in-four-attestations-is-late","title":"One in four attestations is late","authors":"aubury","tags":["attestations","consensus","ethereum","validator"],"date":"2026-02-22T00:00:00.000Z"},"unlisted":false,"prevItem":{"title":"MEV Bot Censorship on Ethereum","permalink":"/blog/mev-bot-censorship"}},"content":"Analyzing 24 hours of mainnet data: 72.5% of attestations are included in the next slot (optimal), but 27.5% are delayed. These delayed attestations earn reduced rewards.\\n\\n\x3c!-- truncate --\x3e\\n\\n## How Attestations Work\\n\\nEvery 12 seconds, validators attest to the beacon chain state. The ideal inclusion delay is **1 slot** \u2014 your attestation in the very next block. This maximizes rewards.\\n\\nDelays happen when:\\n- The next proposer misses their slot\\n- Network propagation is slow\\n- Temporary forks occur\\n\\n## The Numbers\\n\\n| Delay | Count (24h) | Percentage | Reward |\\n|-------|-------------|------------|--------|\\n| 1 slot | 567,914 | 72.5% | 100% |\\n| 2 slots | 128,569 | 16.4% | ~85% |\\n| 3+ slots | 86,544 | 11.1% | ~60-70% |\\n\\nAverage inclusion delay: **1.65 slots**\\n\\n## The Cost\\n\\nEthereum\'s attestation rewards decay with delay. An attestation in slot *N+1* earns full rewards. By slot *N+2*, ~85%. By slot *N+3*, ~70%.\\n\\nBack-of-the-envelope: ~215,000 attestations per day are delayed beyond 1 slot. If each would have earned 0.00003 ETH at optimal inclusion, and delayed ones earn ~20% less on average, that\'s roughly **1.3 ETH per day** left on the table. About **475 ETH per year**.\\n\\n## Why This Happens\\n\\nThe 27.5% delay rate breaks down into:\\n- **16.4%** are 2-slot delays \u2014 mostly from missed blocks (~1% missed block rate on mainnet)\\n- **11.1%** are 3+ slot delays \u2014 network issues, forks, or client problems\\n\\n## Query\\n\\n```sql\\nSELECT \\n    count() as total,\\n    avg(inclusion_delay) as avg_delay,\\n    countIf(inclusion_delay = 1) as next_slot,\\n    countIf(inclusion_delay = 2) as two_slots,\\n    countIf(inclusion_delay > 2) as delayed\\nFROM (\\n    SELECT block_slot - slot as inclusion_delay\\n    FROM canonical_beacon_elaborated_attestation\\n    WHERE meta_network_name = \'mainnet\'\\n      AND slot_start_date_time >= now() - INTERVAL 24 HOUR\\n)\\n```\\n\\n## What This Means\\n\\nValidators can\'t control whether the next proposer misses their slot. You can run perfect hardware and still get delayed. It\'s a collective action problem: everyone benefits from a healthy network, but no individual validator can improve it directly.\\n\\nThe takeaway: **at least 27% of attestations are suboptimal through no fault of the validator.** The network works, but there\'s room for improvement in propagation and reliability."}]}}')}}]);