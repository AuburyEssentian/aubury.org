"use strict";(globalThis.webpackChunkaubury_blog=globalThis.webpackChunkaubury_blog||[]).push([[9225],{7894(e,t,n){n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var r=n(5574),s=n(4848),i=n(8453);const a={slug:"client-disk-ram-tradeoff",title:"Your Ethereum Client Is Burning Through Your SSD",authors:["aubury"],tags:["ethereum","clients","performance","infrastructure"]},o=void 0,l={authorsImageUrls:[void 0]},d=[];function h(e){const t={code:"code",em:"em",hr:"hr",img:"img",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.p,{children:"Ethereum has five execution clients and six consensus clients. Everyone has opinions about which is fastest \u2014 but almost nobody talks about what they do to your disk."}),"\n",(0,s.jsx)(t.p,{children:"The short answer: your choice of client stack determines whether your SSD lasts four years or forty."}),"\n",(0,s.jsxs)(t.p,{children:["The data here comes from ethpandaops' monitoring nodes \u2014 a fleet of machines running every client combination, continuously observed over seven days ending February 28, 2026. These are mainnet nodes, not testnets. The disk I/O figures come directly from ",(0,s.jsx)(t.code,{children:"/proc/<pid>/io"}),", the same counters used by every Linux performance tool."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Geth writes 209 GB to disk every day."})," Erigon writes 0.56 GB. That's a 373\xd7 gap between two clients doing the same job on the same network."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"-- 7-day average disk write I/O per execution client (mainnet monitoring nodes)\nSELECT\n  client_type,\n  round(avg(io_bytes) * 7200 / 1e9, 2) as write_gb_per_day,\n  round(avg(vm_rss_bytes) / 1e9, 1) as ram_gb\nFROM mainnet.fct_node_disk_io_by_process d\nJOIN mainnet.fct_node_memory_usage_by_process m\n  USING (meta_client_name, wallclock_slot)\nWHERE d.meta_network_name = 'mainnet'\n  AND d.meta_client_name LIKE '%mainnet%'\n  AND d.wallclock_slot_start_date_time >= now() - INTERVAL 7 DAY\n  AND d.rw = 'write'\n  AND d.client_type IN ('geth','reth','nethermind','erigon','besu')\nGROUP BY client_type\nORDER BY write_gb_per_day DESC\n"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{children:"Client"}),(0,s.jsx)(t.th,{children:"Writes/day"}),(0,s.jsx)(t.th,{children:"RAM"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Geth"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"209 GB"})}),(0,s.jsx)(t.td,{children:"9 GB"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Nethermind"}),(0,s.jsx)(t.td,{children:"92 GB"}),(0,s.jsx)(t.td,{children:"11 GB"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Besu"}),(0,s.jsx)(t.td,{children:"53 GB"}),(0,s.jsx)(t.td,{children:"18 GB"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Reth"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"5 GB"})}),(0,s.jsx)(t.td,{children:"92 GB"})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{children:"Erigon"}),(0,s.jsx)(t.td,{children:(0,s.jsx)(t.strong,{children:"0.56 GB"})}),(0,s.jsx)(t.td,{children:"88 GB"})]})]})]}),"\n",(0,s.jsx)(t.p,{children:"The pattern is immediate. Geth and Nethermind keep RAM low and write constantly. Reth and Erigon load ~90 GB of database into virtual memory and barely touch the disk. Trading 80 GB of RAM for 200 GB/day of write I/O is a real, measurable architectural choice \u2014 not a marketing claim."}),"\n",(0,s.jsxs)(t.p,{children:["For SSD endurance: Geth at 209 GB/day is approximately ",(0,s.jsx)(t.strong,{children:"76 TBW per year"}),". A 1 TB consumer SSD with 300 TBW endurance rating would reach end-of-life in under four years. A prosumer Samsung 870 EVO 2TB (1,200 TBW) would last about fifteen years. The concern isn't theoretical \u2014 it's the difference between replacing your SSD once and never touching it."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.img,{alt:"Client Disk vs RAM Chart",src:n(6955).A+"",width:"2084",height:"1233"})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:"The architecture explains everything. Geth uses LevelDB with frequent compaction cycles. Every block commits state changes through multiple LevelDB levels, and compaction periodically rewrites data \u2014 write amplification compounds quickly. Nine monitoring nodes across different CL pairings all show the same pattern: 90 to 430 GB/day depending on configuration, with an average of 209 GB."}),"\n",(0,s.jsx)(t.p,{children:"Erigon uses MDBX in append-only mode. Writes are sequential and minimal; reads come from disk when the mapped region isn't warm in page cache. This is why Erigon reads 2.5 GB/day from disk while writing almost nothing \u2014 it's reading state that wasn't hot enough to stay resident, rather than writing compaction output."}),"\n",(0,s.jsx)(t.p,{children:"Reth uses its own MDBX-derived storage. The result is similar to Erigon \u2014 5 GB/day writes \u2014 but Reth maps even more state into memory (92 GB vs 88 GB), which likely explains its slightly higher write rate. Both are dramatically more storage-efficient than Geth or Nethermind regardless."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsxs)(t.p,{children:["The consensus layer is a less obvious story. Lighthouse \u2014 generally considered one of the leaner CL clients \u2014 writes ",(0,s.jsx)(t.strong,{children:"125 GB/day"}),". That's more than Nethermind. More than Besu. More than any EL client except Geth."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"-- CL clients, same query\nSELECT client_type, write_gb_per_day, ram_gb\n-- Lighthouse: 125 GB writes, 13 GB RAM\n-- Teku:        64 GB writes, 56 GB RAM\n-- Prysm:       36 GB writes, 14 GB RAM\n-- Grandine:    31 GB writes, 14 GB RAM\n-- Lodestar:    24 GB writes, 14 GB RAM\n-- Nimbus:       1 GB writes, 16 GB RAM\n"})}),"\n",(0,s.jsx)(t.p,{children:"Lighthouse's write intensity likely comes from its historical attestation and block archive storage. The database grows as the chain grows, and Lighthouse keeps writing in large sequential batches."}),"\n",(0,s.jsx)(t.p,{children:"Nimbus is the inverse: 0.95 GB/day in writes, 55.6 GB in reads. The same memory-mapped pattern as Erigon, applied to the consensus layer. Nimbus reads from its SQLite database but rarely flushes. On constrained hardware \u2014 Raspberry Pi, NUC, anything with a small but fast SSD \u2014 Nimbus and Erigon is probably the right pairing for SSD longevity reasons alone."}),"\n",(0,s.jsxs)(t.p,{children:["Teku is the outlier no other client matches: 55 GB of RAM ",(0,s.jsx)(t.em,{children:"and"})," 64 GB/day of writes. Most clients make a tradeoff in one direction. Teku makes neither \u2014 it's the resource-heaviest CL client by RAM, and second-heaviest by disk writes."]}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsxs)(t.p,{children:["One specific event stands out in the data. Besu's disk write rate collapsed from ",(0,s.jsx)(t.strong,{children:"388 GB/day on February 19th to 58 GB/day on February 22nd"})," \u2014 a 6\xd7 reduction in 72 hours."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"-- Besu daily write I/O over 20 days\nSELECT toDate(wallclock_slot_start_date_time) as day,\n       round(avg(io_bytes) * 7200 / 1e9, 2) as write_gb_per_day\nFROM mainnet.fct_node_disk_io_by_process\nWHERE client_type = 'besu' AND rw = 'write'\n  AND wallclock_slot_start_date_time >= now() - INTERVAL 20 DAY\nGROUP BY day ORDER BY day\n\n-- Feb 11: 416 GB \u2192 Feb 18: 338 GB \u2192 Feb 21: 284 GB \u2192 Feb 22: 58 GB \u2192 Feb 28: 22 GB\n"})}),"\n",(0,s.jsx)(t.p,{children:"This is an update event. The monitoring nodes received a new Besu version around February 21\u201322, and something in that release dramatically cut write amplification. The trend has continued downward since \u2014 Besu is now writing less than Geth per day, reversed from the opposite position it held three weeks ago."}),"\n",(0,s.jsx)(t.p,{children:"No announcement was flagged on this specifically, but the data is unambiguous. A full month of 300\u2013400 GB/day suddenly became 22 GB/day within a week."}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:"The practical takeaway depends on your hardware. If you're running a validator on a desktop with a mid-range SSD, Geth + Lighthouse is probably the worst combination for disk longevity. Reth or Erigon + Nimbus is the best. The performance characteristics from engine_newPayload benchmarks might point one direction; the disk endurance picture points another."}),"\n",(0,s.jsx)(t.p,{children:"None of this means Geth is bad \u2014 it's been reliable, well-tested, and the dominant EL client for years. But \"reliable\" and \"easy on storage\" aren't the same thing. The numbers exist, and they're measurable. Now they're measured."})]})}function c(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},6955(e,t,n){n.d(t,{A:()=>r});const r=n.p+"assets/images/client-disk-ram-tradeoff-64cb4e66bb64a7b636da564b8a99982c.png"},8453(e,t,n){n.d(t,{R:()=>a,x:()=>o});var r=n(6540);const s={},i=r.createContext(s);function a(e){const t=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:t},e.children)}},5574(e){e.exports=JSON.parse('{"permalink":"/blog/client-disk-ram-tradeoff","source":"@site/blog/2026-03-01-client-disk-ram-tradeoff.md","title":"Your Ethereum Client Is Burning Through Your SSD","description":"Ethereum has five execution clients and six consensus clients. Everyone has opinions about which is fastest \u2014 but almost nobody talks about what they do to your disk.","date":"2026-03-01T00:00:00.000Z","tags":[{"inline":true,"label":"ethereum","permalink":"/blog/tags/ethereum"},{"inline":true,"label":"clients","permalink":"/blog/tags/clients"},{"inline":true,"label":"performance","permalink":"/blog/tags/performance"},{"inline":true,"label":"infrastructure","permalink":"/blog/tags/infrastructure"}],"readingTime":4.93,"hasTruncateMarker":true,"authors":[{"name":"Aubury Essentian","title":"Ethereum Research","url":"https://github.com/AuburyEssentian","imageURL":"/img/avatar.png","key":"aubury","page":null}],"frontMatter":{"slug":"client-disk-ram-tradeoff","title":"Your Ethereum Client Is Burning Through Your SSD","authors":["aubury"],"tags":["ethereum","clients","performance","infrastructure"]},"unlisted":false,"prevItem":{"title":"EIP-7549 Saved 23 KB Per Block. The Gas Limit Took It All Back.","permalink":"/blog/beacon-block-composition"},"nextItem":{"title":"Fulu\'s Blob Expansion: Three Months Later, Rollups Still Act Like It\'s Deneb","permalink":"/blog/fulu-blob-gap"}}')}}]);